#+TITLE: Practice Hands-on ML Chap11 Training Deep Neural Networks

#+begin_src jupyter-python
from pathlib import Path
import math
import numpy as np

# Visualization
import matplotlib as mpl
import matplotlib.pyplot as plt

# Scikit-learn

# Data processing

# Metrics and model selection

# Models

# Tensorflow
import tensorflow as tf
from tensorflow import keras

# matplotlib and sklearn config
mpl.rc("axes", labelsize=10, linewidth=0.3)
mpl.rc("xtick", labelsize=10)
mpl.rc("ytick", labelsize=10)
mpl.rc("legend", fontsize=8, fancybox=True)
mpl.rc("figure", facecolor="white", dpi=120)
#+end_src

#+RESULTS:


* Vanishing / Exploding Gradients Problem

** Sigmoid and weights initialization with standard normal distribution

#+begin_src jupyter-python
def logit(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-5, 5, 200)

plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([-5, 5], [1, 1], 'k--')
plt.plot([0, 0], [-0.2, 1.2], 'k-')
plt.plot([-5, 5], [-3/4, 7/4], 'g--')
plt.plot(z, logit(z), "b-", linewidth=2)
props = dict(facecolor='black', shrink=0.1)
plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha="center")
plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha="center")
plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha="center")
plt.grid(True)
plt.title("Sigmoid activation function", fontsize=14)
plt.axis([-5, 5, -0.2, 1.2]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c2249f997df3f433cb97837c74c98a95a8c67b4d.png]]


** Xavier and He initialization

#+begin_src jupyter-python
print("Available initializers in Keras:")
print([name for name in dir(keras.initializers) if not name.startswith("_")])
#+end_src

#+RESULTS:
: Available initializers in Keras:
: ['Constant', 'GlorotNormal', 'GlorotUniform', 'HeNormal', 'HeUniform', 'Identity', 'Initializer', 'LecunNormal', 'LecunUniform', 'Ones', 'Orthogonal', 'RandomNormal', 'RandomUniform', 'TruncatedNormal', 'VarianceScaling', 'Zeros', 'constant', 'deserialize', 'get', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform', 'identity', 'lecun_normal', 'lecun_uniform', 'ones', 'orthogonal', 'random_normal', 'random_uniform', 'serialize', 'truncated_normal', 'variance_scaling', 'zeros']

#+begin_src jupyter-python
keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')
#+end_src

#+RESULTS:
: <tensorflow.python.keras.layers.core.Dense at 0x7f57e3a10430>

#+begin_src jupyter-python
he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')
keras.layers.Dense(10, activation='relu', kernel_initializer=he_avg_init)
#+end_src

#+RESULTS:
: <tensorflow.python.keras.layers.core.Dense at 0x7f57e38637c0>

** Non-saturating activation functions

*** Leaky ReLU

#+begin_src jupyter-python
def leaky_relu(z, alpha=0.01):
    return np.maximum(alpha*z, z)

plt.plot(z, leaky_relu(z, 0.05), "b-", linewidth=2)
plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([0, 0], [-0.5, 4.2], 'k-')
plt.grid(True)
props = dict(facecolor='black', shrink=0.1)
plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha="center")
plt.title("Leaky ReLU activation function", fontsize=14)
plt.axis([-5, 5, -0.5, 4.2]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/087b83f58db80d62ec05ef6f01d29c944cde6576.png]]

#+begin_src jupyter-python
print("Available activation functions in Keras:")
print([m for m in dir(keras.activations) if not m.startswith("_")])
print("\nAll ReLU layers in Keras:")
print([m for m in dir(keras.layers) if "elu" in m.lower()])
#+end_src

#+RESULTS:
: Available activation functions in Keras:
: ['deserialize', 'elu', 'exponential', 'gelu', 'get', 'hard_sigmoid', 'linear', 'relu', 'selu', 'serialize', 'sigmoid', 'softmax', 'softplus', 'softsign', 'swish', 'tanh']
:
: All ReLU layers in Keras:
: ['ELU', 'LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']

**** Train a neural network on Fashion MNIST using the Leaky ReLU

#+begin_src jupyter-python
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
X_train_full, X_test = X_train_full / 255.0, X_test / 255.0
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]
#+end_src

#+RESULTS:

#+begin_src jupyter-python
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential(
    [
        keras.layers.Flatten(input_shape=[28, 28]),
        keras.layers.Dense(300, kernel_initializer="he_normal"),
        keras.layers.LeakyReLU(),
        keras.layers.Dense(100, kernel_initializer="he_normal"),
        keras.layers.LeakyReLU(),
        keras.layers.Dense(10, activation="softmax"),
    ]
)

model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer=keras.optimizers.SGD(lr=1e-3),
    metrics=["accuracy"],
)
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/10
1719/1719 [==============================] - 1s 754us/step - loss: 1.6314 - accuracy: 0.5054 - val_loss: 0.8886 - val_accuracy: 0.7160
Epoch 2/10
1719/1719 [==============================] - 1s 680us/step - loss: 0.8416 - accuracy: 0.7246 - val_loss: 0.7130 - val_accuracy: 0.7656
Epoch 3/10
1719/1719 [==============================] - 1s 684us/step - loss: 0.7053 - accuracy: 0.7638 - val_loss: 0.6427 - val_accuracy: 0.7898
Epoch 4/10
1719/1719 [==============================] - 1s 680us/step - loss: 0.6325 - accuracy: 0.7908 - val_loss: 0.5900 - val_accuracy: 0.8064
Epoch 5/10
1719/1719 [==============================] - 1s 680us/step - loss: 0.5992 - accuracy: 0.8021 - val_loss: 0.5582 - val_accuracy: 0.8202
Epoch 6/10
1719/1719 [==============================] - 1s 678us/step - loss: 0.5624 - accuracy: 0.8143 - val_loss: 0.5350 - val_accuracy: 0.8238
Epoch 7/10
1719/1719 [==============================] - 1s 676us/step - loss: 0.5379 - accuracy: 0.8218 - val_loss: 0.5157 - val_accuracy: 0.8304
Epoch 8/10
1719/1719 [==============================] - 1s 670us/step - loss: 0.5152 - accuracy: 0.8296 - val_loss: 0.5079 - val_accuracy: 0.8284
Epoch 9/10
1719/1719 [==============================] - 1s 667us/step - loss: 0.5100 - accuracy: 0.8268 - val_loss: 0.4895 - val_accuracy: 0.8386
Epoch 10/10
1719/1719 [==============================] - 1s 805us/step - loss: 0.4918 - accuracy: 0.8339 - val_loss: 0.4817 - val_accuracy: 0.8396
#+end_example

**** Train with PReLU

#+begin_src jupyter-python
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential(
    [
        keras.layers.Flatten(input_shape=[28, 28]),
        keras.layers.Dense(300, kernel_initializer="he_normal"),
        keras.layers.PReLU(),
        keras.layers.Dense(100, kernel_initializer="he_normal"),
        keras.layers.PReLU(),
        keras.layers.Dense(10, activation="softmax"),
    ]
)
model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer=keras.optimizers.SGD(lr=1e-3),
    metrics=["accuracy"],
)
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/10
1719/1719 [==============================] - 1s 771us/step - loss: 1.6969 - accuracy: 0.4974 - val_loss: 0.9255 - val_accuracy: 0.7186
Epoch 2/10
1719/1719 [==============================] - 1s 751us/step - loss: 0.8706 - accuracy: 0.7247 - val_loss: 0.7305 - val_accuracy: 0.7630
Epoch 3/10
1719/1719 [==============================] - 1s 773us/step - loss: 0.7211 - accuracy: 0.7621 - val_loss: 0.6564 - val_accuracy: 0.7882
Epoch 4/10
1719/1719 [==============================] - 2s 956us/step - loss: 0.6447 - accuracy: 0.7879 - val_loss: 0.6003 - val_accuracy: 0.8048
Epoch 5/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.6077 - accuracy: 0.8004 - val_loss: 0.5656 - val_accuracy: 0.8182
Epoch 6/10
1719/1719 [==============================] - 1s 814us/step - loss: 0.5692 - accuracy: 0.8118 - val_loss: 0.5406 - val_accuracy: 0.8236
Epoch 7/10
1719/1719 [==============================] - 1s 752us/step - loss: 0.5428 - accuracy: 0.8194 - val_loss: 0.5196 - val_accuracy: 0.8314
Epoch 8/10
1719/1719 [==============================] - 1s 769us/step - loss: 0.5193 - accuracy: 0.8283 - val_loss: 0.5113 - val_accuracy: 0.8318
Epoch 9/10
1719/1719 [==============================] - 1s 750us/step - loss: 0.5128 - accuracy: 0.8273 - val_loss: 0.4916 - val_accuracy: 0.8382
Epoch 10/10
1719/1719 [==============================] - 1s 825us/step - loss: 0.4940 - accuracy: 0.8314 - val_loss: 0.4826 - val_accuracy: 0.8394
#+end_example

*** ELU

#+begin_src jupyter-python
def elu(z, alpha=1):
    return np.where(z<0, alpha*(np.exp(z)-1), z)
plt.plot(z, elu(z), "b-", linewidth=2)
plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([-5, 5], [-1, -1], 'k--')
plt.plot([0, 0], [-2.2, 3.2], 'k-')
plt.grid(True)
plt.title(r"ELU activation function ($\alpha=1$)", fontsize=14)
plt.axis([-5, 5, -2.2, 3.2]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/021d8624bf88c75f50f8367bfe75969cca4c321f.png]]

#+begin_src jupyter-python
keras.layers.Dense(10, activation='elu')
#+end_src

#+RESULTS:
: <tensorflow.python.keras.layers.core.Dense at 0x7f57bc1a4fa0>

*** SELU
This activation function was proposed in this great [[https://arxiv.org/pdf/1706.02515.pdf][paper]] by GÃ¼nter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use â1 or â2 regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions.

#+begin_src jupyter-python
from scipy.special import erfc

# alpha and scale to self normalize with mean 0 and standard deviation 1
# (see equation 14 in the paper):
alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)
scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)

def selu(z, scale=scale_0_1, alpha=alpha_0_1):
    return scale * elu(z, alpha)

plt.plot(z, selu(z), "b-", linewidth=2)
plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([-5, 5], [-1.758, -1.758], 'k--')
plt.plot([0, 0], [-2.2, 3.2], 'k-')
plt.grid(True)
plt.title("SELU activation function", fontsize=14)
plt.axis([-5, 5, -2.2, 3.2]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b773fe4faf65870a1c510a8f0e7f1cf54dd4e697.png]]

By default, the SELU hyperparameters (scale and alpha) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:

#+begin_src jupyter-python
np.random.seed(42)
Z = np.random.normal(size=(500, 100)) # standardized inputs
for layer in range(1000):
    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization
    Z = selu(np.dot(Z, W))
    means = np.mean(Z, axis=0).mean()
    stds = np.std(Z, axis=0).mean()
    if layer % 100 == 0:
        print("Layer {}: mean {:.2f}, std deviation {:.2f}".format(layer, means, stds))
#+end_src

#+RESULTS:
#+begin_example
Layer 0: mean -0.00, std deviation 1.00
Layer 100: mean 0.02, std deviation 0.96
Layer 200: mean 0.01, std deviation 0.90
Layer 300: mean -0.02, std deviation 0.92
Layer 400: mean 0.05, std deviation 0.89
Layer 500: mean 0.01, std deviation 0.93
Layer 600: mean 0.02, std deviation 0.92
Layer 700: mean -0.02, std deviation 0.90
Layer 800: mean 0.05, std deviation 0.83
Layer 900: mean 0.02, std deviation 1.00
#+end_example

**** Create a neural net for Fashion MNIST with 100 hidden layers using SELU

#+begin_src jupyter-python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation="selu",
                             kernel_initializer="lecun_normal"))
for layer in range(99):
    model.add(keras.layers.Dense(100, activation="selu",
                                 kernel_initializer="lecun_normal"))
model.add(keras.layers.Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(lr=1e-3),
              metrics=["accuracy"])
#+end_src

#+RESULTS:

Before training, scale the inputs to mean 0 and standard deviation 1.

#+begin_src jupyter-python
pixel_means = X_train.mean(axis=0, keepdims=True)
pixel_stds = X_train.std(axis=0, keepdims=True)
X_train_scaled = (X_train - pixel_means) / pixel_stds
X_valid_scaled = (X_valid - pixel_means) / pixel_stds
X_test_scaled = (X_test - pixel_means) / pixel_stds

history = model.fit(X_train_scaled, y_train, epochs=5,
                    validation_data=(X_valid_scaled, y_valid))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/5
1719/1719 [==============================] - 1s 728us/step - loss: 0.9468 - accuracy: 0.6778 - val_loss: 0.5345 - val_accuracy: 0.8156
Epoch 2/5
1719/1719 [==============================] - 1s 673us/step - loss: 0.5215 - accuracy: 0.8183 - val_loss: 0.4741 - val_accuracy: 0.8340
Epoch 3/5
1719/1719 [==============================] - 1s 672us/step - loss: 0.4713 - accuracy: 0.8365 - val_loss: 0.4510 - val_accuracy: 0.8432
Epoch 4/5
1719/1719 [==============================] - 1s 668us/step - loss: 0.4429 - accuracy: 0.8459 - val_loss: 0.4327 - val_accuracy: 0.8488
Epoch 5/5
1719/1719 [==============================] - 1s 648us/step - loss: 0.4354 - accuracy: 0.8492 - val_loss: 0.4216 - val_accuracy: 0.8538
#+end_example

Compare with ReLU, suffering from the vanishing / exploding gradients problem.

#+begin_src jupyter-python
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation="relu", kernel_initializer="he_normal"))
for layer in range(99):
    model.add(keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal"))
model.add(keras.layers.Dense(10, activation="softmax"))

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(lr=1e-3),
              metrics=["accuracy"])

history = model.fit(X_train_scaled, y_train, epochs=5,
                    validation_data=(X_valid_scaled, y_valid))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/5
1719/1719 [==============================] - 11s 6ms/step - loss: 2.0610 - accuracy: 0.1872 - val_loss: 1.6005 - val_accuracy: 0.3170
Epoch 2/5
1719/1719 [==============================] - 9s 5ms/step - loss: 1.2244 - accuracy: 0.4732 - val_loss: 1.2055 - val_accuracy: 0.5082
Epoch 3/5
1719/1719 [==============================] - 9s 5ms/step - loss: 1.0743 - accuracy: 0.5505 - val_loss: 0.8802 - val_accuracy: 0.6350
Epoch 4/5
1719/1719 [==============================] - 9s 5ms/step - loss: 0.8585 - accuracy: 0.6582 - val_loss: 0.7745 - val_accuracy: 0.7008
Epoch 5/5
1719/1719 [==============================] - 9s 5ms/step - loss: 0.7509 - accuracy: 0.6960 - val_loss: 0.8195 - val_accuracy: 0.6486
#+end_example

** Batch Normalization

#+begin_src jupyter-python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(10, activation="softmax")
])
model.summary()
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 784)               0
_________________________________________________________________
batch_normalization (BatchNo (None, 784)               3136
_________________________________________________________________
dense (Dense)                (None, 300)               235500
_________________________________________________________________
batch_normalization_1 (Batch (None, 300)               1200
_________________________________________________________________
dense_1 (Dense)              (None, 100)               30100
_________________________________________________________________
batch_normalization_2 (Batch (None, 100)               400
_________________________________________________________________
dense_2 (Dense)              (None, 10)                1010
=================================================================
Total params: 271,346
Trainable params: 268,978
Non-trainable params: 2,368
_________________________________________________________________
#+end_example

#+begin_src jupyter-python
bn1 = model.layers[1]
[(var.name, var.trainable) for var in bn1.variables]
#+end_src

#+RESULTS:
| batch_normalization/gamma:0           | True  |
| batch_normalization/beta:0            | True  |
| batch_normalization/moving_mean:0     | False |
| batch_normalization/moving_variance:0 | False |

#+begin_src jupyter-python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(lr=1e-3),
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/10
1719/1719 [==============================] - 2s 1ms/step - loss: 1.1437 - accuracy: 0.6195 - val_loss: 0.5615 - val_accuracy: 0.8112
Epoch 2/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5913 - accuracy: 0.7981 - val_loss: 0.4863 - val_accuracy: 0.8350
Epoch 3/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5191 - accuracy: 0.8211 - val_loss: 0.4500 - val_accuracy: 0.8486
Epoch 4/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4883 - accuracy: 0.8326 - val_loss: 0.4265 - val_accuracy: 0.8566
Epoch 5/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4570 - accuracy: 0.8407 - val_loss: 0.4099 - val_accuracy: 0.8608
Epoch 6/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4400 - accuracy: 0.8464 - val_loss: 0.3985 - val_accuracy: 0.8642
Epoch 7/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4214 - accuracy: 0.8516 - val_loss: 0.3882 - val_accuracy: 0.8662
Epoch 8/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4073 - accuracy: 0.8579 - val_loss: 0.3795 - val_accuracy: 0.8700
Epoch 9/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.3960 - accuracy: 0.8613 - val_loss: 0.3732 - val_accuracy: 0.8728
Epoch 10/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.3941 - accuracy: 0.8606 - val_loss: 0.3661 - val_accuracy: 0.8740
#+end_example

Sometimes applying BN before the activation function works better (there's a debate on this topic). Moreover, the layer before a BatchNormalization layer does not need to have bias terms, since the BatchNormalization layer some as well, it would be a waste of parameters, so you can set use_bias=False when creating those layers:

#+begin_src jupyter-python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(300, use_bias=False),
    keras.layers.BatchNormalization(),
    keras.layers.Activation("relu"),
    keras.layers.Dense(100, use_bias=False),
    keras.layers.BatchNormalization(),
    keras.layers.Activation("relu"),
    keras.layers.Dense(10, activation="softmax")
])

model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(lr=1e-3),
              metrics=["accuracy"])

history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/10
1719/1719 [==============================] - 2s 1ms/step - loss: 1.4697 - accuracy: 0.5332 - val_loss: 0.6670 - val_accuracy: 0.7920
Epoch 2/10
1719/1719 [==============================] - 3s 1ms/step - loss: 0.7061 - accuracy: 0.7740 - val_loss: 0.5423 - val_accuracy: 0.8264
Epoch 3/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.6038 - accuracy: 0.8005 - val_loss: 0.4886 - val_accuracy: 0.8412
Epoch 4/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5449 - accuracy: 0.8186 - val_loss: 0.4548 - val_accuracy: 0.8488
Epoch 5/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5162 - accuracy: 0.8259 - val_loss: 0.4305 - val_accuracy: 0.8570
Epoch 6/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4951 - accuracy: 0.8298 - val_loss: 0.4142 - val_accuracy: 0.8600
Epoch 7/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4705 - accuracy: 0.8370 - val_loss: 0.4015 - val_accuracy: 0.8646
Epoch 8/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4552 - accuracy: 0.8422 - val_loss: 0.3914 - val_accuracy: 0.8686
Epoch 9/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4449 - accuracy: 0.8466 - val_loss: 0.3829 - val_accuracy: 0.8682
Epoch 10/10
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4360 - accuracy: 0.8474 - val_loss: 0.3750 - val_accuracy: 0.8694
#+end_example

** Gradient Clipping

All Keras optimizers accept clipnorm or clipvalue arguments:

#+begin_src jupyter-python
optimizer = keras.optimizers.SGD(clipvalue=1.0)
optimizer = keras.optimizers.SGD(clipnorm=1.0)
#+end_src


* Reusing Pretrained Layers

** Reusing a Keras model

Let's split the fashion MNIST training set in two:

    X_train_A: all images of all items except for sandals and shirts (classes 5 and 6).
    X_train_B: a much smaller training set of just the first 200 images of sandals or shirts.

The validation set and the test set are also split this way, but without restricting the number of images.

We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using Dense layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter).

#+begin_src jupyter-python
def split_dataset(X, y):
    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts
    y_A = y[~y_5_or_6]
    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7
    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?
    return ((X[~y_5_or_6], y_A),
            (X[y_5_or_6], y_B))

(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)
(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)
(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)
X_train_B = X_train_B[:200]
y_train_B = y_train_B[:200]
#+end_src

#+RESULTS:

#+begin_src jupyter-python
print(X_train_A.shape)
print(X_train_B.shape)
#+end_src

#+RESULTS:
: (43986, 28, 28)
: (200, 28, 28)

#+begin_src jupyter-python
print(y_train_A[:30])
print(y_train_B[:30])
#+end_src

#+RESULTS:
: [4 0 5 7 7 7 4 4 3 4 0 1 6 3 4 3 2 6 5 3 4 5 1 3 4 2 0 6 7 1]
: [1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.
:  1. 0. 1. 1. 1. 1.]

#+begin_src jupyter-python
tf.random.set_seed(42)
np.random.seed(42)

model_A = keras.models.Sequential()
model_A.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_hidden in (300, 100, 50, 50, 50):
    model_A.add(keras.layers.Dense(n_hidden, activation="selu"))
model_A.add(keras.layers.Dense(8, activation="softmax"))

model_A.compile(loss="sparse_categorical_crossentropy",
                optimizer=keras.optimizers.SGD(lr=1e-3),
                metrics=["accuracy"])
history = model_A.fit(X_train_A, y_train_A, epochs=20,
                      validation_data=(X_valid_A, y_valid_A))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/20
1375/1375 [==============================] - 1s 829us/step - loss: 0.9249 - accuracy: 0.6994 - val_loss: 0.3896 - val_accuracy: 0.8662
Epoch 2/20
1375/1375 [==============================] - 1s 755us/step - loss: 0.3651 - accuracy: 0.8745 - val_loss: 0.3288 - val_accuracy: 0.8827
Epoch 3/20
1375/1375 [==============================] - 1s 770us/step - loss: 0.3182 - accuracy: 0.8897 - val_loss: 0.3013 - val_accuracy: 0.8991
Epoch 4/20
1375/1375 [==============================] - 1s 777us/step - loss: 0.3048 - accuracy: 0.8954 - val_loss: 0.2896 - val_accuracy: 0.9021
Epoch 5/20
1375/1375 [==============================] - 1s 780us/step - loss: 0.2804 - accuracy: 0.9029 - val_loss: 0.2773 - val_accuracy: 0.9061
Epoch 6/20
1375/1375 [==============================] - 1s 774us/step - loss: 0.2701 - accuracy: 0.9075 - val_loss: 0.2735 - val_accuracy: 0.9066
Epoch 7/20
1375/1375 [==============================] - 1s 771us/step - loss: 0.2627 - accuracy: 0.9093 - val_loss: 0.2721 - val_accuracy: 0.9081
Epoch 8/20
1375/1375 [==============================] - 1s 780us/step - loss: 0.2609 - accuracy: 0.9122 - val_loss: 0.2589 - val_accuracy: 0.9141
Epoch 9/20
1375/1375 [==============================] - 1s 739us/step - loss: 0.2558 - accuracy: 0.9110 - val_loss: 0.2562 - val_accuracy: 0.9136
Epoch 10/20
1375/1375 [==============================] - 1s 726us/step - loss: 0.2512 - accuracy: 0.9138 - val_loss: 0.2544 - val_accuracy: 0.9160
Epoch 11/20
1375/1375 [==============================] - 1s 791us/step - loss: 0.2431 - accuracy: 0.9170 - val_loss: 0.2495 - val_accuracy: 0.9153
Epoch 12/20
1375/1375 [==============================] - 1s 758us/step - loss: 0.2422 - accuracy: 0.9168 - val_loss: 0.2515 - val_accuracy: 0.9126
Epoch 13/20
1375/1375 [==============================] - 1s 764us/step - loss: 0.2360 - accuracy: 0.9181 - val_loss: 0.2446 - val_accuracy: 0.9160
Epoch 14/20
1375/1375 [==============================] - 1s 768us/step - loss: 0.2266 - accuracy: 0.9232 - val_loss: 0.2415 - val_accuracy: 0.9178
Epoch 15/20
1375/1375 [==============================] - 1s 771us/step - loss: 0.2225 - accuracy: 0.9239 - val_loss: 0.2449 - val_accuracy: 0.9195
Epoch 16/20
1375/1375 [==============================] - 1s 761us/step - loss: 0.2261 - accuracy: 0.9216 - val_loss: 0.2383 - val_accuracy: 0.9195
Epoch 17/20
1375/1375 [==============================] - 1s 745us/step - loss: 0.2191 - accuracy: 0.9253 - val_loss: 0.2413 - val_accuracy: 0.9178
Epoch 18/20
1375/1375 [==============================] - 1s 755us/step - loss: 0.2171 - accuracy: 0.9255 - val_loss: 0.2430 - val_accuracy: 0.9160
Epoch 19/20
1375/1375 [==============================] - 1s 751us/step - loss: 0.2180 - accuracy: 0.9246 - val_loss: 0.2330 - val_accuracy: 0.9208
Epoch 20/20
1375/1375 [==============================] - 1s 755us/step - loss: 0.2112 - accuracy: 0.9273 - val_loss: 0.2332 - val_accuracy: 0.9205
#+end_example

#+begin_src jupyter-python
model_A.save("my_model_A.h5")
#+end_src

#+RESULTS:

#+begin_src jupyter-python
model_B = keras.models.Sequential()
model_B.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_hidden in (300, 100, 50, 50, 50):
    model_B.add(keras.layers.Dense(n_hidden, activation="selu"))
model_B.add(keras.layers.Dense(1, activation="sigmoid"))

model_B.compile(loss="binary_crossentropy",
                optimizer=keras.optimizers.SGD(lr=1e-3),
                metrics=['accuracy'])
history = model_B.fit(X_train_B, y_train_B, epochs=20, validation_data=(X_valid_B, y_valid_B))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/20
7/7 [==============================] - 0s 18ms/step - loss: 1.0360 - accuracy: 0.4975 - val_loss: 0.6314 - val_accuracy: 0.6004
Epoch 2/20
7/7 [==============================] - 0s 6ms/step - loss: 0.5883 - accuracy: 0.6971 - val_loss: 0.4784 - val_accuracy: 0.8529
Epoch 3/20
7/7 [==============================] - 0s 5ms/step - loss: 0.4380 - accuracy: 0.8854 - val_loss: 0.4102 - val_accuracy: 0.8945
Epoch 4/20
7/7 [==============================] - 0s 6ms/step - loss: 0.4021 - accuracy: 0.8712 - val_loss: 0.3647 - val_accuracy: 0.9178
Epoch 5/20
7/7 [==============================] - 0s 7ms/step - loss: 0.3361 - accuracy: 0.9348 - val_loss: 0.3300 - val_accuracy: 0.9320
Epoch 6/20
7/7 [==============================] - 0s 6ms/step - loss: 0.3113 - accuracy: 0.9233 - val_loss: 0.3019 - val_accuracy: 0.9402
Epoch 7/20
7/7 [==============================] - 0s 5ms/step - loss: 0.2817 - accuracy: 0.9299 - val_loss: 0.2804 - val_accuracy: 0.9422
Epoch 8/20
7/7 [==============================] - 0s 6ms/step - loss: 0.2632 - accuracy: 0.9379 - val_loss: 0.2606 - val_accuracy: 0.9473
Epoch 9/20
7/7 [==============================] - 0s 6ms/step - loss: 0.2373 - accuracy: 0.9481 - val_loss: 0.2428 - val_accuracy: 0.9523
Epoch 10/20
7/7 [==============================] - 0s 6ms/step - loss: 0.2229 - accuracy: 0.9657 - val_loss: 0.2281 - val_accuracy: 0.9544
Epoch 11/20
7/7 [==============================] - 0s 5ms/step - loss: 0.2155 - accuracy: 0.9590 - val_loss: 0.2150 - val_accuracy: 0.9584
Epoch 12/20
7/7 [==============================] - 0s 6ms/step - loss: 0.1834 - accuracy: 0.9738 - val_loss: 0.2036 - val_accuracy: 0.9584
Epoch 13/20
7/7 [==============================] - 0s 5ms/step - loss: 0.1671 - accuracy: 0.9828 - val_loss: 0.1931 - val_accuracy: 0.9615
Epoch 14/20
7/7 [==============================] - 0s 6ms/step - loss: 0.1527 - accuracy: 0.9915 - val_loss: 0.1838 - val_accuracy: 0.9635
Epoch 15/20
7/7 [==============================] - 0s 6ms/step - loss: 0.1595 - accuracy: 0.9904 - val_loss: 0.1746 - val_accuracy: 0.9686
Epoch 16/20
7/7 [==============================] - 0s 6ms/step - loss: 0.1473 - accuracy: 0.9937 - val_loss: 0.1674 - val_accuracy: 0.9686
Epoch 17/20
7/7 [==============================] - 0s 6ms/step - loss: 0.1412 - accuracy: 0.9944 - val_loss: 0.1604 - val_accuracy: 0.9706
Epoch 18/20
7/7 [==============================] - 0s 6ms/step - loss: 0.1242 - accuracy: 0.9931 - val_loss: 0.1539 - val_accuracy: 0.9706
Epoch 19/20
7/7 [==============================] - 0s 6ms/step - loss: 0.1224 - accuracy: 0.9931 - val_loss: 0.1482 - val_accuracy: 0.9716
Epoch 20/20
7/7 [==============================] - 0s 6ms/step - loss: 0.1096 - accuracy: 0.9912 - val_loss: 0.1431 - val_accuracy: 0.9716
#+end_example

#+begin_src jupyter-python
model_B.summary()
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_3 (Flatten)          (None, 784)               0
_________________________________________________________________
dense_12 (Dense)             (None, 300)               235500
_________________________________________________________________
dense_13 (Dense)             (None, 100)               30100
_________________________________________________________________
dense_14 (Dense)             (None, 50)                5050
_________________________________________________________________
dense_15 (Dense)             (None, 50)                2550
_________________________________________________________________
dense_16 (Dense)             (None, 50)                2550
_________________________________________________________________
dense_17 (Dense)             (None, 1)                 51
=================================================================
Total params: 275,801
Trainable params: 275,801
Non-trainable params: 0
_________________________________________________________________
#+end_example

#+begin_src jupyter-python
model_A = keras.models.load_model("my_model_A.h5")
model_B_on_A = keras.models.Sequential(model_A.layers[:-1])
model_B_on_A.add(keras.layers.Dense(1, activation="sigmoid"))

model_A_clone = keras.models.clone_model(model_A)
model_A_clone.set_weights(model_A.get_weights())

for layer in model_B_on_A.layers[:-1]:
    layer.trainable = False

model_B_on_A.compile(loss="binary_crossentropy",
                     optimizer=keras.optimizers.SGD(lr=1e-3),
                     metrics=['accuracy'])

history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))
#+end_src

#+RESULTS:
: Epoch 1/4
: 7/7 [==============================] - 0s 18ms/step - loss: 0.6125 - accuracy: 0.6233 - val_loss: 0.5824 - val_accuracy: 0.6359
: Epoch 2/4
: 7/7 [==============================] - 0s 5ms/step - loss: 0.5525 - accuracy: 0.6638 - val_loss: 0.5451 - val_accuracy: 0.6836
: Epoch 3/4
: 7/7 [==============================] - 0s 6ms/step - loss: 0.4875 - accuracy: 0.7482 - val_loss: 0.5131 - val_accuracy: 0.7099
: Epoch 4/4
: 7/7 [==============================] - 0s 6ms/step - loss: 0.4878 - accuracy: 0.7355 - val_loss: 0.4845 - val_accuracy: 0.7343

#+begin_src jupyter-python
for layer in model_B_on_A.layers[:-1]:
    layer.trainable = True

model_B_on_A.compile(loss="binary_crossentropy",
                     optimizer=keras.optimizers.SGD(lr=1e-3),
                     metrics=['accuracy'])
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/16
7/7 [==============================] - 0s 18ms/step - loss: 0.4363 - accuracy: 0.7774 - val_loss: 0.3457 - val_accuracy: 0.8651
Epoch 2/16
7/7 [==============================] - 0s 6ms/step - loss: 0.2966 - accuracy: 0.9143 - val_loss: 0.2602 - val_accuracy: 0.9270
Epoch 3/16
7/7 [==============================] - 0s 5ms/step - loss: 0.2032 - accuracy: 0.9777 - val_loss: 0.2111 - val_accuracy: 0.9554
Epoch 4/16
7/7 [==============================] - 0s 5ms/step - loss: 0.1754 - accuracy: 0.9719 - val_loss: 0.1792 - val_accuracy: 0.9696
Epoch 5/16
7/7 [==============================] - 0s 6ms/step - loss: 0.1349 - accuracy: 0.9809 - val_loss: 0.1563 - val_accuracy: 0.9757
Epoch 6/16
7/7 [==============================] - 0s 5ms/step - loss: 0.1173 - accuracy: 0.9973 - val_loss: 0.1394 - val_accuracy: 0.9797
Epoch 7/16
7/7 [==============================] - 0s 6ms/step - loss: 0.1138 - accuracy: 0.9931 - val_loss: 0.1268 - val_accuracy: 0.9838
Epoch 8/16
7/7 [==============================] - 0s 5ms/step - loss: 0.1001 - accuracy: 0.9931 - val_loss: 0.1165 - val_accuracy: 0.9858
Epoch 9/16
7/7 [==============================] - 0s 5ms/step - loss: 0.0835 - accuracy: 1.0000 - val_loss: 0.1067 - val_accuracy: 0.9888
Epoch 10/16
7/7 [==============================] - 0s 6ms/step - loss: 0.0776 - accuracy: 1.0000 - val_loss: 0.1001 - val_accuracy: 0.9899
Epoch 11/16
7/7 [==============================] - 0s 5ms/step - loss: 0.0690 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 0.9899
Epoch 12/16
7/7 [==============================] - 0s 6ms/step - loss: 0.0720 - accuracy: 1.0000 - val_loss: 0.0889 - val_accuracy: 0.9899
Epoch 13/16
7/7 [==============================] - 0s 7ms/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 0.9899
Epoch 14/16
7/7 [==============================] - 0s 6ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9899
Epoch 15/16
7/7 [==============================] - 0s 6ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9899
Epoch 16/16
7/7 [==============================] - 0s 6ms/step - loss: 0.0473 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9899
#+end_example

#+begin_src jupyter-python
model_B.evaluate(X_test_B, y_test_B)
#+end_src

#+RESULTS:
:RESULTS:
: 63/63 [==============================] - 0s 586us/step - loss: 0.1408 - accuracy: 0.9705
| 0.1408407837152481 | 0.9704999923706055 |
:END:

#+begin_src jupyter-python
model_B_on_A.evaluate(X_test_B, y_test_B)
#+end_src

#+RESULTS:
:RESULTS:
: 63/63 [==============================] - 0s 688us/step - loss: 0.0683 - accuracy: 0.9935
| 0.06827671080827713 | 0.9934999942779541 |
:END:

Great! We got quite a bit of transfer: the error rate dropped by a factor of 4.5!

#+begin_src jupyter-python
(100 - 97.05) / (100 - 99.35)
#+end_src

#+RESULTS:
: 4.538461538461503


* Faster Optimizers

** Momentum optimization

#+begin_src jupyter-python
optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)
#+end_src

** Nesterov Accelerated Gradient

#+begin_src jupyter-python
optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)
#+end_src

** AdaGrad

#+begin_src jupyter-python
optimizer = keras.optimizers.Adagrad(lr=0.001)
#+end_src

** RMSProp

#+begin_src jupyter-python
optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)
#+end_src

** Adam Optimization

#+begin_src jupyter-python
optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
#+end_src

** Adamax Optimization

#+begin_src jupyter-python
optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)
#+end_src

** Nadam Optimization

#+begin_src jupyter-python
optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)
#+end_src


** Learning Rate Scheduling

*** Power scheduling
- lr = lr0 / (1 + steps / s) ** c
  Keras uses c=1 and s = 1/decay

#+begin_src jupyter-python
pow_opt = keras.optimizers.SGD(lr=0.01, decay=1e-4)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer=pow_opt, metrics=["accuracy"])

n_epochs = 25
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/25
1719/1719 [==============================] - 2s 922us/step - loss: 0.5951 - accuracy: 0.7920 - val_loss: 0.4029 - val_accuracy: 0.8634
Epoch 2/25
1719/1719 [==============================] - 1s 704us/step - loss: 0.3805 - accuracy: 0.8637 - val_loss: 0.3760 - val_accuracy: 0.8672
Epoch 3/25
1719/1719 [==============================] - 1s 695us/step - loss: 0.3473 - accuracy: 0.8771 - val_loss: 0.3779 - val_accuracy: 0.8726
Epoch 4/25
1719/1719 [==============================] - 1s 705us/step - loss: 0.3261 - accuracy: 0.8820 - val_loss: 0.3540 - val_accuracy: 0.8772
Epoch 5/25
1719/1719 [==============================] - 1s 684us/step - loss: 0.3153 - accuracy: 0.8878 - val_loss: 0.3487 - val_accuracy: 0.8750
Epoch 6/25
1719/1719 [==============================] - 1s 695us/step - loss: 0.2897 - accuracy: 0.8962 - val_loss: 0.3459 - val_accuracy: 0.8794
Epoch 7/25
1719/1719 [==============================] - 1s 695us/step - loss: 0.2845 - accuracy: 0.8986 - val_loss: 0.3410 - val_accuracy: 0.8828
Epoch 8/25
1719/1719 [==============================] - 1s 732us/step - loss: 0.2688 - accuracy: 0.9041 - val_loss: 0.3447 - val_accuracy: 0.8784
Epoch 9/25
1719/1719 [==============================] - 1s 700us/step - loss: 0.2704 - accuracy: 0.9015 - val_loss: 0.3350 - val_accuracy: 0.8850
Epoch 10/25
1719/1719 [==============================] - 1s 667us/step - loss: 0.2554 - accuracy: 0.9078 - val_loss: 0.3295 - val_accuracy: 0.8838
Epoch 11/25
1719/1719 [==============================] - 1s 647us/step - loss: 0.2486 - accuracy: 0.9117 - val_loss: 0.3307 - val_accuracy: 0.8834
Epoch 12/25
1719/1719 [==============================] - 1s 648us/step - loss: 0.2437 - accuracy: 0.9133 - val_loss: 0.3367 - val_accuracy: 0.8802
Epoch 13/25
1719/1719 [==============================] - 1s 645us/step - loss: 0.2375 - accuracy: 0.9172 - val_loss: 0.3291 - val_accuracy: 0.8866
Epoch 14/25
1719/1719 [==============================] - 1s 642us/step - loss: 0.2337 - accuracy: 0.9170 - val_loss: 0.3306 - val_accuracy: 0.8852
Epoch 15/25
1719/1719 [==============================] - 1s 646us/step - loss: 0.2315 - accuracy: 0.9182 - val_loss: 0.3265 - val_accuracy: 0.8866
Epoch 16/25
1719/1719 [==============================] - 1s 646us/step - loss: 0.2270 - accuracy: 0.9197 - val_loss: 0.3257 - val_accuracy: 0.8882
Epoch 17/25
1719/1719 [==============================] - 1s 641us/step - loss: 0.2196 - accuracy: 0.9235 - val_loss: 0.3276 - val_accuracy: 0.8876
Epoch 18/25
1719/1719 [==============================] - 1s 639us/step - loss: 0.2190 - accuracy: 0.9233 - val_loss: 0.3244 - val_accuracy: 0.8874
Epoch 19/25
1719/1719 [==============================] - 1s 646us/step - loss: 0.2189 - accuracy: 0.9233 - val_loss: 0.3262 - val_accuracy: 0.8874
Epoch 20/25
1719/1719 [==============================] - 1s 641us/step - loss: 0.2174 - accuracy: 0.9244 - val_loss: 0.3241 - val_accuracy: 0.8884
Epoch 21/25
1719/1719 [==============================] - 1s 641us/step - loss: 0.2140 - accuracy: 0.9259 - val_loss: 0.3251 - val_accuracy: 0.8896
Epoch 22/25
1719/1719 [==============================] - 1s 642us/step - loss: 0.2121 - accuracy: 0.9265 - val_loss: 0.3224 - val_accuracy: 0.8902
Epoch 23/25
1719/1719 [==============================] - 1s 638us/step - loss: 0.2080 - accuracy: 0.9260 - val_loss: 0.3236 - val_accuracy: 0.8894
Epoch 24/25
1719/1719 [==============================] - 1s 635us/step - loss: 0.2019 - accuracy: 0.9310 - val_loss: 0.3249 - val_accuracy: 0.8912
Epoch 25/25
1719/1719 [==============================] - 1s 638us/step - loss: 0.2053 - accuracy: 0.9289 - val_loss: 0.3244 - val_accuracy: 0.8916
#+end_example

#+begin_src jupyter-python
learning_rate = 0.01
decay = 1e-4
batch_size = 32
n_steps_per_epoch = math.ceil(len(X_train) / batch_size)
epochs = np.arange(n_epochs)
lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)

plt.plot(epochs, lrs,  "o-")
plt.axis([0, n_epochs - 1, 0, 0.01])
plt.xlabel("Epoch")
plt.ylabel("Learning Rate")
plt.title("Power Scheduling", fontsize=14)
plt.grid(True);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b01b0df7a7da2ff187fba2b03cc2fe869c2b05df.png]]

*** Exponential Scheduling

- lr = lr0 * 0.1 ** (epoch / s)

#+begin_src jupyter-python
def exponential_decay_fn(epoch):
    return 0.01 * 0.1**(epoch / 20)

def exponential_decay(lr0, s):
    def exponential_decay_fn(epoch):
        return lr0 * 0.1**(epoch / s)
    return exponential_decay_fn

exponential_decay_fn = exponential_decay(lr0=0.01, s=20)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])

n_epochs = 25
lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[lr_scheduler])
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/25
1719/1719 [==============================] - 3s 2ms/step - loss: 1.1501 - accuracy: 0.7354 - val_loss: 0.9740 - val_accuracy: 0.7116
Epoch 2/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.7139 - accuracy: 0.7796 - val_loss: 0.6105 - val_accuracy: 0.7670
Epoch 3/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.6342 - accuracy: 0.8059 - val_loss: 1.1470 - val_accuracy: 0.7172
Epoch 4/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.7101 - accuracy: 0.8028 - val_loss: 0.5860 - val_accuracy: 0.8312
Epoch 5/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5496 - accuracy: 0.8368 - val_loss: 0.7801 - val_accuracy: 0.8530
Epoch 6/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5128 - accuracy: 0.8497 - val_loss: 0.5066 - val_accuracy: 0.8522
Epoch 7/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4476 - accuracy: 0.8650 - val_loss: 0.5334 - val_accuracy: 0.8504
Epoch 8/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4222 - accuracy: 0.8689 - val_loss: 0.6068 - val_accuracy: 0.8476
Epoch 9/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4048 - accuracy: 0.8749 - val_loss: 0.5392 - val_accuracy: 0.8096
Epoch 10/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3577 - accuracy: 0.8853 - val_loss: 0.5620 - val_accuracy: 0.8662
Epoch 11/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3366 - accuracy: 0.8938 - val_loss: 0.5156 - val_accuracy: 0.8660
Epoch 12/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3125 - accuracy: 0.8979 - val_loss: 0.5062 - val_accuracy: 0.8614
Epoch 13/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3046 - accuracy: 0.9030 - val_loss: 0.4481 - val_accuracy: 0.8708
Epoch 14/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2744 - accuracy: 0.9106 - val_loss: 0.4473 - val_accuracy: 0.8740
Epoch 15/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2701 - accuracy: 0.9144 - val_loss: 0.4478 - val_accuracy: 0.8764
Epoch 16/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2497 - accuracy: 0.9200 - val_loss: 0.4853 - val_accuracy: 0.8744
Epoch 17/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2305 - accuracy: 0.9270 - val_loss: 0.4807 - val_accuracy: 0.8730
Epoch 18/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2235 - accuracy: 0.9284 - val_loss: 0.4815 - val_accuracy: 0.8752
Epoch 19/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2090 - accuracy: 0.9332 - val_loss: 0.4846 - val_accuracy: 0.8766
Epoch 20/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2048 - accuracy: 0.9340 - val_loss: 0.5139 - val_accuracy: 0.8790
Epoch 21/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.1921 - accuracy: 0.9378 - val_loss: 0.5341 - val_accuracy: 0.8774
Epoch 22/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.1806 - accuracy: 0.9420 - val_loss: 0.5456 - val_accuracy: 0.8760
Epoch 23/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.1759 - accuracy: 0.9438 - val_loss: 0.5550 - val_accuracy: 0.8782
Epoch 24/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.1651 - accuracy: 0.9484 - val_loss: 0.5752 - val_accuracy: 0.8796
Epoch 25/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.1648 - accuracy: 0.9470 - val_loss: 0.5883 - val_accuracy: 0.8804
#+end_example

#+begin_src jupyter-python
plt.plot(history.epoch, history.history["lr"], "o-")
plt.axis([0, n_epochs - 1, 0, 0.011])
plt.xlabel("Epoch")
plt.ylabel("Learning Rate")
plt.title("Exponential Scheduling", fontsize=14)
plt.grid(True);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/51b019f59211acb6116840be0c7e526f51bd12e6.png]]


The schedule function can take the current learning rate as a second argument:

#+begin_src jupyter-python
def exponential_decay_fn(epoch, lr):
    return lr * 0.1**(1 / 20)
#+end_src

#+RESULTS:

If you want to update the learning rate at each iteration rather than at each epoch, you must write your own callback class:

#+begin_src jupyter-python
K = keras.backend

class ExponentialDecay(keras.callbacks.Callback):
    def __init__(self, s=40000):
        super().__init__()
        self.s = s

    def on_batch_begin(self, batch, logs=None):
        # Note: the `batch` argument is reset at each epoch
        lr = K.get_value(self.model.optimizer.lr)
        K.set_value(self.model.optimizer.lr, lr * 0.1**(1 / s))

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = K.get_value(self.model.optimizer.lr)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
lr0 = 0.01
optimizer = keras.optimizers.Nadam(lr=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
n_epochs = 25

s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)
exp_decay = ExponentialDecay(s)
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[exp_decay])
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/25
1719/1719 [==============================] - 4s 2ms/step - loss: 1.1020 - accuracy: 0.7356 - val_loss: 0.7976 - val_accuracy: 0.7600
Epoch 2/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.6361 - accuracy: 0.8005 - val_loss: 0.5933 - val_accuracy: 0.8150
Epoch 3/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5696 - accuracy: 0.8240 - val_loss: 0.8239 - val_accuracy: 0.7894
Epoch 4/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5517 - accuracy: 0.8272 - val_loss: 0.4806 - val_accuracy: 0.8544
Epoch 5/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.4789 - accuracy: 0.8473 - val_loss: 0.4781 - val_accuracy: 0.8634
Epoch 6/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.4119 - accuracy: 0.8664 - val_loss: 0.4593 - val_accuracy: 0.8616
Epoch 7/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4087 - accuracy: 0.8716 - val_loss: 0.4500 - val_accuracy: 0.8636
Epoch 8/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3525 - accuracy: 0.8838 - val_loss: 0.4630 - val_accuracy: 0.8630
Epoch 9/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.3387 - accuracy: 0.8874 - val_loss: 0.4526 - val_accuracy: 0.8694
Epoch 10/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.3138 - accuracy: 0.8967 - val_loss: 0.4073 - val_accuracy: 0.8834
Epoch 11/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2791 - accuracy: 0.9041 - val_loss: 0.4556 - val_accuracy: 0.8716
Epoch 12/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2604 - accuracy: 0.9098 - val_loss: 0.4614 - val_accuracy: 0.8748
Epoch 13/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.2506 - accuracy: 0.9159 - val_loss: 0.4409 - val_accuracy: 0.8830
Epoch 14/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2217 - accuracy: 0.9236 - val_loss: 0.4581 - val_accuracy: 0.8776
Epoch 15/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2207 - accuracy: 0.9258 - val_loss: 0.4143 - val_accuracy: 0.8870
Epoch 16/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.1973 - accuracy: 0.9319 - val_loss: 0.4354 - val_accuracy: 0.8900
Epoch 17/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.1799 - accuracy: 0.9386 - val_loss: 0.4670 - val_accuracy: 0.8872
Epoch 18/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.1710 - accuracy: 0.9421 - val_loss: 0.4785 - val_accuracy: 0.8914
Epoch 19/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1555 - accuracy: 0.9485 - val_loss: 0.4872 - val_accuracy: 0.8916
Epoch 20/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.1500 - accuracy: 0.9492 - val_loss: 0.4919 - val_accuracy: 0.8850
Epoch 21/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1373 - accuracy: 0.9537 - val_loss: 0.5454 - val_accuracy: 0.8884
Epoch 22/25
1719/1719 [==============================] - 5s 3ms/step - loss: 0.1321 - accuracy: 0.9558 - val_loss: 0.5314 - val_accuracy: 0.8930
Epoch 23/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1179 - accuracy: 0.9611 - val_loss: 0.5643 - val_accuracy: 0.8908
Epoch 24/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.1098 - accuracy: 0.9645 - val_loss: 0.5999 - val_accuracy: 0.8934
Epoch 25/25
1719/1719 [==============================] - 4s 3ms/step - loss: 0.1066 - accuracy: 0.9663 - val_loss: 0.6234 - val_accuracy: 0.8908
#+end_example

#+begin_src jupyter-python
n_steps = n_epochs * len(X_train) // 32
steps = np.arange(n_steps)
lrs = lr0 * 0.1**(steps / s)

plt.plot(steps, lrs, "-", linewidth=2)
plt.axis([0, n_steps - 1, 0, lr0 * 1.1])
plt.xlabel("Batch")
plt.ylabel("Learning Rate")
plt.title("Exponential Scheduling (per batch)", fontsize=14)
plt.grid(True);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/54a9882f5e5ce664875f1aff40adb714cd232f4e.png]]

*** Piecewise constant scheduling

#+begin_src jupyter-python
def piecewise_constant_fn(epoch):
    if epoch < 5:
        return 0.01
    elif epoch < 15:
        return 0.005
    else:
        return 0.001

def piecewise_constant(boundaries, values):
    boundaries = np.array([0] + boundaries)
    values = np.array(values)
    def piecewise_constant_fn(epoch):
        return values[np.argmax(boundaries > epoch) - 1]
    return piecewise_constant_fn

piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])

lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 25
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[lr_scheduler])
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/25
1719/1719 [==============================] - 3s 2ms/step - loss: 1.1239 - accuracy: 0.7351 - val_loss: 0.8650 - val_accuracy: 0.7080
Epoch 2/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.8446 - accuracy: 0.7580 - val_loss: 0.7680 - val_accuracy: 0.7328
Epoch 3/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.7920 - accuracy: 0.7689 - val_loss: 1.8509 - val_accuracy: 0.5046
Epoch 4/25
1719/1719 [==============================] - 3s 2ms/step - loss: 1.0224 - accuracy: 0.6910 - val_loss: 1.1396 - val_accuracy: 0.6466
Epoch 5/25
1719/1719 [==============================] - 3s 2ms/step - loss: 1.0519 - accuracy: 0.6627 - val_loss: 1.3539 - val_accuracy: 0.6090
Epoch 6/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.7559 - accuracy: 0.7260 - val_loss: 0.7038 - val_accuracy: 0.7456
Epoch 7/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.6545 - accuracy: 0.7535 - val_loss: 0.7164 - val_accuracy: 0.7494
Epoch 8/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.6489 - accuracy: 0.7577 - val_loss: 0.6548 - val_accuracy: 0.7660
Epoch 9/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.6364 - accuracy: 0.7634 - val_loss: 0.7035 - val_accuracy: 0.7534
Epoch 10/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.6127 - accuracy: 0.7711 - val_loss: 0.7100 - val_accuracy: 0.7654
Epoch 11/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5924 - accuracy: 0.7757 - val_loss: 0.6904 - val_accuracy: 0.7666
Epoch 12/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.6031 - accuracy: 0.7759 - val_loss: 0.7547 - val_accuracy: 0.7584
Epoch 13/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5844 - accuracy: 0.7819 - val_loss: 0.7526 - val_accuracy: 0.7510
Epoch 14/25
1719/1719 [==============================] - 4s 2ms/step - loss: 0.5699 - accuracy: 0.7874 - val_loss: 0.7517 - val_accuracy: 0.7506
Epoch 15/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.5905 - accuracy: 0.7770 - val_loss: 0.7682 - val_accuracy: 0.7742
Epoch 16/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4787 - accuracy: 0.8072 - val_loss: 0.6258 - val_accuracy: 0.7972
Epoch 17/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4260 - accuracy: 0.8316 - val_loss: 0.6084 - val_accuracy: 0.8242
Epoch 18/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3964 - accuracy: 0.8638 - val_loss: 0.5432 - val_accuracy: 0.8612
Epoch 19/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3539 - accuracy: 0.8879 - val_loss: 0.5558 - val_accuracy: 0.8596
Epoch 20/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3354 - accuracy: 0.8929 - val_loss: 0.5475 - val_accuracy: 0.8690
Epoch 21/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3228 - accuracy: 0.8991 - val_loss: 0.5833 - val_accuracy: 0.8654
Epoch 22/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3118 - accuracy: 0.9027 - val_loss: 0.5757 - val_accuracy: 0.8720
Epoch 23/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3032 - accuracy: 0.9040 - val_loss: 0.5601 - val_accuracy: 0.8646
Epoch 24/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2842 - accuracy: 0.9083 - val_loss: 0.5585 - val_accuracy: 0.8716
Epoch 25/25
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2917 - accuracy: 0.9077 - val_loss: 0.5934 - val_accuracy: 0.8724
#+end_example

#+begin_src jupyter-python
plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], "o-")
plt.axis([0, n_epochs - 1, 0, 0.011])
plt.xlabel("Epoch")
plt.ylabel("Learning Rate")
#+end_src

#+RESULTS:
:RESULTS:
: Text(0, 0.5, 'Learning Rate')
[[file:./.ob-jupyter/fa62ca9b69fb3956bdcfe48c99d0840ac6ad8a8c.png]]
:END:

*** Performance scheduling

#+begin_src jupyter-python
tf.random.set_seed(42)
np.random.seed(42)

lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
n_epochs = 25
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[lr_scheduler])
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/25
1719/1719 [==============================] - 1s 792us/step - loss: 0.7106 - accuracy: 0.7771 - val_loss: 0.5125 - val_accuracy: 0.8486
Epoch 2/25
1719/1719 [==============================] - 1s 765us/step - loss: 0.4918 - accuracy: 0.8382 - val_loss: 0.6228 - val_accuracy: 0.8306
Epoch 3/25
1719/1719 [==============================] - 1s 721us/step - loss: 0.5085 - accuracy: 0.8422 - val_loss: 0.4834 - val_accuracy: 0.8558
Epoch 4/25
1719/1719 [==============================] - 1s 818us/step - loss: 0.5015 - accuracy: 0.8487 - val_loss: 0.5006 - val_accuracy: 0.8514
Epoch 5/25
1719/1719 [==============================] - 2s 879us/step - loss: 0.5169 - accuracy: 0.8465 - val_loss: 0.6444 - val_accuracy: 0.8280
Epoch 6/25
1719/1719 [==============================] - 1s 713us/step - loss: 0.4875 - accuracy: 0.8577 - val_loss: 0.5703 - val_accuracy: 0.8554
Epoch 7/25
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5200 - accuracy: 0.8564 - val_loss: 0.6182 - val_accuracy: 0.8408
Epoch 8/25
1719/1719 [==============================] - 1s 751us/step - loss: 0.5094 - accuracy: 0.8570 - val_loss: 0.6273 - val_accuracy: 0.8302
Epoch 9/25
1719/1719 [==============================] - 1s 717us/step - loss: 0.3206 - accuracy: 0.8900 - val_loss: 0.3824 - val_accuracy: 0.8822
Epoch 10/25
1719/1719 [==============================] - 1s 696us/step - loss: 0.2464 - accuracy: 0.9134 - val_loss: 0.3941 - val_accuracy: 0.8870
Epoch 11/25
1719/1719 [==============================] - 1s 699us/step - loss: 0.2153 - accuracy: 0.9206 - val_loss: 0.4201 - val_accuracy: 0.8808
Epoch 12/25
1719/1719 [==============================] - 1s 701us/step - loss: 0.2053 - accuracy: 0.9252 - val_loss: 0.4491 - val_accuracy: 0.8792
Epoch 13/25
1719/1719 [==============================] - 1s 710us/step - loss: 0.1906 - accuracy: 0.9283 - val_loss: 0.4708 - val_accuracy: 0.8858
Epoch 14/25
1719/1719 [==============================] - 1s 764us/step - loss: 0.1784 - accuracy: 0.9321 - val_loss: 0.4560 - val_accuracy: 0.8758
Epoch 15/25
1719/1719 [==============================] - 1s 717us/step - loss: 0.1416 - accuracy: 0.9447 - val_loss: 0.4131 - val_accuracy: 0.8884
Epoch 16/25
1719/1719 [==============================] - 1s 703us/step - loss: 0.1189 - accuracy: 0.9533 - val_loss: 0.4473 - val_accuracy: 0.8898
Epoch 17/25
1719/1719 [==============================] - 1s 704us/step - loss: 0.1060 - accuracy: 0.9583 - val_loss: 0.4570 - val_accuracy: 0.8928
Epoch 18/25
1719/1719 [==============================] - 1s 720us/step - loss: 0.1012 - accuracy: 0.9602 - val_loss: 0.4706 - val_accuracy: 0.8914
Epoch 19/25
1719/1719 [==============================] - 1s 731us/step - loss: 0.0951 - accuracy: 0.9640 - val_loss: 0.4905 - val_accuracy: 0.8916
Epoch 20/25
1719/1719 [==============================] - 1s 733us/step - loss: 0.0802 - accuracy: 0.9706 - val_loss: 0.4804 - val_accuracy: 0.8924
Epoch 21/25
1719/1719 [==============================] - 1s 745us/step - loss: 0.0706 - accuracy: 0.9737 - val_loss: 0.4920 - val_accuracy: 0.8926
Epoch 22/25
1719/1719 [==============================] - 1s 740us/step - loss: 0.0687 - accuracy: 0.9745 - val_loss: 0.5083 - val_accuracy: 0.8926
Epoch 23/25
1719/1719 [==============================] - 1s 725us/step - loss: 0.0645 - accuracy: 0.9773 - val_loss: 0.5073 - val_accuracy: 0.8922
Epoch 24/25
1719/1719 [==============================] - 1s 728us/step - loss: 0.0598 - accuracy: 0.9787 - val_loss: 0.5262 - val_accuracy: 0.8928
Epoch 25/25
1719/1719 [==============================] - 1s 726us/step - loss: 0.0548 - accuracy: 0.9811 - val_loss: 0.5355 - val_accuracy: 0.8920
#+end_example

#+begin_src jupyter-python
plt.plot(history.epoch, history.history["lr"], "bo-")
plt.xlabel("Epoch")
plt.ylabel("Learning Rate", color='b')
plt.tick_params('y', colors='b')
plt.gca().set_xlim(0, n_epochs - 1)
plt.grid(True)

ax2 = plt.gca().twinx()
ax2.plot(history.epoch, history.history["val_loss"], "r^-")
ax2.set_ylabel('Validation Loss', color='r')
ax2.tick_params('y', colors='r')

plt.title("Reduce LR on Plateau", fontsize=14);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/baac3b4f06c15fd013520809b46d2a4cc49d1b3a.png]]

*** tf.keras schedulers

#+begin_src jupyter-python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)
learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)
optimizer = keras.optimizers.SGD(learning_rate)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
n_epochs = 25
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/25
1719/1719 [==============================] - 1s 692us/step - loss: 0.5995 - accuracy: 0.7924 - val_loss: 0.4094 - val_accuracy: 0.8598
Epoch 2/25
1719/1719 [==============================] - 1s 740us/step - loss: 0.3889 - accuracy: 0.8613 - val_loss: 0.3739 - val_accuracy: 0.8694
Epoch 3/25
1719/1719 [==============================] - 1s 808us/step - loss: 0.3530 - accuracy: 0.8775 - val_loss: 0.3728 - val_accuracy: 0.8694
Epoch 4/25
1719/1719 [==============================] - 1s 645us/step - loss: 0.3296 - accuracy: 0.8814 - val_loss: 0.3492 - val_accuracy: 0.8810
Epoch 5/25
1719/1719 [==============================] - 1s 641us/step - loss: 0.3177 - accuracy: 0.8867 - val_loss: 0.3429 - val_accuracy: 0.8796
Epoch 6/25
1719/1719 [==============================] - 1s 791us/step - loss: 0.2929 - accuracy: 0.8957 - val_loss: 0.3411 - val_accuracy: 0.8826
Epoch 7/25
1719/1719 [==============================] - 1s 643us/step - loss: 0.2853 - accuracy: 0.8989 - val_loss: 0.3352 - val_accuracy: 0.8798
Epoch 8/25
1719/1719 [==============================] - 1s 717us/step - loss: 0.2711 - accuracy: 0.9043 - val_loss: 0.3364 - val_accuracy: 0.8810
Epoch 9/25
1719/1719 [==============================] - 1s 653us/step - loss: 0.2713 - accuracy: 0.9047 - val_loss: 0.3261 - val_accuracy: 0.8854
Epoch 10/25
1719/1719 [==============================] - 1s 637us/step - loss: 0.2569 - accuracy: 0.9086 - val_loss: 0.3235 - val_accuracy: 0.8854
Epoch 11/25
1719/1719 [==============================] - 1s 643us/step - loss: 0.2500 - accuracy: 0.9111 - val_loss: 0.3246 - val_accuracy: 0.8866
Epoch 12/25
1719/1719 [==============================] - 1s 808us/step - loss: 0.2452 - accuracy: 0.9145 - val_loss: 0.3297 - val_accuracy: 0.8816
Epoch 13/25
1719/1719 [==============================] - 1s 648us/step - loss: 0.2407 - accuracy: 0.9153 - val_loss: 0.3216 - val_accuracy: 0.8868
Epoch 14/25
1719/1719 [==============================] - 1s 646us/step - loss: 0.2377 - accuracy: 0.9163 - val_loss: 0.3218 - val_accuracy: 0.8860
Epoch 15/25
1719/1719 [==============================] - 1s 659us/step - loss: 0.2375 - accuracy: 0.9167 - val_loss: 0.3205 - val_accuracy: 0.8884
Epoch 16/25
1719/1719 [==============================] - 1s 650us/step - loss: 0.2315 - accuracy: 0.9192 - val_loss: 0.3180 - val_accuracy: 0.8894
Epoch 17/25
1719/1719 [==============================] - 1s 653us/step - loss: 0.2264 - accuracy: 0.9213 - val_loss: 0.3195 - val_accuracy: 0.8898
Epoch 18/25
1719/1719 [==============================] - 1s 645us/step - loss: 0.2284 - accuracy: 0.9192 - val_loss: 0.3165 - val_accuracy: 0.8904
Epoch 19/25
1719/1719 [==============================] - 1s 646us/step - loss: 0.2284 - accuracy: 0.9211 - val_loss: 0.3194 - val_accuracy: 0.8890
Epoch 20/25
1719/1719 [==============================] - 1s 644us/step - loss: 0.2285 - accuracy: 0.9217 - val_loss: 0.3166 - val_accuracy: 0.8910
Epoch 21/25
1719/1719 [==============================] - 1s 657us/step - loss: 0.2264 - accuracy: 0.9214 - val_loss: 0.3176 - val_accuracy: 0.8910
Epoch 22/25
1719/1719 [==============================] - 1s 658us/step - loss: 0.2255 - accuracy: 0.9205 - val_loss: 0.3160 - val_accuracy: 0.8912
Epoch 23/25
1719/1719 [==============================] - 1s 651us/step - loss: 0.2221 - accuracy: 0.9231 - val_loss: 0.3167 - val_accuracy: 0.8896
Epoch 24/25
1719/1719 [==============================] - 1s 641us/step - loss: 0.2180 - accuracy: 0.9246 - val_loss: 0.3163 - val_accuracy: 0.8912
Epoch 25/25
1719/1719 [==============================] - 1s 645us/step - loss: 0.2221 - accuracy: 0.9224 - val_loss: 0.3162 - val_accuracy: 0.8912
#+end_example

For piecewise constant scheduling, try this:

#+begin_src jupyter-python
learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch],
    values=[0.01, 0.005, 0.001])
#+end_src


*** 1Cycle scheduling

#+begin_src jupyter-python
K = keras.backend


class ExponentialLearningRate(keras.callbacks.Callback):
    def __init__(self, factor):
        self.factor = factor
        self.rates = []
        self.losses = []

    def on_batch_end(self, batch, logs):
        self.rates.append(K.get_value(self.model.optimizer.lr))
        self.losses.append(logs["loss"])
        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)


def find_learning_rate(
    model, X, y, epochs=1, batch_size=32, min_rate=10 ** -5, max_rate=10
):
    init_weights = model.get_weights()
    iterations = math.ceil(len(X) / batch_size) * epochs
    factor = np.exp(np.log(max_rate / min_rate) / iterations)
    init_lr = K.get_value(model.optimizer.lr)
    K.set_value(model.optimizer.lr, min_rate)
    exp_lr = ExponentialLearningRate(factor)
    history = model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[exp_lr])
    K.set_value(model.optimizer.lr, init_lr)
    model.set_weights(init_weights)
    return exp_lr.rates, exp_lr.losses


def plot_lr_vs_loss(rates, losses):
    plt.plot(rates, losses)
    plt.gca().set_xscale("log")
    plt.hlines(min(losses), min(rates), max(rates))
    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])
    plt.xlabel("Learning rate")
    plt.ylabel("Loss")
#+end_src

#+RESULTS:

Warning: In the on_batch_end() method, logs["loss"] used to contain the batch loss, but in TensorFlow 2.2.0 it was replaced with the mean loss (since the start of the epoch). This explains why the graph below is much smoother than in the book (if you are using TF 2.2 or above). It also means that there is a lag between the moment the batch loss starts exploding and the moment the explosion becomes clear in the graph. So you should choose a slightly smaller learning rate than you would have chosen with the "noisy" graph. Alternatively, you can tweak the ExponentialLearningRate callback above so it computes the batch loss (based on the current mean loss and the previous mean loss):

#+begin_src jupyter-python
class ExponentialLearningRate(keras.callbacks.Callback):
    def __init__(self, factor):
        self.factor = factor
        self.rates = []
        self.losses = []
    def on_epoch_begin(self, epoch, logs=None):
        self.prev_loss = 0
    def on_batch_end(self, batch, logs=None):
        batch_loss = logs["loss"] * (batch + 1) - self.prev_loss * batch
        self.prev_loss = logs["loss"]
        self.rates.append(K.get_value(self.model.optimizer.lr))
        self.losses.append(batch_loss)
        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)
#+end_src

#+begin_src jupyter-python
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=keras.optimizers.SGD(lr=1e-3),
              metrics=["accuracy"])

batch_size = 128
rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)
plot_lr_vs_loss(rates, losses)
#+end_src

#+RESULTS:
:RESULTS:
: 430/430 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.3117
[[file:./.ob-jupyter/a7f5df4e1725992e3533a21c9b11609fcc741881.png]]
:END:

#+begin_src jupyter-python
class OneCycleScheduler(keras.callbacks.Callback):
    def __init__(self, iterations, max_rate, start_rate=None,
                 last_iterations=None, last_rate=None):
        self.iterations = iterations
        self.max_rate = max_rate
        self.start_rate = start_rate or max_rate / 10
        self.last_iterations = last_iterations or iterations // 10 + 1
        self.half_iteration = (iterations - self.last_iterations) // 2
        self.last_rate = last_rate or self.start_rate / 1000
        self.iteration = 0

    def _interpolate(self, iter1, iter2, rate1, rate2):
        return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)

    def on_batch_begin(self, batch, logs):
        if self.iteration < self.half_iteration:
            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)
        elif self.iteration < 2 * self.half_iteration:
            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_rate, self.start_rate)
        else:
            rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_rate, self.last_rate)
        self.iteration += 1
        K.set_value(self.model.optimizer.lr, rate)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
n_epochs = 25
onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.05)
history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,
                    validation_data=(X_valid_scaled, y_valid), callbacks=[onecycle])
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/25
430/430 [==============================] - 1s 2ms/step - loss: 0.6572 - accuracy: 0.7740 - val_loss: 0.4872 - val_accuracy: 0.8336
Epoch 2/25
430/430 [==============================] - 1s 1ms/step - loss: 0.4581 - accuracy: 0.8396 - val_loss: 0.4275 - val_accuracy: 0.8524
Epoch 3/25
430/430 [==============================] - 1s 1ms/step - loss: 0.4122 - accuracy: 0.8545 - val_loss: 0.4115 - val_accuracy: 0.8582
Epoch 4/25
430/430 [==============================] - 1s 1ms/step - loss: 0.3837 - accuracy: 0.8642 - val_loss: 0.3869 - val_accuracy: 0.8688
Epoch 5/25
430/430 [==============================] - 1s 1ms/step - loss: 0.3639 - accuracy: 0.8717 - val_loss: 0.3765 - val_accuracy: 0.8684
Epoch 6/25
430/430 [==============================] - 1s 1ms/step - loss: 0.3457 - accuracy: 0.8775 - val_loss: 0.3743 - val_accuracy: 0.8698
Epoch 7/25
430/430 [==============================] - 1s 1ms/step - loss: 0.3330 - accuracy: 0.8809 - val_loss: 0.3633 - val_accuracy: 0.8706
Epoch 8/25
430/430 [==============================] - 1s 1ms/step - loss: 0.3184 - accuracy: 0.8860 - val_loss: 0.3947 - val_accuracy: 0.8618
Epoch 9/25
430/430 [==============================] - 1s 1ms/step - loss: 0.3065 - accuracy: 0.8888 - val_loss: 0.3493 - val_accuracy: 0.8762
Epoch 10/25
430/430 [==============================] - 1s 1ms/step - loss: 0.2945 - accuracy: 0.8922 - val_loss: 0.3402 - val_accuracy: 0.8790
Epoch 11/25
430/430 [==============================] - 1s 1ms/step - loss: 0.2840 - accuracy: 0.8960 - val_loss: 0.3456 - val_accuracy: 0.8810
Epoch 12/25
430/430 [==============================] - 1s 1ms/step - loss: 0.2710 - accuracy: 0.9020 - val_loss: 0.3657 - val_accuracy: 0.8696
Epoch 13/25
430/430 [==============================] - 1s 1ms/step - loss: 0.2538 - accuracy: 0.9080 - val_loss: 0.3357 - val_accuracy: 0.8832
Epoch 14/25
430/430 [==============================] - 1s 1ms/step - loss: 0.2406 - accuracy: 0.9133 - val_loss: 0.3461 - val_accuracy: 0.8794
Epoch 15/25
430/430 [==============================] - 1s 1ms/step - loss: 0.2280 - accuracy: 0.9182 - val_loss: 0.3260 - val_accuracy: 0.8844
Epoch 16/25
430/430 [==============================] - 1s 1ms/step - loss: 0.2160 - accuracy: 0.9231 - val_loss: 0.3298 - val_accuracy: 0.8836
Epoch 17/25
430/430 [==============================] - 1s 2ms/step - loss: 0.2063 - accuracy: 0.9264 - val_loss: 0.3349 - val_accuracy: 0.8872
Epoch 18/25
430/430 [==============================] - 1s 1ms/step - loss: 0.1979 - accuracy: 0.9302 - val_loss: 0.3247 - val_accuracy: 0.8902
Epoch 19/25
430/430 [==============================] - 1s 1ms/step - loss: 0.1892 - accuracy: 0.9343 - val_loss: 0.3237 - val_accuracy: 0.8902
Epoch 20/25
430/430 [==============================] - 1s 1ms/step - loss: 0.1821 - accuracy: 0.9371 - val_loss: 0.3228 - val_accuracy: 0.8926
Epoch 21/25
430/430 [==============================] - 1s 1ms/step - loss: 0.1752 - accuracy: 0.9402 - val_loss: 0.3223 - val_accuracy: 0.8916
Epoch 22/25
430/430 [==============================] - 1s 1ms/step - loss: 0.1700 - accuracy: 0.9419 - val_loss: 0.3184 - val_accuracy: 0.8946
Epoch 23/25
430/430 [==============================] - 1s 1ms/step - loss: 0.1654 - accuracy: 0.9439 - val_loss: 0.3190 - val_accuracy: 0.8940
Epoch 24/25
430/430 [==============================] - 1s 1ms/step - loss: 0.1626 - accuracy: 0.9457 - val_loss: 0.3181 - val_accuracy: 0.8940
Epoch 25/25
430/430 [==============================] - 1s 1ms/step - loss: 0.1609 - accuracy: 0.9464 - val_loss: 0.3174 - val_accuracy: 0.8944
#+end_example


* Avoiding Overfitting Through Regularization

** $\mathcal{l}_1$ and $\mathcal{l}_2$ regularization

#+begin_src jupyter-python
layer = keras.layers.Dense(100, activation="elu",
                          kernel_initializer="he_normal",
                          kernel_regularizer=keras.regularizers.l2(0.01))
# or l1(0.1) for â1 regularization with a factor or 0.1
# or l1_l2(0.1, 0.01) for both â1 and â2 regularization, with factors 0.1 and 0.01 respectively
#+end_src

#+RESULTS:

#+begin_src jupyter-python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="elu",
                       kernel_initializer="he_normal",
                       kernel_regularizer=keras.regularizers.l2(0.01)),
    keras.layers.Dense(100, activation="elu",
                       kernel_initializer="he_normal",
                       kernel_regularizer=keras.regularizers.l2(0.01)),
    keras.layers.Dense(10, activation="softmax",
                       kernel_regularizer=keras.regularizers.l2(0.01))
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 2
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
#+end_src

#+RESULTS:
: Epoch 1/2
: 1719/1719 [==============================] - 4s 2ms/step - loss: 3.3713 - accuracy: 0.7922 - val_loss: 0.7200 - val_accuracy: 0.8324
: Epoch 2/2
: 1719/1719 [==============================] - 3s 2ms/step - loss: 0.7285 - accuracy: 0.8247 - val_loss: 0.6826 - val_accuracy: 0.8376

#+begin_src jupyter-python
from functools import partial

RegularizedDense = partial(keras.layers.Dense,
                           activation="elu",
                           kernel_initializer="he_normal",
                           kernel_regularizer=keras.regularizers.l2(0.01))

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    RegularizedDense(300),
    RegularizedDense(100),
    RegularizedDense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 2
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
#+end_src

#+RESULTS:
: Epoch 1/2
: 1719/1719 [==============================] - 4s 2ms/step - loss: 3.3041 - accuracy: 0.7912 - val_loss: 0.7156 - val_accuracy: 0.8318
: Epoch 2/2
: 1719/1719 [==============================] - 4s 2ms/step - loss: 0.7266 - accuracy: 0.8256 - val_loss: 0.6821 - val_accuracy: 0.8370

** Dropout

#+begin_src jupyter-python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(300, activation="elu", kernel_initializer="he_normal"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 2
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
#+end_src

#+RESULTS:
: Epoch 1/2
: 1719/1719 [==============================] - 4s 2ms/step - loss: 0.7216 - accuracy: 0.7648 - val_loss: 0.3605 - val_accuracy: 0.8646
: Epoch 2/2
: 1719/1719 [==============================] - 3s 2ms/step - loss: 0.4274 - accuracy: 0.8414 - val_loss: 0.3438 - val_accuracy: 0.8680

** Alpha Dropout

#+begin_src jupyter-python
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(300, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal"),
    keras.layers.AlphaDropout(rate=0.2),
    keras.layers.Dense(10, activation="softmax")
])
optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
n_epochs = 20
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.8023 - accuracy: 0.7146 - val_loss: 0.5778 - val_accuracy: 0.8446
Epoch 2/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5662 - accuracy: 0.7903 - val_loss: 0.5160 - val_accuracy: 0.8518
Epoch 3/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5260 - accuracy: 0.8058 - val_loss: 0.4899 - val_accuracy: 0.8610
Epoch 4/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5129 - accuracy: 0.8093 - val_loss: 0.4766 - val_accuracy: 0.8592
Epoch 5/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.5073 - accuracy: 0.8119 - val_loss: 0.4237 - val_accuracy: 0.8710
Epoch 6/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4784 - accuracy: 0.8208 - val_loss: 0.4614 - val_accuracy: 0.8636
Epoch 7/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4720 - accuracy: 0.8266 - val_loss: 0.4725 - val_accuracy: 0.8608
Epoch 8/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4575 - accuracy: 0.8288 - val_loss: 0.4159 - val_accuracy: 0.8700
Epoch 9/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4625 - accuracy: 0.8284 - val_loss: 0.4281 - val_accuracy: 0.8740
Epoch 10/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4552 - accuracy: 0.8336 - val_loss: 0.4340 - val_accuracy: 0.8620
Epoch 11/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4467 - accuracy: 0.8325 - val_loss: 0.4173 - val_accuracy: 0.8708
Epoch 12/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4408 - accuracy: 0.8359 - val_loss: 0.5168 - val_accuracy: 0.8562
Epoch 13/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4337 - accuracy: 0.8396 - val_loss: 0.4274 - val_accuracy: 0.8734
Epoch 14/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4307 - accuracy: 0.8399 - val_loss: 0.4551 - val_accuracy: 0.8618
Epoch 15/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4323 - accuracy: 0.8383 - val_loss: 0.4425 - val_accuracy: 0.8672
Epoch 16/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4264 - accuracy: 0.8399 - val_loss: 0.4136 - val_accuracy: 0.8790
Epoch 17/20
1719/1719 [==============================] - 2s 1ms/step - loss: 0.4195 - accuracy: 0.8435 - val_loss: 0.5245 - val_accuracy: 0.8586
Epoch 18/20
1719/1719 [==============================] - 2s 951us/step - loss: 0.4350 - accuracy: 0.8404 - val_loss: 0.4732 - val_accuracy: 0.8702
Epoch 19/20
1719/1719 [==============================] - 2s 961us/step - loss: 0.4284 - accuracy: 0.8403 - val_loss: 0.4674 - val_accuracy: 0.8764
Epoch 20/20
1719/1719 [==============================] - 2s 971us/step - loss: 0.4182 - accuracy: 0.8428 - val_loss: 0.4349 - val_accuracy: 0.8746
#+end_example

#+begin_src jupyter-python
model.evaluate(X_test_scaled, y_test)
#+end_src

#+RESULTS:
:RESULTS:
: 313/313 [==============================] - 0s 534us/step - loss: 0.4711 - accuracy: 0.8586
| 0.47112908959388733 | 0.8586000204086304 |
:END:

#+begin_src jupyter-python
model.evaluate(X_train_scaled, y_train)
#+end_src

#+RESULTS:
:RESULTS:
: 1719/1719 [==============================] - 1s 476us/step - loss: 0.3488 - accuracy: 0.8823
| 0.34875163435935974 | 0.8822908997535706 |
:END:

#+begin_src jupyter-python
history = model.fit(X_train_scaled, y_train)
#+end_src

#+RESULTS:
: 1719/1719 [==============================] - 2s 970us/step - loss: 0.4229 - accuracy: 0.8433

** MC Dropout

#+begin_src jupyter-python
tf.random.set_seed(42)
np.random.seed(42)

y_probas = np.stack([model(X_test_scaled, training=True)
                     for sample in range(100)])
y_proba = y_probas.mean(axis=0)
y_std = y_probas.std(axis=0)

np.round(model.predict(X_test_scaled[:1]), 2)
#+end_src

#+RESULTS:
: array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],
:       dtype=float32)

#+begin_src jupyter-python
np.round(y_probas[:, :1], 2)
#+end_src

#+RESULTS:
#+begin_example
array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.83, 0.  , 0.14]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.96, 0.  , 0.03]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.  , 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.9 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.39, 0.  , 0.6 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.62, 0.  , 0.38]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.39, 0.  , 0.52]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.39, 0.  , 0.51]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.62, 0.  , 0.12, 0.  , 0.26]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.22, 0.  , 0.77]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.39, 0.  , 0.22, 0.  , 0.39]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.31, 0.  , 0.65]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.26, 0.  , 0.61]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.26, 0.  , 0.69]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.11, 0.  , 0.73]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.71, 0.  , 0.27]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.41, 0.  , 0.57]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.02, 0.  , 0.95]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.81, 0.03, 0.02, 0.  , 0.14]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.1 , 0.  , 0.81]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.91]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.07, 0.  , 0.85]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.65, 0.  , 0.29]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.15, 0.  , 0.83]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.37, 0.  , 0.47]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.07, 0.  , 0.9 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.47, 0.  , 0.23]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.45, 0.  , 0.04, 0.  , 0.51]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.83, 0.  , 0.1 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.15, 0.  , 0.8 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.64, 0.  , 0.32]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.89]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.37, 0.  , 0.62]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.61, 0.  , 0.06, 0.  , 0.32]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.26, 0.  , 0.74]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.33, 0.  , 0.45]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.13, 0.  , 0.77]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.49, 0.  , 0.5 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.27, 0.  , 0.72]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.34, 0.  , 0.39]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.89, 0.  , 0.1 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.05, 0.  , 0.8 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.07, 0.  , 0.8 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.94]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.69]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.16, 0.  , 0.82]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.24, 0.  , 0.69]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.2 , 0.  , 0.74]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.04, 0.  , 0.84]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.32, 0.  , 0.67]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.03, 0.  , 0.94]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.34, 0.  , 0.61]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.58, 0.  , 0.17]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.54, 0.  , 0.35, 0.  , 0.1 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.45, 0.  , 0.39]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.92]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.28, 0.  , 0.67]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.61, 0.  , 0.26]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.2 , 0.  , 0.76]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.18, 0.  , 0.78]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.34, 0.  , 0.63]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.28, 0.  , 0.64]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.02, 0.  , 0.81]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.11, 0.  , 0.87]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.63, 0.  , 0.35]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.43, 0.  , 0.55]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.16, 0.  , 0.82]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.89]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.06, 0.  , 0.92]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.12, 0.  , 0.85]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.46, 0.  , 0.5 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.77, 0.  , 0.19, 0.  , 0.04]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.33, 0.  , 0.05, 0.  , 0.61]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.94]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.26, 0.  , 0.7 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.1 , 0.  , 0.88]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.47, 0.  , 0.43]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.23, 0.  , 0.58]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.09, 0.  , 0.89]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.25, 0.  , 0.74]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.11, 0.  , 0.88]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.29, 0.  , 0.4 ]],

       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.09, 0.  , 0.76]]],
      dtype=float32)
#+end_example

#+begin_src jupyter-python
np.round(y_proba[:1], 2)
#+end_src

#+RESULTS:
: array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.23, 0.  , 0.68]],
:       dtype=float32)

#+begin_src jupyter-python
y_std = y_probas.std(axis=0)
np.round(y_std[:1], 2)
#+end_src

#+RESULTS:
: array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.22, 0.  , 0.27]],
:       dtype=float32)

#+begin_src jupyter-python
y_pred = np.argmax(y_proba, axis=1)
accuracy = np.sum(y_pred == y_test) / len(y_test)
accuracy
#+end_src

#+RESULTS:
: 0.8665

#+begin_src jupyter-python
class MCDropout(keras.layers.Dropout):
    def call(self, inputs):
        return super().call(inputs, training=True)

class MCAlphaDropout(keras.layers.AlphaDropout):
    def call(self, inputs):
        return super().call(inputs, training=True)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
tf.random.set_seed(42)
np.random.seed(42)

mc_model = keras.models.Sequential([
    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer
    for layer in model.layers
])
mc_model.summary()
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential_17"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_16 (Flatten)         (None, 784)               0
_________________________________________________________________
mc_alpha_dropout (MCAlphaDro (None, 784)               0
_________________________________________________________________
dense_49 (Dense)             (None, 300)               235500
_________________________________________________________________
mc_alpha_dropout_1 (MCAlphaD (None, 300)               0
_________________________________________________________________
dense_50 (Dense)             (None, 100)               30100
_________________________________________________________________
mc_alpha_dropout_2 (MCAlphaD (None, 100)               0
_________________________________________________________________
dense_51 (Dense)             (None, 10)                1010
=================================================================
Total params: 266,610
Trainable params: 266,610
Non-trainable params: 0
_________________________________________________________________
#+end_example

#+begin_src jupyter-python
optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)
mc_model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])

mc_model.set_weights(model.get_weights())
#+end_src

#+RESULTS:

Now we can use the model with MC Dropout:

#+begin_src jupyter-python
np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)
#+end_src

#+RESULTS:
: array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.26, 0.  , 0.62]],
:       dtype=float32)

** Max norm

#+begin_src jupyter-python
MaxNormDense = partial(keras.layers.Dense,
                       activation="selu", kernel_initializer="lecun_normal",
                       kernel_constraint=keras.constraints.max_norm(1.))

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    MaxNormDense(300),
    MaxNormDense(100),
    keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam", metrics=["accuracy"])
n_epochs = 2
history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
                    validation_data=(X_valid_scaled, y_valid))
#+end_src

#+RESULTS:
: Epoch 1/2
: 1719/1719 [==============================] - 4s 2ms/step - loss: 0.5756 - accuracy: 0.8027 - val_loss: 0.3776 - val_accuracy: 0.8654
: Epoch 2/2
: 1719/1719 [==============================] - 3s 2ms/step - loss: 0.3545 - accuracy: 0.8699 - val_loss: 0.3746 - val_accuracy: 0.8714


* Exercises

** 8. Practice training a deep neural network on the CIFAR10 image dataset:

*** a. Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function.

#+begin_src jupyter-python
keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))
for _ in range(20):
    model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))
#+end_src

#+RESULTS:

*** b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 Ã 32âpixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.

- Add the output layer to the model:

#+begin_src jupyter-python
model.add(keras.layers.Dense(10, activation="softmax"))
#+end_src

#+RESULTS:

- Let's use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better.

#+begin_src jupyter-python
optimizer = keras.optimizers.Nadam(lr=5e-5)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
#+end_src

#+RESULTS:

- Let's load the CIFAR10 dataset. We also want to use early stopping, so we need a validation set. Let's use the first 5,000 images of the original training set as the validation set:

#+begin_src jupyter-python
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()

X_train, y_train = X_train_full[5000:], y_train_full[5000:]
X_valid, y_valid = X_train_full[:5000], y_train_full[:5000]
#+end_src

#+RESULTS:
: Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
: 170500096/170498071 [==============================] - 4s 0us/step

- Now we can create the callbacks we need and train the model:

#+begin_src jupyter-python
early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)
model_checkpoint_cb = keras.callbacks.ModelCheckpoint("my_cifar10_model.h5", save_best_only=True)
run_index = 1  # increment every time you train the model
run_logdir = Path('.').resolve().joinpath("my_cifar10_logs", f"run_{run_index:03d}")
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]
#+end_src

#+RESULTS:

#+begin_src jupyter-python
%load_ext tensorboard
%tensorboard --logdir=./my_cifar10_logs --port=6006
#+end_src

#+RESULTS:
: Launching TensorBoard...

#+begin_src jupyter-python
model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Epoch 1/100
1407/1407 [==============================] - 7s 4ms/step - loss: 9.6201 - accuracy: 0.1418 - val_loss: 2.1457 - val_accuracy: 0.2250
Epoch 2/100
1407/1407 [==============================] - 7s 5ms/step - loss: 2.1001 - accuracy: 0.2403 - val_loss: 2.1851 - val_accuracy: 0.2178
Epoch 3/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.9837 - accuracy: 0.2798 - val_loss: 2.1055 - val_accuracy: 0.2610
Epoch 4/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.8817 - accuracy: 0.3153 - val_loss: 1.9316 - val_accuracy: 0.3294
Epoch 5/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.8096 - accuracy: 0.3414 - val_loss: 1.8083 - val_accuracy: 0.3346
Epoch 6/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.7582 - accuracy: 0.3584 - val_loss: 1.7506 - val_accuracy: 0.3648
Epoch 7/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.7151 - accuracy: 0.3761 - val_loss: 1.7398 - val_accuracy: 0.3522
Epoch 8/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.6731 - accuracy: 0.3922 - val_loss: 1.6687 - val_accuracy: 0.3980
Epoch 9/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.6506 - accuracy: 0.4030 - val_loss: 1.6749 - val_accuracy: 0.3920
Epoch 10/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.6143 - accuracy: 0.4168 - val_loss: 1.6547 - val_accuracy: 0.4084
Epoch 11/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.5890 - accuracy: 0.4276 - val_loss: 1.6413 - val_accuracy: 0.4086
Epoch 12/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.5738 - accuracy: 0.4308 - val_loss: 1.6541 - val_accuracy: 0.4146
Epoch 13/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.5420 - accuracy: 0.4441 - val_loss: 1.6232 - val_accuracy: 0.4154
Epoch 14/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.5225 - accuracy: 0.4494 - val_loss: 1.5837 - val_accuracy: 0.4292
Epoch 15/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.5184 - accuracy: 0.4539 - val_loss: 1.5924 - val_accuracy: 0.4268
Epoch 16/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.5001 - accuracy: 0.4621 - val_loss: 1.5838 - val_accuracy: 0.4322
Epoch 17/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.4888 - accuracy: 0.4638 - val_loss: 1.5672 - val_accuracy: 0.4458
Epoch 18/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.4691 - accuracy: 0.4757 - val_loss: 1.5858 - val_accuracy: 0.4346
Epoch 19/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.4524 - accuracy: 0.4795 - val_loss: 1.5955 - val_accuracy: 0.4320
Epoch 20/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.4403 - accuracy: 0.4788 - val_loss: 1.5414 - val_accuracy: 0.4486
Epoch 21/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.4354 - accuracy: 0.4841 - val_loss: 1.5258 - val_accuracy: 0.4534
Epoch 22/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.4158 - accuracy: 0.4901 - val_loss: 1.5319 - val_accuracy: 0.4452
Epoch 23/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.3950 - accuracy: 0.4986 - val_loss: 1.5458 - val_accuracy: 0.4444
Epoch 24/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.3911 - accuracy: 0.4969 - val_loss: 1.5381 - val_accuracy: 0.4544
Epoch 25/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.3763 - accuracy: 0.5048 - val_loss: 1.5261 - val_accuracy: 0.4586
Epoch 26/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.3599 - accuracy: 0.5103 - val_loss: 1.5424 - val_accuracy: 0.4540
Epoch 27/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.3549 - accuracy: 0.5117 - val_loss: 1.5318 - val_accuracy: 0.4562
Epoch 28/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.3456 - accuracy: 0.5160 - val_loss: 1.5518 - val_accuracy: 0.4602
Epoch 29/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.3394 - accuracy: 0.5176 - val_loss: 1.5275 - val_accuracy: 0.4584
Epoch 30/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.3389 - accuracy: 0.5176 - val_loss: 1.5537 - val_accuracy: 0.4566
Epoch 31/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.3317 - accuracy: 0.5198 - val_loss: 1.5617 - val_accuracy: 0.4550
Epoch 32/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.3108 - accuracy: 0.5286 - val_loss: 1.5381 - val_accuracy: 0.4620
Epoch 33/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.3114 - accuracy: 0.5263 - val_loss: 1.5134 - val_accuracy: 0.4722
Epoch 34/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2837 - accuracy: 0.5371 - val_loss: 1.5484 - val_accuracy: 0.4554
Epoch 35/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2865 - accuracy: 0.5390 - val_loss: 1.5240 - val_accuracy: 0.4706
Epoch 36/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2770 - accuracy: 0.5423 - val_loss: 1.5257 - val_accuracy: 0.4606
Epoch 37/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2603 - accuracy: 0.5450 - val_loss: 1.5419 - val_accuracy: 0.4628
Epoch 38/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.2539 - accuracy: 0.5473 - val_loss: 1.5147 - val_accuracy: 0.4768
Epoch 39/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.2569 - accuracy: 0.5480 - val_loss: 1.5585 - val_accuracy: 0.4540
Epoch 40/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.2468 - accuracy: 0.5516 - val_loss: 1.5316 - val_accuracy: 0.4742
Epoch 41/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.2330 - accuracy: 0.5532 - val_loss: 1.5479 - val_accuracy: 0.4694
Epoch 42/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.2238 - accuracy: 0.5613 - val_loss: 1.5421 - val_accuracy: 0.4756
Epoch 43/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2276 - accuracy: 0.5592 - val_loss: 1.5636 - val_accuracy: 0.4608
Epoch 44/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.2080 - accuracy: 0.5691 - val_loss: 1.5747 - val_accuracy: 0.4566
Epoch 45/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.2044 - accuracy: 0.5644 - val_loss: 1.5382 - val_accuracy: 0.4704
Epoch 46/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.1947 - accuracy: 0.5694 - val_loss: 1.5112 - val_accuracy: 0.4802
Epoch 47/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.1856 - accuracy: 0.5716 - val_loss: 1.5273 - val_accuracy: 0.4676
Epoch 48/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1867 - accuracy: 0.5755 - val_loss: 1.5565 - val_accuracy: 0.4674
Epoch 49/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1851 - accuracy: 0.5728 - val_loss: 1.5660 - val_accuracy: 0.4636
Epoch 50/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.1663 - accuracy: 0.5833 - val_loss: 1.5506 - val_accuracy: 0.4750
Epoch 51/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.1519 - accuracy: 0.5832 - val_loss: 1.5388 - val_accuracy: 0.4776
Epoch 52/100
1407/1407 [==============================] - 6s 5ms/step - loss: 1.1477 - accuracy: 0.5868 - val_loss: 1.5335 - val_accuracy: 0.4732
Epoch 53/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.1369 - accuracy: 0.5906 - val_loss: 1.5222 - val_accuracy: 0.4806
Epoch 54/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.1301 - accuracy: 0.5903 - val_loss: 1.5479 - val_accuracy: 0.4706
Epoch 55/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1327 - accuracy: 0.5957 - val_loss: 1.5585 - val_accuracy: 0.4820
Epoch 56/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1195 - accuracy: 0.5953 - val_loss: 1.5876 - val_accuracy: 0.4616
Epoch 57/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.1170 - accuracy: 0.5953 - val_loss: 1.5458 - val_accuracy: 0.4770
Epoch 58/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1093 - accuracy: 0.6010 - val_loss: 1.5785 - val_accuracy: 0.4634
Epoch 59/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.0913 - accuracy: 0.6035 - val_loss: 1.5915 - val_accuracy: 0.4770
Epoch 60/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.0923 - accuracy: 0.6054 - val_loss: 1.5956 - val_accuracy: 0.4730
Epoch 61/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.0818 - accuracy: 0.6073 - val_loss: 1.5971 - val_accuracy: 0.4604
Epoch 62/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.0788 - accuracy: 0.6073 - val_loss: 1.5742 - val_accuracy: 0.4766
Epoch 63/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.0769 - accuracy: 0.6108 - val_loss: 1.6403 - val_accuracy: 0.4634
Epoch 64/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.0714 - accuracy: 0.6111 - val_loss: 1.5915 - val_accuracy: 0.4738
Epoch 65/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.0615 - accuracy: 0.6165 - val_loss: 1.5930 - val_accuracy: 0.4756
Epoch 66/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.0492 - accuracy: 0.6207 - val_loss: 1.6358 - val_accuracy: 0.4634
#+end_example
: <tensorflow.python.keras.callbacks.History at 0x7f7cb0451d30>
:END:

#+begin_src jupyter-python
model = keras.models.load_model("my_cifar10_model.h5")
model.evaluate(X_valid, y_valid)
#+end_src

#+RESULTS:
:RESULTS:
: 157/157 [==============================] - 0s 1ms/step - loss: 1.5112 - accuracy: 0.4802
| 1.5111533403396606 | 0.48019999265670776 |
:END:

*** c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?

The code below is very similar to the code above, with a few changes:
    I added a BN layer after every Dense layer (before the activation function), except for the output layer. I also added a BN layer before the first hidden layer.
    I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.
    I renamed the run directories to runbn* and the model file name to my_cifar10_bn_model.h5.

#+begin_src jupyter-python
keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))
model.add(keras.layers.BatchNormalization())
for _ in range(20):
    model.add(keras.layers.Dense(100, kernel_initializer="he_normal"))
    model.add(keras.layers.BatchNormalization())
    model.add(keras.layers.Activation("elu"))
model.add(keras.layers.Dense(10, activation="softmax"))

optimizer = keras.optimizers.Nadam(lr=5e-4)
model.compile(
    loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"]
)

early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)
model_checkpoint_cb = keras.callbacks.ModelCheckpoint(
    "my_cifar10_bn_model.h5", save_best_only=True
)
run_index = 1  # increment every time you train the model
run_logdir = Path(".").resolve().joinpath("my_cifar10_logs", f"run_bn_{run_index:03d}")
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]

model.fit(
    X_train,
    y_train,
    epochs=100,
    validation_data=(X_valid, y_valid),
    callbacks=callbacks,
)

model = keras.models.load_model("my_cifar10_bn_model.h5")
model.evaluate(X_valid, y_valid)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Epoch 1/100
1407/1407 [==============================] - 15s 8ms/step - loss: 1.9790 - accuracy: 0.2945 - val_loss: 1.6887 - val_accuracy: 0.3950
Epoch 2/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.6821 - accuracy: 0.4023 - val_loss: 1.5786 - val_accuracy: 0.4354
Epoch 3/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.6078 - accuracy: 0.4309 - val_loss: 1.5159 - val_accuracy: 0.4598
Epoch 4/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.5495 - accuracy: 0.4474 - val_loss: 1.5030 - val_accuracy: 0.4672
Epoch 5/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.4992 - accuracy: 0.4684 - val_loss: 1.4537 - val_accuracy: 0.4798
Epoch 6/100
1407/1407 [==============================] - 10s 7ms/step - loss: 1.4653 - accuracy: 0.4820 - val_loss: 1.4150 - val_accuracy: 0.4924
Epoch 7/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.4316 - accuracy: 0.4914 - val_loss: 1.4199 - val_accuracy: 0.4892
Epoch 8/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.4085 - accuracy: 0.5037 - val_loss: 1.3814 - val_accuracy: 0.5102
Epoch 9/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.3792 - accuracy: 0.5132 - val_loss: 1.3801 - val_accuracy: 0.5050
Epoch 10/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.3566 - accuracy: 0.5164 - val_loss: 1.3591 - val_accuracy: 0.5136
Epoch 11/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.3242 - accuracy: 0.5334 - val_loss: 1.3368 - val_accuracy: 0.5320
Epoch 12/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.3108 - accuracy: 0.5404 - val_loss: 1.3654 - val_accuracy: 0.5070
Epoch 13/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.2916 - accuracy: 0.5424 - val_loss: 1.4202 - val_accuracy: 0.4988
Epoch 14/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.2852 - accuracy: 0.5447 - val_loss: 1.3445 - val_accuracy: 0.5278
Epoch 15/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.2608 - accuracy: 0.5513 - val_loss: 1.3641 - val_accuracy: 0.5200
Epoch 16/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.2451 - accuracy: 0.5586 - val_loss: 1.3737 - val_accuracy: 0.5170
Epoch 17/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.2317 - accuracy: 0.5626 - val_loss: 1.3231 - val_accuracy: 0.5368
Epoch 18/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.2118 - accuracy: 0.5694 - val_loss: 1.3388 - val_accuracy: 0.5274
Epoch 19/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.1985 - accuracy: 0.5765 - val_loss: 1.3280 - val_accuracy: 0.5332
Epoch 20/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.1851 - accuracy: 0.5813 - val_loss: 1.3733 - val_accuracy: 0.5256
Epoch 21/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.1677 - accuracy: 0.5849 - val_loss: 1.3690 - val_accuracy: 0.5258
Epoch 22/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.1569 - accuracy: 0.5905 - val_loss: 1.3743 - val_accuracy: 0.5200
Epoch 23/100
1407/1407 [==============================] - 12s 8ms/step - loss: 1.1409 - accuracy: 0.5975 - val_loss: 1.3220 - val_accuracy: 0.5430
Epoch 24/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.1265 - accuracy: 0.6021 - val_loss: 1.3237 - val_accuracy: 0.5374
Epoch 25/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.1206 - accuracy: 0.6068 - val_loss: 1.3354 - val_accuracy: 0.5484
Epoch 26/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.1008 - accuracy: 0.6103 - val_loss: 1.3432 - val_accuracy: 0.5360
Epoch 27/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.0842 - accuracy: 0.6192 - val_loss: 1.3458 - val_accuracy: 0.5332
Epoch 28/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.0747 - accuracy: 0.6202 - val_loss: 1.3523 - val_accuracy: 0.5342
Epoch 29/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.0721 - accuracy: 0.6209 - val_loss: 1.3310 - val_accuracy: 0.5418
Epoch 30/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.0674 - accuracy: 0.6227 - val_loss: 1.3681 - val_accuracy: 0.5346
Epoch 31/100
1407/1407 [==============================] - 11s 8ms/step - loss: 1.0600 - accuracy: 0.6262 - val_loss: 1.3534 - val_accuracy: 0.5362
Epoch 32/100
1407/1407 [==============================] - 13s 9ms/step - loss: 1.0363 - accuracy: 0.6347 - val_loss: 1.3703 - val_accuracy: 0.5372
Epoch 33/100
1407/1407 [==============================] - 11s 8ms/step - loss: 1.0281 - accuracy: 0.6364 - val_loss: 1.3539 - val_accuracy: 0.5476
Epoch 34/100
1407/1407 [==============================] - 11s 7ms/step - loss: 1.0195 - accuracy: 0.6398 - val_loss: 1.3688 - val_accuracy: 0.5392
Epoch 35/100
1407/1407 [==============================] - 9s 7ms/step - loss: 1.0162 - accuracy: 0.6403 - val_loss: 1.3595 - val_accuracy: 0.5404
Epoch 36/100
1407/1407 [==============================] - 11s 8ms/step - loss: 1.0022 - accuracy: 0.6467 - val_loss: 1.3544 - val_accuracy: 0.5398
Epoch 37/100
1407/1407 [==============================] - 9s 6ms/step - loss: 0.9883 - accuracy: 0.6559 - val_loss: 1.3639 - val_accuracy: 0.5412
Epoch 38/100
1407/1407 [==============================] - 9s 7ms/step - loss: 0.9800 - accuracy: 0.6537 - val_loss: 1.3861 - val_accuracy: 0.5404
Epoch 39/100
1407/1407 [==============================] - 9s 6ms/step - loss: 0.9727 - accuracy: 0.6557 - val_loss: 1.3921 - val_accuracy: 0.5352
Epoch 40/100
1407/1407 [==============================] - 10s 7ms/step - loss: 0.9590 - accuracy: 0.6634 - val_loss: 1.4103 - val_accuracy: 0.5374
Epoch 41/100
1407/1407 [==============================] - 10s 7ms/step - loss: 0.9649 - accuracy: 0.6630 - val_loss: 1.3833 - val_accuracy: 0.5406
Epoch 42/100
1407/1407 [==============================] - 8s 6ms/step - loss: 0.9430 - accuracy: 0.6694 - val_loss: 1.3903 - val_accuracy: 0.5364
Epoch 43/100
1407/1407 [==============================] - 9s 6ms/step - loss: 0.9456 - accuracy: 0.6707 - val_loss: 1.3861 - val_accuracy: 0.5398
157/157 [==============================] - 0s 1ms/step - loss: 1.3220 - accuracy: 0.5430
#+end_example
| 1.322007656097412 | 0.5429999828338623 |
:END:

- Is the model converging faster than before?
  Much faster! The previous model took 27 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 5 epochs and continued to make progress until the 16th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.
- Does BN produce a better model?
  Yes! The final model is also much better, with 54.3% accuracy instead of 48.02%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic, see chapter 14).
- How does BN affect training speed?
  Although the model converged much faster, each epoch took about 9s instead of 5s, because of the extra computations required by the BN layers. But overall the training time (wall time) was shortened significantly!

*** d. Try replacing Batch Normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).

#+begin_src jupyter-python
keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(keras.layers.Dense(100, kernel_initializer="lecun_normal", activation="selu"))
model.add(keras.layers.Dense(10, activation="softmax"))

optimizer = keras.optimizers.Nadam(lr=7e-4)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])

early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)
model_checkpoint_cb = keras.callbacks.ModelCheckpoint("my_cifar10_selu_model.h5", save_best_only=True)
run_index = 1  # increment every time you train the model
run_logdir = Path(".").resolve().joinpath("my_cifar10_logs", f"run_selu_{run_index:03d}")
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]

X_means = X_train.mean(axis=0)
X_stds = X_train.std(axis=0)
X_train_scaled = (X_train - X_means) / X_stds
X_valid_scaled = (X_valid - X_means) / X_stds
X_test_scaled = (X_test - X_means) / X_stds

model.fit(
    X_train_scaled,
    y_train,
    epochs=100,
    validation_data=(X_valid_scaled, y_valid),
    callbacks=callbacks,
)

model = keras.models.load_model("my_cifar10_selu_model.h5")
model.evaluate(X_valid_scaled, y_valid)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Epoch 1/100
1407/1407 [==============================] - 7s 4ms/step - loss: 2.0616 - accuracy: 0.2678 - val_loss: 1.8302 - val_accuracy: 0.3460
Epoch 2/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.7424 - accuracy: 0.3778 - val_loss: 1.7084 - val_accuracy: 0.4000
Epoch 3/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.6368 - accuracy: 0.4272 - val_loss: 1.6648 - val_accuracy: 0.4070
Epoch 4/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.5536 - accuracy: 0.4544 - val_loss: 1.6482 - val_accuracy: 0.4220
Epoch 5/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.5041 - accuracy: 0.4734 - val_loss: 1.5607 - val_accuracy: 0.4522
Epoch 6/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.4450 - accuracy: 0.4935 - val_loss: 1.5184 - val_accuracy: 0.4724
Epoch 7/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.3987 - accuracy: 0.5175 - val_loss: 1.5547 - val_accuracy: 0.4580
Epoch 8/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.3626 - accuracy: 0.5260 - val_loss: 1.4878 - val_accuracy: 0.4928
Epoch 9/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.3352 - accuracy: 0.5339 - val_loss: 1.5150 - val_accuracy: 0.4768
Epoch 10/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2997 - accuracy: 0.5468 - val_loss: 1.5272 - val_accuracy: 0.4872
Epoch 11/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2682 - accuracy: 0.5653 - val_loss: 1.4673 - val_accuracy: 0.4926
Epoch 12/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2354 - accuracy: 0.5767 - val_loss: 1.4922 - val_accuracy: 0.4894
Epoch 13/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2091 - accuracy: 0.5840 - val_loss: 1.4667 - val_accuracy: 0.5064
Epoch 14/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.1774 - accuracy: 0.5977 - val_loss: 1.4893 - val_accuracy: 0.5054
Epoch 15/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.1718 - accuracy: 0.5981 - val_loss: 1.4996 - val_accuracy: 0.5012
Epoch 16/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.1436 - accuracy: 0.6082 - val_loss: 1.5178 - val_accuracy: 0.5136
Epoch 17/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.1240 - accuracy: 0.6166 - val_loss: 1.4862 - val_accuracy: 0.5152
Epoch 18/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.1022 - accuracy: 0.6245 - val_loss: 1.5141 - val_accuracy: 0.5168
Epoch 19/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.0707 - accuracy: 0.6360 - val_loss: 1.5581 - val_accuracy: 0.5062
Epoch 20/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.0743 - accuracy: 0.6323 - val_loss: 1.4742 - val_accuracy: 0.5118
Epoch 21/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.0395 - accuracy: 0.6430 - val_loss: 1.5392 - val_accuracy: 0.5080
Epoch 22/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.0288 - accuracy: 0.6489 - val_loss: 1.5210 - val_accuracy: 0.5168
Epoch 23/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2121 - accuracy: 0.6140 - val_loss: 1.5469 - val_accuracy: 0.4654
Epoch 24/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.2487 - accuracy: 0.5674 - val_loss: 1.5520 - val_accuracy: 0.4778
Epoch 25/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.1973 - accuracy: 0.5879 - val_loss: 1.5040 - val_accuracy: 0.5026
Epoch 26/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.0973 - accuracy: 0.6210 - val_loss: 1.5237 - val_accuracy: 0.4944
Epoch 27/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.0554 - accuracy: 0.6389 - val_loss: 1.5031 - val_accuracy: 0.5116
Epoch 28/100
1407/1407 [==============================] - 5s 3ms/step - loss: 1.0110 - accuracy: 0.6556 - val_loss: 1.5004 - val_accuracy: 0.5022
Epoch 29/100
1407/1407 [==============================] - 5s 3ms/step - loss: 0.9776 - accuracy: 0.6635 - val_loss: 1.5897 - val_accuracy: 0.5182
Epoch 30/100
1407/1407 [==============================] - 5s 3ms/step - loss: 0.9597 - accuracy: 0.6738 - val_loss: 1.5059 - val_accuracy: 0.5196
Epoch 31/100
1407/1407 [==============================] - 5s 3ms/step - loss: 0.9530 - accuracy: 0.6763 - val_loss: 1.5239 - val_accuracy: 0.5180
Epoch 32/100
1407/1407 [==============================] - 5s 4ms/step - loss: 0.9470 - accuracy: 0.6794 - val_loss: 1.5412 - val_accuracy: 0.5142
Epoch 33/100
1407/1407 [==============================] - 5s 3ms/step - loss: 0.9391 - accuracy: 0.6793 - val_loss: 1.5667 - val_accuracy: 0.5120
157/157 [==============================] - 0s 915us/step - loss: 1.4667 - accuracy: 0.5064
#+end_example
| 1.4666820764541626 | 0.5063999891281128 |
:END:

We get 50.64% accuracy, which is not much better than the original model (48.02%), and not as good as the model using batch normalization (54.3%). However, convergence was almost as fast as with the BN model, plus each epoch took only 5 seconds. So it's by far the fastest model to train so far.

*** e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.

#+begin_src jupyter-python
keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(
        keras.layers.Dense(100, kernel_initializer="lecun_normal", activation="selu")
    )

model.add(keras.layers.AlphaDropout(rate=0.1))
model.add(keras.layers.Dense(10, activation="softmax"))

optimizer = keras.optimizers.Nadam(lr=5e-4)
model.compile(
    loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"]
)

early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)
model_checkpoint_cb = keras.callbacks.ModelCheckpoint(
    "my_cifar10_alpha_dropout_model.h5", save_best_only=True
)
run_index = 1  # increment every time you train the model
run_logdir = (
    Path(".")
    .resolve()
    .joinpath("my_cifar10_logs", f"run_alpha_dropout_{run_index:03d}")
)
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]

X_means = X_train.mean(axis=0)
X_stds = X_train.std(axis=0)
X_train_scaled = (X_train - X_means) / X_stds
X_valid_scaled = (X_valid - X_means) / X_stds
X_test_scaled = (X_test - X_means) / X_stds

model.fit(
    X_train_scaled,
    y_train,
    epochs=100,
    validation_data=(X_valid_scaled, y_valid),
    callbacks=callbacks,
)

model = keras.models.load_model("my_cifar10_alpha_dropout_model.h5")
model.evaluate(X_valid_scaled, y_valid)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Epoch 1/100
1407/1407 [==============================] - 8s 5ms/step - loss: 2.0537 - accuracy: 0.2808 - val_loss: 1.7096 - val_accuracy: 0.3914
Epoch 2/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.6718 - accuracy: 0.4087 - val_loss: 1.6353 - val_accuracy: 0.4188
Epoch 3/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.5808 - accuracy: 0.4463 - val_loss: 1.6741 - val_accuracy: 0.4158
Epoch 4/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.5018 - accuracy: 0.4681 - val_loss: 1.5885 - val_accuracy: 0.4582
Epoch 5/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.4421 - accuracy: 0.4940 - val_loss: 1.5328 - val_accuracy: 0.4760
Epoch 6/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.3911 - accuracy: 0.5129 - val_loss: 1.5277 - val_accuracy: 0.4816
Epoch 7/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.3489 - accuracy: 0.5275 - val_loss: 1.5603 - val_accuracy: 0.4684
Epoch 8/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.2998 - accuracy: 0.5462 - val_loss: 1.4986 - val_accuracy: 0.4912
Epoch 9/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.2733 - accuracy: 0.5526 - val_loss: 1.4833 - val_accuracy: 0.4962
Epoch 10/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.2348 - accuracy: 0.5693 - val_loss: 1.4834 - val_accuracy: 0.5048
Epoch 11/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.1984 - accuracy: 0.5859 - val_loss: 1.6173 - val_accuracy: 0.4884
Epoch 12/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.1783 - accuracy: 0.5930 - val_loss: 1.4862 - val_accuracy: 0.5028
Epoch 13/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.1492 - accuracy: 0.6019 - val_loss: 1.5518 - val_accuracy: 0.5022
Epoch 14/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.1021 - accuracy: 0.6201 - val_loss: 1.5177 - val_accuracy: 0.5042
Epoch 15/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.0972 - accuracy: 0.6190 - val_loss: 1.6022 - val_accuracy: 0.5108
Epoch 16/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.0732 - accuracy: 0.6293 - val_loss: 1.5834 - val_accuracy: 0.5138
Epoch 17/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.0523 - accuracy: 0.6348 - val_loss: 1.6553 - val_accuracy: 0.5148
Epoch 18/100
1407/1407 [==============================] - 5s 4ms/step - loss: 1.0259 - accuracy: 0.6478 - val_loss: 1.6424 - val_accuracy: 0.5038
Epoch 19/100
1407/1407 [==============================] - 6s 4ms/step - loss: 1.0068 - accuracy: 0.6538 - val_loss: 1.6738 - val_accuracy: 0.5070
Epoch 20/100
1407/1407 [==============================] - 5s 4ms/step - loss: 0.9792 - accuracy: 0.6623 - val_loss: 1.6853 - val_accuracy: 0.5028
Epoch 21/100
1407/1407 [==============================] - 5s 4ms/step - loss: 0.9619 - accuracy: 0.6714 - val_loss: 1.7348 - val_accuracy: 0.5066
Epoch 22/100
1407/1407 [==============================] - 6s 4ms/step - loss: 0.9375 - accuracy: 0.6780 - val_loss: 1.7678 - val_accuracy: 0.5074
Epoch 23/100
1407/1407 [==============================] - 5s 4ms/step - loss: 0.9200 - accuracy: 0.6902 - val_loss: 1.7528 - val_accuracy: 0.4966
Epoch 24/100
1407/1407 [==============================] - 6s 4ms/step - loss: 0.9024 - accuracy: 0.6899 - val_loss: 1.7280 - val_accuracy: 0.5020
Epoch 25/100
1407/1407 [==============================] - 5s 4ms/step - loss: 0.8904 - accuracy: 0.6973 - val_loss: 1.7279 - val_accuracy: 0.5030
Epoch 26/100
1407/1407 [==============================] - 5s 4ms/step - loss: 0.8692 - accuracy: 0.7073 - val_loss: 1.7150 - val_accuracy: 0.5090
Epoch 27/100
1407/1407 [==============================] - 6s 4ms/step - loss: 0.8453 - accuracy: 0.7119 - val_loss: 1.7321 - val_accuracy: 0.5162
Epoch 28/100
1407/1407 [==============================] - 6s 4ms/step - loss: 0.8288 - accuracy: 0.7213 - val_loss: 1.8248 - val_accuracy: 0.4946
Epoch 29/100
1407/1407 [==============================] - 6s 4ms/step - loss: 0.8338 - accuracy: 0.7199 - val_loss: 1.8727 - val_accuracy: 0.5098
157/157 [==============================] - 0s 885us/step - loss: 1.4833 - accuracy: 0.4962
#+end_example
| 1.483300805091858 | 0.49619999527931213 |
:END:

The model reaches 49.62% accuracy on the validation set. That's very slightly better than without dropout (48.02%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case.

Let's use MC Dropout now. We will need the MCAlphaDropout class we used earlier, so let's just copy it here for convenience:

#+begin_src jupyter-python
class MCAlphaDropout(keras.layers.AlphaDropout):
    def call(self, inputs):
        return super().call(inputs, training=True)
#+end_src

#+RESULTS:

Now let's create a new model, identical to the one we just trained (with the same weights), but with MCAlphaDropout dropout layers instead of AlphaDropout layers:

#+begin_src jupyter-python
mc_model = keras.models.Sequential([
    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer
    for layer in model.layers
])
#+end_src

#+RESULTS:

Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:

#+begin_src jupyter-python
def mc_dropout_predict_probas(mc_model, X, n_samples=10):
    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]
    return np.mean(Y_probas, axis=0)

def mc_dropout_predict_classes(mc_model, X, n_samples=10):
    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)
    return np.argmax(Y_probas, axis=1)
#+end_src

#+RESULTS:

Now let's make predictions for all the instances in the validation set, and compute the accuracy:

#+begin_src jupyter-python
keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)
accuracy = np.mean(y_pred == y_valid[:, 0])
accuracy
#+end_src

#+RESULTS:
: 0.497

We get no accuracy improvement in this case (we're still at 49.7% accuracy).

So the best model we got in this exercise is the Batch Normalization model.

*** f. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.

#+begin_src jupyter-python
keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(keras.layers.Dense(100,
                                 kernel_initializer="lecun_normal",
                                 activation="selu"))

model.add(keras.layers.AlphaDropout(rate=0.1))
model.add(keras.layers.Dense(10, activation="softmax"))

optimizer = keras.optimizers.SGD(lr=1e-3)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])
#+end_src

#+RESULTS:

#+begin_src jupyter-python
batch_size = 128
rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)
plot_lr_vs_loss(rates, losses)
plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])
#+end_src

#+RESULTS:
:RESULTS:
: 352/352 [==============================] - 2s 4ms/step - loss: nan - accuracy: 0.1251
| 9.999999747378752e-06 | 9.615227699279785 | 2.622215986251831 | 3.939341306686402 |
[[file:./.ob-jupyter/6df7bb793e849c7cf0d131d67d851df2fc4c857f.png]]
:END:

#+begin_src jupyter-python
keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))
for _ in range(20):
    model.add(keras.layers.Dense(100,
                                 kernel_initializer="lecun_normal",
                                 activation="selu"))

model.add(keras.layers.AlphaDropout(rate=0.1))
model.add(keras.layers.Dense(10, activation="softmax"))

optimizer = keras.optimizers.SGD(lr=1e-2)
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=optimizer,
              metrics=["accuracy"])
#+end_src

#+RESULTS:

#+begin_src jupyter-python
n_epochs = 15
onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)
history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,
                    validation_data=(X_valid_scaled, y_valid),
                    callbacks=[onecycle])
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/15
352/352 [==============================] - 3s 6ms/step - loss: 2.2398 - accuracy: 0.2309 - val_loss: 1.7721 - val_accuracy: 0.3778
Epoch 2/15
352/352 [==============================] - 2s 4ms/step - loss: 1.7947 - accuracy: 0.3664 - val_loss: 1.6650 - val_accuracy: 0.4178
Epoch 3/15
352/352 [==============================] - 2s 5ms/step - loss: 1.6358 - accuracy: 0.4239 - val_loss: 1.6489 - val_accuracy: 0.4242
Epoch 4/15
352/352 [==============================] - 2s 5ms/step - loss: 1.5402 - accuracy: 0.4557 - val_loss: 1.6302 - val_accuracy: 0.4398
Epoch 5/15
352/352 [==============================] - 2s 5ms/step - loss: 1.4850 - accuracy: 0.4767 - val_loss: 1.6161 - val_accuracy: 0.4368
Epoch 6/15
352/352 [==============================] - 2s 4ms/step - loss: 1.4328 - accuracy: 0.4902 - val_loss: 1.5974 - val_accuracy: 0.4416
Epoch 7/15
352/352 [==============================] - 2s 4ms/step - loss: 1.4013 - accuracy: 0.5013 - val_loss: 1.5453 - val_accuracy: 0.4590
Epoch 8/15
352/352 [==============================] - 2s 4ms/step - loss: 1.3384 - accuracy: 0.5233 - val_loss: 1.4979 - val_accuracy: 0.4806
Epoch 9/15
352/352 [==============================] - 2s 4ms/step - loss: 1.2679 - accuracy: 0.5475 - val_loss: 1.5281 - val_accuracy: 0.4862
Epoch 10/15
352/352 [==============================] - 2s 5ms/step - loss: 1.1915 - accuracy: 0.5717 - val_loss: 1.5235 - val_accuracy: 0.4936
Epoch 11/15
352/352 [==============================] - 2s 5ms/step - loss: 1.1186 - accuracy: 0.6043 - val_loss: 1.5333 - val_accuracy: 0.4970
Epoch 12/15
352/352 [==============================] - 2s 5ms/step - loss: 1.0543 - accuracy: 0.6261 - val_loss: 1.4947 - val_accuracy: 0.5112
Epoch 13/15
352/352 [==============================] - 2s 5ms/step - loss: 0.9865 - accuracy: 0.6505 - val_loss: 1.5422 - val_accuracy: 0.5138
Epoch 14/15
352/352 [==============================] - 2s 5ms/step - loss: 0.9128 - accuracy: 0.6747 - val_loss: 1.5649 - val_accuracy: 0.5188
Epoch 15/15
352/352 [==============================] - 2s 4ms/step - loss: 0.8898 - accuracy: 0.6826 - val_loss: 1.5885 - val_accuracy: 0.5210
#+end_example

One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 48.02% to 52.1%). The batch normalized model reaches a slightly better performance (54.3%), but it's much slower to train.


*** g. My experiment on Batch Normalization with use_bias set to False in Dense layers.

#+begin_src jupyter-python
keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))
model.add(keras.layers.BatchNormalization())
for _ in range(20):
    model.add(keras.layers.Dense(100, kernel_initializer="he_normal", use_bias=False))
    model.add(keras.layers.BatchNormalization())
    model.add(keras.layers.Activation("elu"))
model.add(keras.layers.Dense(10, activation="softmax"))

optimizer = keras.optimizers.Nadam(lr=5e-4)
model.compile(
    loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"]
)

early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)
model_checkpoint_cb = keras.callbacks.ModelCheckpoint(
    "my_cifar10_bn_model.h5", save_best_only=True
)
run_index = 1  # increment every time you train the model
run_logdir = Path(".").resolve().joinpath("my_cifar10_logs", f"run_bn_{run_index:03d}")
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]

model.fit(
    X_train,
    y_train,
    epochs=100,
    validation_data=(X_valid, y_valid),
    callbacks=callbacks,
)

model = keras.models.load_model("my_cifar10_bn_model.h5")
model.evaluate(X_valid, y_valid)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Epoch 1/100
1407/1407 [==============================] - 13s 6ms/step - loss: 1.9825 - accuracy: 0.2971 - val_loss: 1.6825 - val_accuracy: 0.3992
Epoch 2/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.6774 - accuracy: 0.4045 - val_loss: 1.5803 - val_accuracy: 0.4326
Epoch 3/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.6091 - accuracy: 0.4280 - val_loss: 1.5157 - val_accuracy: 0.4508
Epoch 4/100
1407/1407 [==============================] - 10s 7ms/step - loss: 1.5512 - accuracy: 0.4478 - val_loss: 1.4969 - val_accuracy: 0.4642
Epoch 5/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.4996 - accuracy: 0.4657 - val_loss: 1.4357 - val_accuracy: 0.4830
Epoch 6/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.4614 - accuracy: 0.4771 - val_loss: 1.4113 - val_accuracy: 0.4976
Epoch 7/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.4246 - accuracy: 0.4924 - val_loss: 1.4338 - val_accuracy: 0.4924
Epoch 8/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.4092 - accuracy: 0.5021 - val_loss: 1.3777 - val_accuracy: 0.5186
Epoch 9/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.3791 - accuracy: 0.5143 - val_loss: 1.3657 - val_accuracy: 0.5144
Epoch 10/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.3556 - accuracy: 0.5162 - val_loss: 1.3607 - val_accuracy: 0.5168
Epoch 11/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.3252 - accuracy: 0.5336 - val_loss: 1.3413 - val_accuracy: 0.5228
Epoch 12/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.3117 - accuracy: 0.5345 - val_loss: 1.3918 - val_accuracy: 0.5048
Epoch 13/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.2895 - accuracy: 0.5461 - val_loss: 1.4175 - val_accuracy: 0.5086
Epoch 14/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.2705 - accuracy: 0.5481 - val_loss: 1.3431 - val_accuracy: 0.5310
Epoch 15/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.2538 - accuracy: 0.5569 - val_loss: 1.3662 - val_accuracy: 0.5236
Epoch 16/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.2466 - accuracy: 0.5573 - val_loss: 1.3569 - val_accuracy: 0.5290
Epoch 17/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.2281 - accuracy: 0.5660 - val_loss: 1.3237 - val_accuracy: 0.5376
Epoch 18/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.2088 - accuracy: 0.5702 - val_loss: 1.3216 - val_accuracy: 0.5398
Epoch 19/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.1946 - accuracy: 0.5806 - val_loss: 1.3442 - val_accuracy: 0.5292
Epoch 20/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.1846 - accuracy: 0.5827 - val_loss: 1.3579 - val_accuracy: 0.5268
Epoch 21/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.1676 - accuracy: 0.5838 - val_loss: 1.3605 - val_accuracy: 0.5266
Epoch 22/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.1559 - accuracy: 0.5902 - val_loss: 1.3372 - val_accuracy: 0.5322
Epoch 23/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.1382 - accuracy: 0.6003 - val_loss: 1.3191 - val_accuracy: 0.5422
Epoch 24/100
1407/1407 [==============================] - 7s 5ms/step - loss: 1.1347 - accuracy: 0.5975 - val_loss: 1.3206 - val_accuracy: 0.5432
Epoch 25/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.1247 - accuracy: 0.6059 - val_loss: 1.3044 - val_accuracy: 0.5452
Epoch 26/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.1062 - accuracy: 0.6079 - val_loss: 1.3496 - val_accuracy: 0.5296
Epoch 27/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.0984 - accuracy: 0.6139 - val_loss: 1.3432 - val_accuracy: 0.5468
Epoch 28/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.0821 - accuracy: 0.6185 - val_loss: 1.3592 - val_accuracy: 0.5270
Epoch 29/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.0867 - accuracy: 0.6184 - val_loss: 1.3366 - val_accuracy: 0.5416
Epoch 30/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.0722 - accuracy: 0.6233 - val_loss: 1.3523 - val_accuracy: 0.5390
Epoch 31/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.0644 - accuracy: 0.6271 - val_loss: 1.3387 - val_accuracy: 0.5396
Epoch 32/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.0384 - accuracy: 0.6309 - val_loss: 1.3571 - val_accuracy: 0.5440
Epoch 33/100
1407/1407 [==============================] - 9s 6ms/step - loss: 1.0366 - accuracy: 0.6326 - val_loss: 1.3519 - val_accuracy: 0.5474
Epoch 34/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.0153 - accuracy: 0.6376 - val_loss: 1.3490 - val_accuracy: 0.5480
Epoch 35/100
1407/1407 [==============================] - 8s 6ms/step - loss: 1.0208 - accuracy: 0.6394 - val_loss: 1.3524 - val_accuracy: 0.5430
Epoch 36/100
1407/1407 [==============================] - 8s 5ms/step - loss: 1.0114 - accuracy: 0.6411 - val_loss: 1.3579 - val_accuracy: 0.5402
Epoch 37/100
1407/1407 [==============================] - 8s 5ms/step - loss: 0.9897 - accuracy: 0.6520 - val_loss: 1.3427 - val_accuracy: 0.5506
Epoch 38/100
1407/1407 [==============================] - 8s 5ms/step - loss: 0.9847 - accuracy: 0.6587 - val_loss: 1.3557 - val_accuracy: 0.5448
Epoch 39/100
1407/1407 [==============================] - 8s 6ms/step - loss: 0.9795 - accuracy: 0.6520 - val_loss: 1.3602 - val_accuracy: 0.5412
Epoch 40/100
1407/1407 [==============================] - 8s 6ms/step - loss: 0.9622 - accuracy: 0.6602 - val_loss: 1.3925 - val_accuracy: 0.5400
Epoch 41/100
1407/1407 [==============================] - 8s 5ms/step - loss: 0.9632 - accuracy: 0.6645 - val_loss: 1.3643 - val_accuracy: 0.5518
Epoch 42/100
1407/1407 [==============================] - 8s 5ms/step - loss: 0.9578 - accuracy: 0.6618 - val_loss: 1.4013 - val_accuracy: 0.5408
Epoch 43/100
1407/1407 [==============================] - 8s 5ms/step - loss: 0.9497 - accuracy: 0.6645 - val_loss: 1.3877 - val_accuracy: 0.5414
Epoch 44/100
1407/1407 [==============================] - 8s 6ms/step - loss: 0.9445 - accuracy: 0.6713 - val_loss: 1.3921 - val_accuracy: 0.5404
Epoch 45/100
1407/1407 [==============================] - 8s 5ms/step - loss: 0.9227 - accuracy: 0.6759 - val_loss: 1.3873 - val_accuracy: 0.5392
157/157 [==============================] - 0s 1ms/step - loss: 1.3044 - accuracy: 0.5452
#+end_example
| 1.304371953010559 | 0.545199990272522 |
:END:
