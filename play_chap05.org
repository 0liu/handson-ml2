#+TITLE: Practice Hands-on ML Chap05 - SVM

#+PROPERTY: header-args :exports both

#+begin_src jupyter-python
# Py data
import pandas as pd
import numpy as np
from scipy import stats

# Visualization
import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rc("axes", labelsize=14)
mpl.rc("xtick", labelsize=12)
mpl.rc("ytick", labelsize=12)
mpl.rc("figure", facecolor="white", dpi=120)

# utilities
from copy import deepcopy

# sklearn datasets
from sklearn import datasets

# Data train/test split
from sklearn.model_selection import train_test_split

# Data cleaning and transform
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures

# Model selection / Hyper-parameter tuning
from sklearn.metrics import accuracy_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error

# Models
from sklearn.linear_model import SGDClassifier
from sklearn.svm import LinearSVC, LinearSVR
from sklearn.svm import SVC, SVR
#+end_src

#+RESULTS:


Print all model parameters
#+begin_src jupyter-python
from sklearn import set_config
set_config(print_changed_only=False)
#+end_src

#+RESULTS:


* Linear SVM classification

#+begin_src jupyter-python
def plot_svc_decision_boundary(svm_clf, xmin, xmax):
    w = svm_clf.coef_[0]
    b = svm_clf.intercept_[0]

    # At the decision boundary, w0*x0 + w1*x1 + b = 0
    # => x1 = -w0/w1 * x0 - b/w1
    x0 = np.linspace(xmin, xmax, 200)
    decision_boundary = -w[0]/w[1] * x0 - b/w[1]

    margin = 1/w[1]
    gutter_up = decision_boundary + margin
    gutter_down = decision_boundary - margin

    svs = svm_clf.support_vectors_
    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')
    plt.plot(x0, decision_boundary, "k-", linewidth=2)
    plt.plot(x0, gutter_up, "k--", linewidth=2)
    plt.plot(x0, gutter_down, "k--", linewidth=2)
#+end_src

#+RESULTS:

** Large margin vs. Margin violations

#+begin_src jupyter-python
iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.float64)  # Iris virginica

svm_clf = Pipeline([
    ("scalar", StandardScaler()),
    ("linear_svc", LinearSVC(C=1, loss="hinge", random_state=42)),
])
svm_clf.fit(X, y)
#+end_src

#+RESULTS:
: Pipeline(memory=None,
:          steps=[('scalar',
:                  StandardScaler(copy=True, with_mean=True, with_std=True)),
:                 ('linear_svc',
:                  LinearSVC(C=1, class_weight=None, dual=True,
:                            fit_intercept=True, intercept_scaling=1,
:                            loss='hinge', max_iter=1000, multi_class='ovr',
:                            penalty='l2', random_state=42, tol=0.0001,
:                            verbose=0))],
:          verbose=False)

#+begin_src jupyter-python
svm_clf.predict([[5.5, 1.7]])
#+end_src

#+RESULTS:
: array([1.])

Generate the graph comparing different regularization settings:

#+begin_src jupyter-python
scaler = StandardScaler()
svm_clf1 = LinearSVC(C=1, loss="hinge", random_state=42)
svm_clf2 = LinearSVC(C=100, loss="hinge", random_state=42)

scaled_svm_clf1 = Pipeline([
    ("scaler", scaler),
    ("linear_svc", svm_clf1),
])
scaled_svm_clf2 = Pipeline([
    ("scaler", scaler),
    ("linear_svc", svm_clf2),
])

scaled_svm_clf1.fit(X, y)
scaled_svm_clf2.fit(X, y)
#+end_src

#+RESULTS:
:RESULTS:
: /home/ning/apps/conda/envs/ds/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
:   warnings.warn("Liblinear failed to converge, increase "
: Pipeline(memory=None,
:          steps=[('scaler',
:                  StandardScaler(copy=True, with_mean=True, with_std=True)),
:                 ('linear_svc',
:                  LinearSVC(C=100, class_weight=None, dual=True,
:                            fit_intercept=True, intercept_scaling=1,
:                            loss='hinge', max_iter=1000, multi_class='ovr',
:                            penalty='l2', random_state=42, tol=0.0001,
:                            verbose=0))],
:          verbose=False)
:END:

#+begin_src jupyter-python
print(scaler.mean_, scaler.scale_)
print(svm_clf1.coef_[0], svm_clf2.coef_[0])
print(svm_clf1.intercept_, svm_clf2.intercept_)
#+end_src

#+RESULTS:
: [3.758      1.19933333] [1.75940407 0.75969263]
: [1.63328027 2.38786154] [6.39953951 4.83584243]
: [-2.50520656] [-6.97682194]

Convert to unscaled parameters

#+begin_src jupyter-python
b1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])
b2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])
w1 = svm_clf1.coef_[0] / scaler.scale_
w2 = svm_clf2.coef_[0] / scaler.scale_
svm_clf1.intercept_ = np.array([b1])
svm_clf2.intercept_ = np.array([b2])
svm_clf1.coef_ = np.array([w1])
svm_clf2.coef_ = np.array([w2])
#+end_src

#+RESULTS:

Find support vectors (LinearSVC does not do this automatically)

#+begin_src jupyter-python
t = y * 2 - 1
support_vector_idx1 = (t * (X @ w1 + b1) < 1).ravel()  # Index for all margin violations
support_vector_idx2 = (t * (X @ w2 + b2) < 1).ravel()
svm_clf1.support_vectors_ = X[support_vector_idx1]
svm_clf2.support_vectors_ = X[support_vector_idx2]
#+end_src

#+RESULTS:

Plot
#+begin_src jupyter-python
fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)

plt.sca(axes[0])
plt.plot(X[:, 0][y==1], X[:, 1][y==1], "g^", label="Iris virginica")
plt.plot(X[:, 0][y==0], X[:, 1][y==0], "bs", label="Iris versicolor")
plot_svc_decision_boundary(svm_clf1, 4, 5.9)
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="upper left", fontsize=14)
plt.title("$C = {}$".format(svm_clf1.C), fontsize=16)
plt.axis([4, 5.9, 0.8, 2.8])

plt.sca(axes[1])
plt.plot(X[:, 0][y==1], X[:, 1][y==1], "g^")
plt.plot(X[:, 0][y==0], X[:, 1][y==0], "bs")
plot_svc_decision_boundary(svm_clf2, 4, 5.99)
plt.xlabel("Petal length", fontsize=14)
plt.title("$C = {}$".format(svm_clf2.C), fontsize=16)
plt.axis([4, 5.9, 0.8, 2.8]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/4196a9bae7afcd148837692c890725b9d3a1c8ac.png]]


* Non-linear classification

*** Load and plot moon data

#+begin_src jupyter-python
def plot_dataset(X, y, axes):
    plt.plot(X[:, 0][y==0], X[:, 1][y==0], 'bs')
    plt.plot(X[:, 0][y==1], X[:, 1][y==1], 'g^')
    plt.axis(axes)
    plt.grid(True, which='both')
    plt.xlabel(r"$x_1$", fontsize=20)
    plt.ylabel(r"$x_2$", fontsize=20, rotation=0)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
X, y = datasets.make_moons(n_samples=100, noise=0.15, random_state=42)
plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/fcdee5104c84ba7be2d3df1e5d5de7b44b0c2309.png]]

*** Polynomial features with LinearSVC

#+begin_src jupyter-python
polynomial_svm_clf = Pipeline([
        ("poly_features", PolynomialFeatures(degree=3)),
        ("scaler", StandardScaler()),
        ("svm_clf", LinearSVC(C=10, loss="hinge", random_state=42))
    ])

polynomial_svm_clf.fit(X, y)
#+end_src

#+RESULTS:
:RESULTS:
: /home/ning/apps/conda/envs/ds/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
:   warnings.warn("Liblinear failed to converge, increase "
#+begin_example
Pipeline(memory=None,
         steps=[('poly_features',
                 PolynomialFeatures(degree=3, include_bias=True,
                                    interaction_only=False, order='C')),
                ('scaler',
                 StandardScaler(copy=True, with_mean=True, with_std=True)),
                ('svm_clf',
                 LinearSVC(C=10, class_weight=None, dual=True,
                           fit_intercept=True, intercept_scaling=1,
                           loss='hinge', max_iter=1000, multi_class='ovr',
                           penalty='l2', random_state=42, tol=0.0001,
                           verbose=0))],
         verbose=False)
#+end_example
:END:

#+begin_src jupyter-python
def plot_predictions(clf, axes):
    x0s = np.linspace(axes[0], axes[1], 100)
    x1s = np.linspace(axes[2], axes[3], 100)
    x0, x1 = np.meshgrid(x0s, x1s)
    X = np.c_[x0.ravel(), x1.ravel()]
    y_pred = clf.predict(X).reshape(x0.shape)
    y_decision = clf.decision_function(X).reshape(x0.shape)
    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)
    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)

plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.5, -1, 1.5]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/bd45e2d465401fdab1c0486a1b2459606f049c45.png]]

*** Polynomial Kernel

#+begin_src jupyter-python
poly_kernel_svm_clf = Pipeline([
    ('scaler', StandardScaler()),
    ('svm_clf', SVC(kernel='poly', degree=3, coef0=1, C=5))
])
poly_kernel_svm_clf.fit(X, y)
#+end_src

#+RESULTS:
: Pipeline(memory=None,
:          steps=[('scaler',
:                  StandardScaler(copy=True, with_mean=True, with_std=True)),
:                 ('svm_clf',
:                  SVC(C=5, break_ties=False, cache_size=200, class_weight=None,
:                      coef0=1, decision_function_shape='ovr', degree=3,
:                      gamma='scale', kernel='poly', max_iter=-1,
:                      probability=False, random_state=None, shrinking=True,
:                      tol=0.001, verbose=False))],
:          verbose=False)

#+begin_src jupyter-python
poly100_kernel_svm_clf = Pipeline([
    ('scaler', StandardScaler()),
    ('svm_clf', SVC(kernel='poly', degree=10, coef0=100, C=5))
])
poly100_kernel_svm_clf.fit(X, y)
#+end_src

#+RESULTS:
: Pipeline(memory=None,
:          steps=[('scaler',
:                  StandardScaler(copy=True, with_mean=True, with_std=True)),
:                 ('svm_clf',
:                  SVC(C=5, break_ties=False, cache_size=200, class_weight=None,
:                      coef0=100, decision_function_shape='ovr', degree=10,
:                      gamma='scale', kernel='poly', max_iter=-1,
:                      probability=False, random_state=None, shrinking=True,
:                      tol=0.001, verbose=False))],
:          verbose=False)

#+begin_src jupyter-python
fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)

plt.sca(axes[0])
plot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])
plt.title(r"$d=3, r=1, C=5$", fontsize=18)

plt.sca(axes[1])
plot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])
plt.title(r"$d=10, r=100, C=5$", fontsize=18)
plt.ylabel("");
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/d4accb1fa1b6c1d29bfd2197e5cafd825e5c6359.png]]

*** Gaussian RBF kernel

#+begin_src jupyter-python
def gaussian_rbf(x, landmark, gamma):
    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
rbf_kernel_svm_clf = Pipeline([
    ('scaler', StandardScaler()),
    ('svm_clf', SVC(kernel='rbf', gamma=5, C=0.001)),
])
rbf_kernel_svm_clf.fit(X, y)
#+end_src

#+RESULTS:
#+begin_example
Pipeline(memory=None,
         steps=[('scaler',
                 StandardScaler(copy=True, with_mean=True, with_std=True)),
                ('svm_clf',
                 SVC(C=0.001, break_ties=False, cache_size=200,
                     class_weight=None, coef0=0.0,
                     decision_function_shape='ovr', degree=3, gamma=5,
                     kernel='rbf', max_iter=-1, probability=False,
                     random_state=None, shrinking=True, tol=0.001,
                     verbose=False))],
         verbose=False)
#+end_example

#+begin_src jupyter-python
gamma1, gamma2 = 0.1, 5
C1, C2 = 0.001, 1000
hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)

svm_clfs = []
for gamma, C in hyperparams:
    rbf_kernel_svm_clf = Pipeline([
            ("scaler", StandardScaler()),
            ("svm_clf", SVC(kernel="rbf", gamma=gamma, C=C))
        ])
    rbf_kernel_svm_clf.fit(X, y)
    svm_clfs.append(rbf_kernel_svm_clf)

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)

for i, svm_clf in enumerate(svm_clfs):
    plt.sca(axes[i // 2, i % 2])
    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])
    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])
    gamma, C = hyperparams[i]
    plt.title(r"$\gamma = {}, C = {}$".format(gamma, C), fontsize=16);
    if i in (0, 1):
        plt.xlabel("");
    if i in (1, 3):
        plt.ylabel("");
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2506fbf9f8551cd28fe695966a1b5fe966c64a52.png]]


* Regression

** LinearSVR

Generate data.
#+begin_src jupyter-python
np.random.seed(42)
m = 50
X = 2 * np.random.rand(m, 1)
y = (4 + 3 * X + np.random.randn(m, 1)).ravel()
#+end_src

#+RESULTS:

#+begin_src jupyter-python
svm_reg = LinearSVR(epsilon=1.5, random_state=42)
svm_reg.fit(X, y)
#+end_src

#+RESULTS:
: LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,
:           intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,
:           random_state=42, tol=0.0001, verbose=0)

#+begin_src jupyter-python
svm_reg1 = LinearSVR(epsilon=1.5, random_state=42)
svm_reg2 = LinearSVR(epsilon=0.5, random_state=42)
svm_reg1.fit(X, y)
svm_reg2.fit(X, y)

def find_support_vectors(svm_reg, X, y):
    y_pred = svm_reg.predict(X)
    off_margin = (np.abs(y - y_pred) >= svm_reg.epsilon)
    return np.argwhere(off_margin)

svm_reg1.support_ = find_support_vectors(svm_reg1, X, y)  # support_ attributes are indices of support_vectors_.
svm_reg2.support_ = find_support_vectors(svm_reg2, X, y)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
eps_x1 = 1
eps_y_pred = svm_reg1.predict([[eps_x1]])

def plot_svm_regression(svm_reg, X, y, axes):
    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)
    y_pred = svm_reg.predict(x1s)
    plt.plot(x1s, y_pred, "k-", linewidth=2, label=r"$\hat{y}$")
    plt.plot(x1s, y_pred + svm_reg.epsilon, "k--")
    plt.plot(x1s, y_pred - svm_reg.epsilon, "k--")
    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')
    plt.plot(X, y, "bo")
    plt.xlabel(r"$x_1$", fontsize=18)
    plt.legend(loc="upper left", fontsize=18)
    plt.axis(axes)

fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)
plt.sca(axes[0])
plot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11])
plt.title(r"$\epsilon = {}$".format(svm_reg1.epsilon), fontsize=18)
plt.ylabel(r"$y$", fontsize=18, rotation=0)
plt.annotate(
        '', xy=(eps_x1, eps_y_pred), xycoords='data',
        xytext=(eps_x1, eps_y_pred - svm_reg1.epsilon),
        textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5}
    )
plt.text(0.91, 5.6, r"$\epsilon$", fontsize=20)
plt.sca(axes[1])
plot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])
plt.title(r"$\epsilon = {}$".format(svm_reg2.epsilon), fontsize=18);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/be8596158d50fc6ebc530848895d49897c28b6f6.png]]

** Polynomial Kernel SVR

#+begin_src jupyter-python
np.random.seed(42)
m = 100
X = 2 * np.random.rand(m, 1) - 1
y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()
#+end_src

#+RESULTS:

#+begin_src jupyter-python
svm_poly_reg1 = SVR(kernel="poly", degree=2, C=100, epsilon=0.1, gamma="scale")
svm_poly_reg2 = SVR(kernel="poly", degree=2, C=0.01, epsilon=0.1, gamma="scale")
svm_poly_reg1.fit(X, y)
svm_poly_reg2.fit(X, y)
#+end_src

#+RESULTS:
: SVR(C=0.01, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='scale',
:     kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)

#+begin_src jupyter-python
fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)
plt.sca(axes[0])
plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])
plt.title(r"degree={}, $C$={}, $\epsilon$ = {}".format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon), fontsize=18)
plt.ylabel(r"$y$", fontsize=18, rotation=0)
plt.sca(axes[1])
plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])
plt.title(r"degree={}, $C$={}, $\epsilon$ = {}".format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon), fontsize=18);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/56d852dc098ffd46ff1e2d65cb7b240cd09a0038.png]]

* Under the hood

#+begin_src jupyter-python
iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.float64)  # Iris virginica
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def plot_3D_decision_function(ax, w, b, x1_lim=[4, 6], x2_lim=[0.8, 2.8]):
    x1_in_bounds = (X[:, 0] > x1_lim[0]) & (X[:, 0] < x1_lim[1])
    X_crop = X[x1_in_bounds]
    y_crop = y[x1_in_bounds]
    x1s = np.linspace(x1_lim[0], x1_lim[1], 20)
    x2s = np.linspace(x2_lim[0], x2_lim[1], 20)
    x1, x2 = np.meshgrid(x1s, x2s)
    xs = np.c_[x1.ravel(), x2.ravel()]
    df = (xs.dot(w) + b).reshape(x1.shape)  # decision function
    m = 1 / np.linalg.norm(w)
    boundary_x2s = -x1s*(w[0]/w[1])-b/w[1]  # get x2 by setting decision = 0
    margin_x2s_1 = -x1s*(w[0]/w[1])-(b-1)/w[1]
    margin_x2s_2 = -x1s*(w[0]/w[1])-(b+1)/w[1]
    ax.plot_surface(x1s, x2, np.zeros_like(x1),
                    color="b", alpha=0.2, cstride=100, rstride=100)  # plot zero surface
    ax.plot(x1s, boundary_x2s, 0, "k-", linewidth=2, label=r"$h=0$")
    ax.plot(x1s, margin_x2s_1, 0, "k--", linewidth=2, label=r"$h=\pm 1$")
    ax.plot(x1s, margin_x2s_2, 0, "k--", linewidth=2)
    ax.plot(X_crop[:, 0][y_crop==1], X_crop[:, 1][y_crop==1], 0, "g^")
    ax.plot_wireframe(x1, x2, df, alpha=0.3, color="k")  # plot decision function surface
    ax.plot(X_crop[:, 0][y_crop==0], X_crop[:, 1][y_crop==0], 0, "bs")
    ax.axis(x1_lim + x2_lim)
    ax.text(4.5, 2.5, 3.8, "Decision function $h$", fontsize=12)
    ax.set_xlabel(r"Petal length", fontsize=12, labelpad=10)
    ax.set_ylabel(r"Petal width", fontsize=12, labelpad=10)
    ax.set_zlabel(r"$h = \mathbf{w}^T \mathbf{x} + b$", fontsize=12, labelpad=5)
    ax.legend(loc="upper left", fontsize=10)

fig = plt.figure(figsize=(11, 6))
ax1 = fig.add_subplot(111, projection='3d')
plot_3D_decision_function(ax1, w=svm_clf2.coef_[0], b=svm_clf2.intercept_[0]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a3780fd6a598287f1d0d0ac01dc7b64bef185140.png]]


* Exercises

** 8. Train a ~LinearSVC~ on a linearly separable dataset. Then train an ~SVC~ and a ~SGDClassifier~ on the same dataset. See if you can get them to produce roughly the same model.

Load Iris setosa or veriscolor data.
#+begin_src jupyter-python
iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = iris["target"]

setosa_or_versicolor = (y == 0) | (y == 1)
X = X[setosa_or_versicolor]
y = y[setosa_or_versicolor]
#+end_src

#+RESULTS:

Training models.
#+begin_src jupyter-python
C = 5  # regularization parameter for SVM
alpha = 1  # regularization paramter for SGDClassifier

lin_clf = LinearSVC(loss='hinge', C=C, random_state=42)
svm_clf = SVC(kernel='linear', C=C)
sgd_clf = SGDClassifier(
    loss='hinge', learning_rate="constant", eta0=0.001, alpha=alpha, max_iter=1000, tol=1e-3, random_state=42)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

lin_clf.fit(X_scaled, y)
svm_clf.fit(X_scaled, y)
sgd_clf.fit(X_scaled, y)

print("LinearSVC:                   ", lin_clf.intercept_, lin_clf.coef_)
print("SVC:                         ", svm_clf.intercept_, svm_clf.coef_)
print("SGDClassifier(alpha={:.5f}):".format(sgd_clf.alpha), sgd_clf.intercept_, sgd_clf.coef_)
#+end_src

#+RESULTS:
: LinearSVC:                    [0.28475098] [[1.05364854 1.09903804]]
: SVC:                          [0.31896852] [[1.1203284  1.02625193]]
: SGDClassifier(alpha=1.00000): [-0.039] [[0.49362829 0.47333832]]

Plot decision boundaries.

#+begin_src jupyter-python
# Compute the slope and bias of each decision boundary
w1 = (
    -lin_clf.coef_[0, 0] / lin_clf.coef_[0, 1]
)  # x1 = (-w0/w1) * x0  on decision boundary
b1 = -lin_clf.intercept_[0] / lin_clf.coef_[0, 1]  # intercept at (x0=0, x1=-b/w1)
w2 = -svm_clf.coef_[0, 0] / svm_clf.coef_[0, 1]
b2 = -svm_clf.intercept_[0] / svm_clf.coef_[0, 1]
w3 = -sgd_clf.coef_[0, 0] / sgd_clf.coef_[0, 1]
b3 = -sgd_clf.intercept_[0] / sgd_clf.coef_[0, 1]

# Transform the decision boundary lines back to the original scale
line1 = scaler.inverse_transform([[-10, -10 * w1 + b1], [10, 10 * w1 + b1]])
line2 = scaler.inverse_transform([[-10, -10 * w2 + b2], [10, 10 * w2 + b2]])
line3 = scaler.inverse_transform([[-10, -10 * w3 + b3], [10, 10 * w3 + b3]])

# Plot all three decision boundaries
plt.figure(figsize=(9, 3))
plt.plot(line1[:, 0], line1[:, 1], "k:", label="LinearSVC")
plt.plot(line2[:, 0], line2[:, 1], "b--", linewidth=2, label="SVC")
plt.plot(line3[:, 0], line3[:, 1], "r-", label="SGDClassifier")
plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], "bs")  # label="Iris versicolor"
plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], "yo")  # label="Iris setosa"
plt.xlabel("Petal length", fontsize=10)
plt.ylabel("Petal width", fontsize=10)
plt.legend(loc="upper center", fontsize=10)
plt.axis([0, 5.5, 0, 2.25]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2b1e6a34abdd05d5bdeb861edecb764b3dbf7c1d.png]]


** 9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus-the-rest to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach?

#+begin_src jupyter-python
mnist = datasets.fetch_openml('mnist_784', version=1, cache=True)

X = mnist["data"]
y = mnist["target"].astype(np.uint8)

X_train = X[:60000]
y_train = y[:60000]
X_test = X[60000:]
y_test = y[60000:]
#+end_src

#+RESULTS:

#+begin_src jupyter-python
lin_clf = LinearSVC(random_state=42)
lin_clf.fit(X_train, y_train)
#+end_src

#+RESULTS:
:RESULTS:
: /home/ning/apps/conda/envs/ds/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
:   warnings.warn("Liblinear failed to converge, increase "
: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
:           intercept_scaling=1, loss='squared_hinge', max_iter=1000,
:           multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,
:           verbose=0)
:END:

#+begin_src jupyter-python
y_pred = lin_clf.predict(X_train)
accuracy_score(y_train, y_pred)
#+end_src

#+RESULTS:
: 0.8348666666666666

#+begin_src jupyter-python
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.astype(np.float32))
X_test_scaled = scaler.transform(X_test.astype(np.float32))
#+end_src

#+RESULTS:

#+begin_src jupyter-python
lin_clf = LinearSVC(random_state=42)
lin_clf.fit(X_train_scaled, y_train)
y_pred = lin_clf.predict(X_train_scaled)
accuracy_score(y_train, y_pred)
#+end_src

#+RESULTS:
:RESULTS:
: /home/ning/apps/conda/envs/ds/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
:   warnings.warn("Liblinear failed to converge, increase "
: 0.9217333333333333
:END:

#+begin_src jupyter-python
svm_clf = SVC(gamma='scale')  # default RBF kernel
svm_clf.fit(X_train_scaled[:10000], y_train[:10000])
y_pred = svm_clf.predict(X_train_scaled)
accuracy_score(y_train, y_pred)
#+end_src

#+RESULTS:
: 0.9455333333333333

#+begin_src jupyter-python
param_distr = {
    "gamma": stats.reciprocal(0.001, 0.1),
    "C": stats.uniform(1, 10),
}
rnd_search_cv = RandomizedSearchCV(svm_clf, param_distr, n_iter=10, cv=3, verbose=2)
rnd_search_cv.fit(X_train_scaled[:1000], y_train[:1000])
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Fitting 3 folds for each of 10 candidates, totalling 30 fits
[CV] END ....C=7.689240596630997, gamma=0.053497702356221796; total time=   0.2s
[CV] END ....C=7.689240596630997, gamma=0.053497702356221796; total time=   0.2s
[CV] END ....C=7.689240596630997, gamma=0.053497702356221796; total time=   0.2s
[CV] END ...C=3.3018526824155527, gamma=0.009962922677707813; total time=   0.2s
[CV] END ...C=3.3018526824155527, gamma=0.009962922677707813; total time=   0.2s
[CV] END ...C=3.3018526824155527, gamma=0.009962922677707813; total time=   0.2s
[CV] END ....C=6.720041992091831, gamma=0.034443559676607075; total time=   0.2s
[CV] END ....C=6.720041992091831, gamma=0.034443559676607075; total time=   0.2s
[CV] END ....C=6.720041992091831, gamma=0.034443559676607075; total time=   0.2s
[CV] END ....C=1.4360377175443375, gamma=0.09752164572338576; total time=   0.2s
[CV] END ....C=1.4360377175443375, gamma=0.09752164572338576; total time=   0.2s
[CV] END ....C=1.4360377175443375, gamma=0.09752164572338576; total time=   0.2s
[CV] END ....C=5.69944513990943, gamma=0.0036234367402567607; total time=   0.2s
[CV] END ....C=5.69944513990943, gamma=0.0036234367402567607; total time=   0.2s
[CV] END ....C=5.69944513990943, gamma=0.0036234367402567607; total time=   0.2s
[CV] END .....C=9.83494022266259, gamma=0.031292304511228025; total time=   0.2s
[CV] END .....C=9.83494022266259, gamma=0.031292304511228025; total time=   0.2s
[CV] END .....C=9.83494022266259, gamma=0.031292304511228025; total time=   0.2s
[CV] END ...C=10.530718470239531, gamma=0.004586702893394872; total time=   0.2s
[CV] END ...C=10.530718470239531, gamma=0.004586702893394872; total time=   0.2s
[CV] END ...C=10.530718470239531, gamma=0.004586702893394872; total time=   0.2s
[CV] END ....C=6.527649668354899, gamma=0.013950344686768637; total time=   0.2s
[CV] END ....C=6.527649668354899, gamma=0.013950344686768637; total time=   0.2s
[CV] END ....C=6.527649668354899, gamma=0.013950344686768637; total time=   0.2s
[CV] END ..C=10.803315837160456, gamma=0.0014147917283055171; total time=   0.1s
[CV] END ..C=10.803315837160456, gamma=0.0014147917283055171; total time=   0.1s
[CV] END ..C=10.803315837160456, gamma=0.0014147917283055171; total time=   0.1s
[CV] END ....C=4.056970192871819, gamma=0.002408918252446824; total time=   0.2s
[CV] END ....C=4.056970192871819, gamma=0.002408918252446824; total time=   0.2s
[CV] END ....C=4.056970192871819, gamma=0.002408918252446824; total time=   0.2s
#+end_example
#+begin_example
RandomizedSearchCV(cv=3, error_score=nan,
                   estimator=SVC(C=1.0, break_ties=False, cache_size=200,
                                 class_weight=None, coef0=0.0,
                                 decision_function_shape='ovr', degree=3,
                                 gamma='scale', kernel='rbf', max_iter=-1,
                                 probability=False, random_state=None,
                                 shrinking=True, tol=0.001, verbose=False),
                   n_iter=10, n_jobs=None,
                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3d9a0c6070>,
                                        'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3d9b9c0070>},
                   pre_dispatch='2*n_jobs', random_state=None, refit=True,
                   return_train_score=False, scoring=None, verbose=2)
#+end_example
:END:

#+begin_src jupyter-python
rnd_search_cv.best_estimator_
#+end_src

#+RESULTS:
: SVC(C=10.803315837160456, break_ties=False, cache_size=200, class_weight=None,
:     coef0=0.0, decision_function_shape='ovr', degree=3,
:     gamma=0.0014147917283055171, kernel='rbf', max_iter=-1, probability=False,
:     random_state=None, shrinking=True, tol=0.001, verbose=False)

#+begin_src jupyter-python
rnd_search_cv.best_score_, rnd_search_cv.best_params_
#+end_src

#+RESULTS:
| 0.8639927352502204 | (C : 10.803315837160456 gamma : 0.0014147917283055171) |

#+begin_src jupyter-python
rnd_search_cv.best_estimator_.fit(X_train_scaled, y_train)
#+end_src

#+RESULTS:
: SVC(C=10.803315837160456, break_ties=False, cache_size=200, class_weight=None,
:     coef0=0.0, decision_function_shape='ovr', degree=3,
:     gamma=0.0014147917283055171, kernel='rbf', max_iter=-1, probability=False,
:     random_state=None, shrinking=True, tol=0.001, verbose=False)

#+begin_src jupyter-python
y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)
accuracy_score(y_train, y_pred)
#+end_src

#+RESULTS:
: 0.9994666666666666

#+begin_src jupyter-python
y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)
accuracy_score(y_test, y_pred)
#+end_src

#+RESULTS:
: 0.9724

** 10. Train an SVM regressor on the California housing dataset.

#+begin_src jupyter-python
housing = datasets.fetch_california_housing()
X = housing['data']
y = housing['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
lin_svr = LinearSVR(random_state=42)
lin_svr.fit(X_train_scaled, y_train)
y_pred = lin_svr.predict(X_train_scaled)
mse = mean_squared_error(y_train, y_pred)
print("RMSE = ", np.sqrt(mse))
#+end_src

#+RESULTS:
: RMSE =  0.9819256687727764
: /home/ning/apps/conda/envs/ds/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
:   warnings.warn("Liblinear failed to converge, increase "

#+begin_src jupyter-python
param_distributions = {
    "gamma": stats.reciprocal(0.001, 0.1),
    "C": stats.uniform(1, 10)}
rnd_search_cv = RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, cv=3, random_state=42, n_jobs=6)
rnd_search_cv.fit(X_train_scaled, y_train)
#+end_src

#+RESULTS:
:RESULTS:
: Fitting 3 folds for each of 10 candidates, totalling 30 fits
: RandomizedSearchCV(cv=3, error_score=nan,
:                    estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3,
:                                  epsilon=0.1, gamma='scale', kernel='rbf',
:                                  max_iter=-1, shrinking=True, tol=0.001,
:                                  verbose=False),
:                    n_iter=10, n_jobs=6,
:                    param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3daee20700>,
:                                         'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f3dafd05a00>},
:                    pre_dispatch='2*n_jobs', random_state=42, refit=True,
:                    return_train_score=False, scoring=None, verbose=2)
:END:

#+begin_src jupyter-python
rnd_search_cv.best_estimator_
#+end_src

#+RESULTS:
: SVR(C=4.745401188473625, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,
:     gamma=0.07969454818643928, kernel='rbf', max_iter=-1, shrinking=True,
:     tol=0.001, verbose=False)

#+begin_src jupyter-python
y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)
mse = mean_squared_error(y_train, y_pred)
np.sqrt(mse)
#+end_src

#+RESULTS:
: 0.5727524770785367

#+begin_src jupyter-python
y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)
mse = mean_squared_error(y_test, y_pred)
np.sqrt(mse)
#+end_src

#+RESULTS:
: 0.5929168385528745
