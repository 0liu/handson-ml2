#+TITLE: Practice Hands-on ML Chap08 Dimensionality Reduction

#+begin_src jupyter-python
# Py data
import numpy as np
import pandas as pd
from scipy import stats
import time

# Visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.patches import FancyArrowPatch
from mpl_toolkits.mplot3d import proj3d, Axes3D

# Scikit-learn
import sklearn
from sklearn.datasets import make_swiss_roll
from sklearn.datasets import fetch_openml

# Data processing
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Metrics and model selection
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV

# Models
from sklearn.base import clone
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA
from sklearn.manifold import LocallyLinearEmbedding, MDS, Isomap, TSNE
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier

# matplotlib and sklearn config
mpl.rc("axes", labelsize=10, linewidth=0.3)
mpl.rc("xtick", labelsize=10)
mpl.rc("ytick", labelsize=10)
mpl.rc("legend", fontsize=8, fancybox=True)
mpl.rc("figure", facecolor="white", dpi=120)

sklearn.set_config(print_changed_only=False)
#+end_src

#+RESULTS:


* Main Approaches

** Projection methods

*** Build 3D data set.

#+begin_src jupyter-python
np.random.seed(4)
m = 60
w1, w2 = 0.1, 0.3
noise = 0.1
angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5
X = np.empty((m, 3))
X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2
X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2
X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)
#+end_src

#+RESULTS:

*** PCA using SVD

#+begin_src jupyter-python
X_centered = X - X.mean(axis=0)  # 60 x 3
U, s, Vt = np.linalg.svd(X_centered)  # U: 60x60,  s: 3, Vt: 3x3
c1 = Vt.T[:, 0]
c2 = Vt.T[:, 1]

m, n = X.shape
S = np.zeros(X_centered.shape)
S[:n, :n] = np.diag(s)  # 60x3, s on first 3 rows
#+end_src

#+RESULTS:

#+begin_src jupyter-python
np.allclose(X_centered, U @ S @ Vt)
#+end_src

#+RESULTS:
: True

#+begin_src jupyter-python
W2 = Vt.T[:, :2]  # 3 x 2
X2D_using_svd = X_centered @ W2  # 60 x 2
#+end_src

#+RESULTS:

*** PCA using Scikit-learn

#+begin_src jupyter-python
pca = PCA(n_components=2)
X2D = pca.fit_transform(X)
X2D[:5]
#+end_src

#+RESULTS:
: array([[ 1.26203346,  0.42067648],
:        [-0.08001485, -0.35272239],
:        [ 1.17545763,  0.36085729],
:        [ 0.89305601, -0.30862856],
:        [ 0.73016287, -0.25404049]])

- Same but vector directions are opposite.

#+begin_src jupyter-python
X2D_using_svd[:5]
#+end_src

#+RESULTS:
: array([[-1.26203346, -0.42067648],
:        [ 0.08001485,  0.35272239],
:        [-1.17545763, -0.36085729],
:        [-0.89305601,  0.30862856],
:        [-0.73016287,  0.25404049]])

#+begin_src jupyter-python
np.allclose(X2D, -X2D_using_svd)
#+end_src

#+RESULTS:
: True

*** Reconstruct 3D points from 2D subspace.

#+begin_src jupyter-python
X3D_inv = pca.inverse_transform(X2D)
np.allclose(X3D_inv, X)
#+end_src

#+RESULTS:
: False

#+begin_src jupyter-python
print("Construction error = ", np.mean(np.sum(np.square(X3D_inv - X), axis=1)))
#+end_src

#+RESULTS:
: Construction error =  0.010170337792848549

- Manual reconstruct in the SVD approach

#+begin_src jupyter-python
X3D_inv_using_svd = X2D_using_svd @ Vt[:2, :]
np.allclose(X3D_inv_using_svd, X3D_inv - pca.mean_)
#+end_src

#+RESULTS:
: True

*** Principal Components

#+begin_src jupyter-python
print(pca.components_)
print(Vt[:2])
#+end_src

#+RESULTS:
: [[-0.93636116 -0.29854881 -0.18465208]
:  [ 0.34027485 -0.90119108 -0.2684542 ]]
: [[ 0.93636116  0.29854881  0.18465208]
:  [-0.34027485  0.90119108  0.2684542 ]]

*** Explained Variance Ratio

#+begin_src jupyter-python
print(pca.explained_variance_)
print(pca.explained_variance_ratio_)
print(1 - pca.explained_variance_ratio_.sum())
#+end_src

#+RESULTS:
: [0.77830975 0.1351726 ]
: [0.84248607 0.14631839]
: 0.011195535570688975

- Explained variance ratio in SVD approach

#+begin_src jupyter-python
np.square(s) / np.square(s).sum()
#+end_src

#+RESULTS:
: array([0.84248607, 0.14631839, 0.01119554])

*** Plots

#+begin_src jupyter-python
class Arrow3D(FancyArrowPatch):
    def __init__(self, xs, ys, zs, *args, **kwargs):
        super().__init__((0,0), (0,0), *args, **kwargs)
        self._verts3d = xs, ys, zs

    def draw(self, renderer):
        xs3d, ys3d, zs3d = self._verts3d
        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)
        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))
        super().draw(renderer)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
axes = [-1.8, 1.8, -1.3, 1.3, -1.0, 1.0]

x1s = np.linspace(axes[0], axes[1], 10)
x2s = np.linspace(axes[2], axes[3], 10)
x1, x2 = np.meshgrid(x1s, x2s)

C = pca.components_
R = C.T.dot(C)
z = (R[0, 2] * x1 + R[1, 2] * x2) / (1 - R[2, 2])
#+end_src

#+RESULTS:

#+begin_src jupyter-python

fig = plt.figure(figsize=(6, 3.8))
ax = fig.add_subplot(111, projection='3d')

X3D_above = X[X[:, 2] > X3D_inv[:, 2]]
X3D_below = X[X[:, 2] <= X3D_inv[:, 2]]

ax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], "bo", alpha=0.5)

ax.plot_surface(x1, x2, z, alpha=0.2, color="k")
np.linalg.norm(C, axis=0)
ax.add_artist(Arrow3D([0, C[0, 0]],[0, C[0, 1]],[0, C[0, 2]], mutation_scale=15, lw=1, arrowstyle="-|>", color="k"))
ax.add_artist(Arrow3D([0, C[1, 0]],[0, C[1, 1]],[0, C[1, 2]], mutation_scale=15, lw=1, arrowstyle="-|>", color="k"))
ax.plot([0], [0], [0], "k.")

for i in range(m):
    if X[i, 2] > X3D_inv[i, 2]:
        ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], "k-")
    else:
        ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], "k-", color="#505050")

ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], "k+")
ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], "k.")
ax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], "bo")
ax.set_xlabel("$x_1$", fontsize=18, labelpad=10)
ax.set_ylabel("$x_2$", fontsize=18, labelpad=10)
ax.set_zlabel("$x_3$", fontsize=18, labelpad=10)
ax.set_xlim(axes[0:2])
ax.set_ylim(axes[2:4])
ax.set_zlim(axes[4:6]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/83079e90f8e391df9399b2f6707a6cb746d26596.png]]

#+begin_src jupyter-python
fig = plt.figure()
ax = fig.add_subplot(111, aspect='equal')
ax.plot(X2D[:, 0], X2D[:, 1], "k+")
ax.plot(X2D[:, 0], X2D[:, 1], "k.")
ax.plot([0], [0], "ko")
ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')
ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')
ax.set_xlabel("$z_1$", fontsize=18)
ax.set_ylabel("$z_2$", fontsize=18, rotation=0)
ax.axis([-1.5, 1.3, -1.2, 1.2])
ax.grid(True);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/e3676f35ba77c4b36a7cb31fccdf4d187a1e8703.png]]

** Manifold learning

#+begin_src jupyter-python
X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)

axes = [-11.5, 14, -2, 23, -12, 15]

fig = plt.figure(figsize=(6, 5))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot)
ax.view_init(10, -70)
ax.set_xlabel("$x_1$", fontsize=18)
ax.set_ylabel("$x_2$", fontsize=18)
ax.set_zlabel("$x_3$", fontsize=18)
ax.set_xlim(axes[0:2])
ax.set_ylim(axes[2:4])
ax.set_zlim(axes[4:6]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/784c53d606e208f75292574fb6cf65d0ba2be63f.png]]

#+begin_src jupyter-python
plt.figure(figsize=(11, 4))
plt.subplot(121)
plt.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.hot)
plt.axis(axes[:4])
plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$x_2$", fontsize=18, rotation=0)
plt.grid(True)

plt.subplot(122)
plt.scatter(t, X[:, 1], c=t, cmap=plt.cm.hot)
plt.axis([4, 15, axes[2], axes[3]])
plt.xlabel("$z_1$", fontsize=18)
plt.grid(True);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/ce1937df38b7ded9ad6e8d29c0f2480025f34525.png]]


* PCA
** Choosing the right number of dimensions; Data compression

#+begin_src jupyter-python
mnist = fetch_openml('mnist_784', version=1)
mnist.target = mnist.target.astype(np.uint8)

X = mnist['data']
y = mnist['target']
X_train, X_test, y_train, y_test = train_test_split(X, y)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1  # np.argmax finds the first occurence of cumsum >= 0.95
d
#+end_src

#+RESULTS:
: 154

#+begin_src jupyter-python
plt.figure(figsize=(6,4))
plt.plot(cumsum, linewidth=3)
plt.axis([0, 400, 0, 1])
plt.xlabel("Dimensions")
plt.ylabel("Explained Variance")
plt.plot([d, d], [0, 0.95], "k:")
plt.plot([0, d], [0.95, 0.95], "k:")
plt.plot(d, 0.95, "ko")
plt.annotate("Elbow", xy=(65, 0.85), xytext=(70, 0.7),
             arrowprops=dict(arrowstyle="->"), fontsize=16)
plt.grid(True);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/48869a44ca448b9a93fca94441c90ed5e8eb4d3f.png]]


#+begin_src jupyter-python
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_train)
print("d = ", pca.n_components_)
print("Explained Variance Ratio = ", np.sum(pca.explained_variance_ratio_))
#+end_src

#+RESULTS:
: d =  154
: Explained Variance Ratio =  0.9503684424557436

#+begin_src jupyter-python
pca = PCA(n_components=154)
X_reduced_pca = pca.fit_transform(X_train)
X_recovered_pca = pca.inverse_transform(X_reduced_pca)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def plot_digits(instances, images_per_row=5, **options):
    size = 28
    images_per_row = min(len(instances), images_per_row)
    images = [instance.reshape(size,size) for instance in instances]
    n_rows = (len(instances) - 1) // images_per_row + 1
    row_images = []
    n_empty = n_rows * images_per_row - len(instances)
    images.append(np.zeros((size, size * n_empty)))
    for row in range(n_rows):
        rimages = images[row * images_per_row : (row + 1) * images_per_row]
        row_images.append(np.concatenate(rimages, axis=1))
    image = np.concatenate(row_images, axis=0)
    plt.imshow(image, cmap = mpl.cm.binary, **options)
    plt.axis("off")

plt.figure(figsize=(7, 4))
plt.subplot(121)
plot_digits(X_train[::2100].values)
plt.title("Original", fontsize=16)
plt.subplot(122)
plot_digits(X_recovered_pca[::2100])
plt.title("Compressed", fontsize=16);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/724a3ce04ac8c0af36649084b2368c4d59f6718b.png]]


** Incremental PCA

#+begin_src jupyter-python
n_batches = 100
inc_pca = IncrementalPCA(n_components=154)
for X_batch in np.array_split(X_train, n_batches):
    print(".", end="")
    inc_pca.partial_fit(X_batch)
X_reduced_inc_pca = inc_pca.transform(X_train)
#+end_src

#+RESULTS:
: ....................................................................................................

#+begin_src jupyter-python
X_recovered_inc_pca = inc_pca.inverse_transform(X_reduced_inc_pca)
plt.figure(figsize=(7, 4))
plt.subplot(121)
plot_digits(X_train[::2100].values)
plt.subplot(122)
plot_digits(X_recovered_inc_pca[::2100])
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/9b1fb483b57b6fd71d453fa11777a946f6969aa8.png]]

Compare regular PCA and IPCA for means and X_reduced
#+begin_src jupyter-python
print(np.allclose(pca.mean_, inc_pca.mean_))
print(np.allclose(X_reduced_pca, X_reduced_inc_pca))
#+end_src

#+RESULTS:
: True
: False

*** Using ~memmap()~
Create the memmap() structure and copy the MNIST data into it.
#+begin_src jupyter-python
filename = "my_mnist.data"
X_mm = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)
X_mm[:] = X_train
X_mm.flush()
#+end_src

#+RESULTS:

#+begin_src jupyter-python
X_mm = np.memmap(filename, dtype="float32", mode="readonly", shape=X_train.shape)
batch_size = X_train.shape[0] // n_batches
inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
inc_pca.fit(X_mm)
#+end_src

#+RESULTS:
: IncrementalPCA(batch_size=525, copy=True, n_components=154, whiten=False)

** Randomized PCA
#+begin_src jupyter-python
rnd_pca = PCA(n_components=154, svd_solver="randomized", random_state=42)
X_reduced_rnd_pca = rnd_pca.fit_transform(X_train)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
print(np.allclose(pca.mean_, rnd_pca.mean_))
print(np.allclose(X_reduced_pca, X_reduced_rnd_pca))
print(rnd_pca.explained_variance_ratio_.sum())
#+end_src

#+RESULTS:
: True
: False
: 0.9500220342169728

** Time Complexity - PCA vs. IPCA vs. RPCA

#+begin_src jupyter-python
for n_components in (2, 10, 154):
    print(f"{n_components = }")
    regular_pca = PCA(n_components=n_components)
    inc_pca = IncrementalPCA(n_components=n_components, batch_size=500)
    rnd_pca = PCA(n_components=n_components, random_state=42, svd_solver="randomized")

    for pca in (regular_pca, inc_pca, rnd_pca):
        t1 = time.time()
        pca.fit(X_train)
        t2 = time.time()
        print(f"    {pca.__class__.__name__}: {t2-t1:.1f} seconds")
#+end_src

#+RESULTS:
#+begin_example
n_components = 2
    PCA: 0.5 seconds
    IncrementalPCA: 3.2 seconds
    PCA: 0.4 seconds
n_components = 10
    PCA: 0.5 seconds
    IncrementalPCA: 3.4 seconds
    PCA: 0.5 seconds
n_components = 154
    PCA: 1.2 seconds
    IncrementalPCA: 4.4 seconds
    PCA: 1.2 seconds
#+end_example

#+begin_src jupyter-python
times_rpca = []
times_pca = []
sizes = [1000, 10000, 20000, 30000, 40000, 50000, 70000, 100000, 200000, 500000, 1000000]
for n_samples in sizes:
    X = np.random.randn(n_samples, 5)
    pca = PCA(n_components = 2, svd_solver="randomized", random_state=42)
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_rpca.append(t2 - t1)
    pca = PCA(n_components = 2)
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_pca.append(t2 - t1)

plt.plot(sizes, times_rpca, "b-o", label="RPCA")
plt.plot(sizes, times_pca, "r-s", label="PCA")
plt.xlabel("n_samples")
plt.ylabel("Training time")
plt.legend(loc="upper left")
plt.title("PCA and Randomized PCA time complexity ");
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/8caa6e55f307187343e5c64379e63f2474fc2c6a.png]]

#+begin_src jupyter-python
times_rpca = []
times_pca = []
sizes = [1000, 2000, 3000, 4000, 5000, 6000]
for n_features in sizes:
    X = np.random.randn(2000, n_features)
    pca = PCA(n_components = 2, random_state=42, svd_solver="randomized")
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_rpca.append(t2 - t1)
    pca = PCA(n_components = 2)
    t1 = time.time()
    pca.fit(X)
    t2 = time.time()
    times_pca.append(t2 - t1)

plt.plot(sizes, times_rpca, "b-o", label="RPCA")
plt.plot(sizes, times_pca, "r-s", label="PCA")
plt.xlabel("n_features")
plt.ylabel("Training time")
plt.legend(loc="upper left")
plt.title("PCA and Randomized PCA time complexity ");
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/55c3f0e40c30ed7fa4ebf58626e914c47c266e8c.png]]


* Kernel PCA

#+begin_src jupyter-python
X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)

lin_pca = KernelPCA(n_components = 2, kernel="linear", fit_inverse_transform=True)
rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433, fit_inverse_transform=True)
sig_pca = KernelPCA(n_components = 2, kernel="sigmoid", gamma=0.001, coef0=1, fit_inverse_transform=True)

y = t > 6.9

plt.figure(figsize=(11, 4))
for subplot, pca, title in ((131, lin_pca, "Linear kernel"), (132, rbf_pca, "RBF kernel, $\gamma=0.04$"), (133, sig_pca, "Sigmoid kernel, $\gamma=10^{-3}, r=1$")):
    X_reduced = pca.fit_transform(X)
    if subplot == 132:
        X_reduced_rbf = X_reduced

    plt.subplot(subplot)
    #plt.plot(X_reduced[y, 0], X_reduced[y, 1], "gs")
    #plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], "y^")
    plt.title(title, fontsize=14)
    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
    plt.xlabel("$z_1$", fontsize=18)
    if subplot == 131:
        plt.ylabel("$z_2$", fontsize=18, rotation=0)
    plt.grid(True)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/84e9f20f0a3b7124ea8fe16738a3b4a212619dba.png]]

#+begin_src jupyter-python
plt.figure(figsize=(6, 5))

X_inverse = rbf_pca.inverse_transform(X_reduced_rbf)

ax = plt.subplot(111, projection='3d')
ax.view_init(10, -70)
ax.scatter(X_inverse[:, 0], X_inverse[:, 1], X_inverse[:, 2], c=t, cmap=plt.cm.hot, marker="x")
ax.set_xlabel("")
ax.set_ylabel("")
ax.set_zlabel("")
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_zticklabels([]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/98a4c669823ef8de70753e1462e0718ec62a51f9.png]]

#+begin_src jupyter-python
X_reduced = rbf_pca.fit_transform(X)

plt.figure(figsize=(11, 4))
plt.subplot(132)
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot, marker="x")
plt.xlabel("$z_1$", fontsize=18)
plt.ylabel("$z_2$", fontsize=18, rotation=0)
plt.grid(True)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/ea60843dd54fe16f433c52d9554c9f976f5747bd.png]]

Use final classification score to search best model and parameters

#+begin_src jupyter-python
clf = Pipeline([
        ("kpca", KernelPCA(n_components=2)),
        ("log_reg", LogisticRegression(solver="lbfgs"))
    ])

param_grid = [{
        "kpca__gamma": np.linspace(0.03, 0.05, 10),
        "kpca__kernel": ["rbf", "sigmoid"]
    }]

grid_search = GridSearchCV(clf, param_grid, cv=3)
grid_search.fit(X, y)
#+end_src

#+RESULTS:
#+begin_example
GridSearchCV(cv=3, error_score=nan,
             estimator=Pipeline(memory=None,
                                steps=[('kpca',
                                        KernelPCA(alpha=1.0, coef0=1,
                                                  copy_X=True, degree=3,
                                                  eigen_solver='auto',
                                                  fit_inverse_transform=False,
                                                  gamma=None, kernel='linear',
                                                  kernel_params=None,
                                                  max_iter=None, n_components=2,
                                                  n_jobs=None,
                                                  random_state=None,
                                                  remove_zero_eig=False,
                                                  tol=0)),
                                       ('log_reg',
                                        LogisticRegression(C=1.0,
                                                           cl...
                                                           random_state=None,
                                                           solver='lbfgs',
                                                           tol=0.0001,
                                                           verbose=0,
                                                           warm_start=False))],
                                verbose=False),
             n_jobs=None,
             param_grid=[{'kpca__gamma': array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,
       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),
                          'kpca__kernel': ['rbf', 'sigmoid']}],
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring=None, verbose=0)
#+end_example

#+begin_src jupyter-python
print(grid_search.best_params_)
#+end_src

#+RESULTS:
: {'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}

#+begin_src jupyter-python
rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433,
                    fit_inverse_transform=True)
X_reduced = rbf_pca.fit_transform(X)
X_preimage = rbf_pca.inverse_transform(X_reduced)
print("Reconstruction error = ", mean_squared_error(X, X_preimage))
#+end_src

#+RESULTS:
: Reconstruction error =  8.34185631447549e-27


* LLE

#+begin_src jupyter-python
X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=41)
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)
X_reduced = lle.fit_transform(X)

plt.title("Unrolled swiss roll using LLE", fontsize=14)
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
plt.xlabel("$z_1$", fontsize=18)
plt.ylabel("$z_2$", fontsize=18)
plt.axis([-0.065, 0.055, -0.1, 0.12])
plt.grid(True)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/587cbf47edc0f9920d3661b28149289b3fc6e3f3.png]]


* MDS, Isomap and t-SNE

#+begin_src jupyter-python
mds = MDS(n_components=2, random_state=42)
X_reduced_mds = mds.fit_transform(X)

isomap = Isomap(n_components=2)
X_reduced_isomap = isomap.fit_transform(X)

tsne = TSNE(n_components=2, random_state=42)
X_reduced_tsne = tsne.fit_transform(X)

lda = LinearDiscriminantAnalysis(n_components=2)
X_mnist = mnist["data"]
y_mnist = mnist["target"]
lda.fit(X_mnist, y_mnist)
X_reduced_lda = lda.transform(X_mnist)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
titles = ["MDS", "Isomap", "t-SNE"]

plt.figure(figsize=(11,4))

for subplot, title, X_reduced in zip((131, 132, 133), titles,
                                     (X_reduced_mds, X_reduced_isomap, X_reduced_tsne)):
    plt.subplot(subplot)
    plt.title(title, fontsize=14)
    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)
    plt.xlabel("$z_1$", fontsize=18)
    if subplot == 131:
        plt.ylabel("$z_2$", fontsize=18, rotation=0)
    plt.grid(True)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/e4bab52bc2f7492d9e6cc349e12960ae9cdd2a0c.png]]


* Exercises

** 9. PCA + RandomForest Classifier on MNIST

*** Load the MNIST dataset (introduced in chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing).

#+begin_src jupyter-python
X_train = mnist['data'][:60000]
y_train = mnist['target'][:60000]

X_test = mnist['data'][60000:]
y_test = mnist['target'][60000:]
#+end_src

#+RESULTS:

*** Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set.

#+begin_src jupyter-python
rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)

t0 = time.time()
rnd_clf.fit(X_train, y_train)
t1 = time.time()
print(f"Training time: {t1-t0:.2f} seconds.")
#+end_src

#+RESULTS:
: Training time: 22.33 seconds.

#+begin_src jupyter-python
y_pred = rnd_clf.predict(X_test)
accuracy_score(y_test, y_pred)
#+end_src

#+RESULTS:
: 0.9705

*** Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.

#+begin_src jupyter-python
pca = PCA(n_components=0.95)
X_train_reduced = pca.fit_transform(X_train)
#+end_src

#+RESULTS:

*** Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster?

#+begin_src jupyter-python
rnd_clf2 = RandomForestClassifier(n_estimators=100, random_state=42)
t0 = time.time()
rnd_clf2.fit(X_train_reduced, y_train)
t1 = time.time()
print(f"Training time: {t1-t0:.2f} seconds.")
#+end_src

#+RESULTS:
: Training time: 56.68 seconds.

Oh no! Training is actually more than twice slower now! How can that be? Well, as we saw in this chapter, dimensionality reduction does not always lead to faster training time: it depends on the dataset, the model and the training algorithm. See figure 8-6 (the manifold_decision_boundary_plot* plots above). If you try a softmax classifier instead of a random forest classifier, you will find that training time is reduced by a factor of 3 when using PCA. Actually, we will do this in a second, but first let's check the precision of the new random forest classifier.

*** Next evaluate the classifier on the test set: how does it compare to the previous classifier?

#+begin_src jupyter-python
X_test_reduced = pca.transform(X_test)

y_pred = rnd_clf2.predict(X_test_reduced)
accuracy_score(y_test, y_pred)
#+end_src

#+RESULTS:
: 0.9481

It is common for performance to drop slightly when reducing dimensionality, because we do lose some useful signal in the process. However, the performance drop is rather severe in this case. So PCA really did not help: it slowed down training and reduced performance. :(

*** Let's see if it helps when using softmax regression

#+begin_src jupyter-python
log_clf = LogisticRegression(multi_class="multinomial", solver="lbfgs", random_state=42)
t0 = time.time()
log_clf.fit(X_train, y_train)
t1 = time.time()
print(f"Training time: {t1-t0:.2f} seconds.")
#+end_src

#+RESULTS:
: Training time: 5.50 seconds.
: /home/ning/apps/conda/envs/ds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):
: STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
:
: Increase the number of iterations (max_iter) or scale the data as shown in:
:     https://scikit-learn.org/stable/modules/preprocessing.html
: Please also refer to the documentation for alternative solver options:
:     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
:   n_iter_i = _check_optimize_result(

#+begin_src jupyter-python
y_pred = log_clf.predict(X_test)
accuracy_score(y_test, y_pred)
#+end_src

#+RESULTS:
: 0.9255

Okay, so softmax regression takes much longer to train on this dataset than the random forest classifier, plus it performs worse on the test set. But that's not what we are interested in right now, we want to see how much PCA can help softmax regression. Let's train the softmax regression model using the reduced dataset:

#+begin_src jupyter-python
log_clf2 = LogisticRegression(multi_class="multinomial", solver="lbfgs", random_state=42)
t0 = time.time()
log_clf2.fit(X_train_reduced, y_train)
t1 = time.time()
print(f"Training time: {t1-t0:.2f} seconds.")
#+end_src

#+RESULTS:
: Training time: 2.37 seconds.
: /home/ning/apps/conda/envs/ds/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):
: STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
:
: Increase the number of iterations (max_iter) or scale the data as shown in:
:     https://scikit-learn.org/stable/modules/preprocessing.html
: Please also refer to the documentation for alternative solver options:
:     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
:   n_iter_i = _check_optimize_result(

#+begin_src jupyter-python
y_pred = log_clf2.predict(X_test_reduced)
accuracy_score(y_test, y_pred)
#+end_src

#+RESULTS:
: 0.9201

Reducing dimensionality led to a 4× speedup. A very slight drop in performance, which might be a reasonable price to pay for a 4× speedup, depending on the application.


** 10. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image's target class.

Dimensionality reduction on the full 60,000 images takes a very long time, so let's only do this on a random subset of 10,000 images:
#+begin_src jupyter-python
np.random.seed(42)

m = 10000
idx = np.random.permutation(60000)[:m]

X = mnist['data'].values[idx]
y = mnist['target'].values[idx]
#+end_src

#+RESULTS:

#+begin_src jupyter-python
tsne = TSNE(n_components=2, random_state=42)
X_reduced = tsne.fit_transform(X)

plt.figure(figsize=(13,10))
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap="jet")
#plt.axis('off')
plt.colorbar();
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/6f62bba5f1dcaf313de4162c1e218bf89634718e.png]]
This plot tells us which numbers are easily distinguishable from the others (e.g., 0s, 6s, and most 8s are rather well separated clusters), and it also tells us which numbers are often hard to distinguish (e.g., 4s and 9s, 5s and 3s, and so on).

Let's focus on digits 3 and 5, which seem to overlap a lot.

#+begin_src jupyter-python
plt.figure(figsize=(9,9))
cmap = mpl.cm.get_cmap("jet")
for digit in (2, 3, 5):
    plt.scatter(X_reduced[y == digit, 0], X_reduced[y == digit, 1], c=[cmap(digit / 9)])
#plt.axis('off')
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/74b5d7b561905eca4965102ea2b0ddb86c81bc07.png]]


Let's see if we can produce a nicer image by running t-SNE on these 3 digits:

#+begin_src jupyter-python
idx = (y == 2) | (y == 3) | (y == 5)
X_subset = X[idx]
y_subset = y[idx]

tsne_subset = TSNE(n_components=2, random_state=42)
X_subset_reduced = tsne_subset.fit_transform(X_subset)

plt.figure(figsize=(9,9))
for digit in (2, 3, 5):
    plt.scatter(X_subset_reduced[y_subset == digit, 0], X_subset_reduced[y_subset == digit, 1], c=[cmap(digit / 9)])
plt.axis('off');
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/a3f3a15d5afa65d538904f803ac7d8e68a7c689a.png]]

Much better, now the clusters have far less overlap. But some 3s are all over the place. Plus, there are two distinct clusters of 2s, and also two distinct clusters of 5s. It would be nice if we could visualize a few digits from each cluster, to understand why this is the case. Let's do that now.

*** Alternatively, you can write colored digits at the location of each instance, or even plot scaled-down versions of the digit images themselves (if you plot all digits, the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits.

Let's create a plot_digits() function that will draw a scatterplot (similar to the above scatterplots) plus write colored digits, with a minimum distance guaranteed between these digits. If the digit images are provided, they are plotted instead. This implementation was inspired from one of Scikit-Learn's excellent examples (plot_lle_digits, based on a different digit dataset).

#+begin_src jupyter-python


from sklearn.preprocessing import MinMaxScaler
from matplotlib.offsetbox import AnnotationBbox, OffsetImage

def plot_digits(X, y, min_distance=0.05, images=None, figsize=(13, 10)):
    # Let's scale the input features so that they range from 0 to 1
    X_normalized = MinMaxScaler().fit_transform(X)
    # Now we create the list of coordinates of the digits plotted so far.
    # We pretend that one is already plotted far away at the start, to
    # avoid `if` statements in the loop below
    neighbors = np.array([[10., 10.]])
    # The rest should be self-explanatory
    plt.figure(figsize=figsize)
    cmap = mpl.cm.get_cmap("jet")
    digits = np.unique(y)
    for digit in digits:
        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1], c=[cmap(digit / 9)])
    plt.axis("off")
    ax = plt.gcf().gca()  # get current axes in current figure
    for index, image_coord in enumerate(X_normalized):
        closest_distance = np.linalg.norm(np.array(neighbors) - image_coord, axis=1).min()
        if closest_distance > min_distance:
            neighbors = np.r_[neighbors, [image_coord]]
            if images is None:
                plt.text(image_coord[0], image_coord[1], str(int(y[index])),
                         color=cmap(y[index] / 9), fontdict={"weight": "bold", "size": 16})
            else:
                image = images[index].reshape(28, 28)
                imagebox = AnnotationBbox(OffsetImage(image, cmap="binary"), image_coord)
                ax.add_artist(imagebox)
#+end_src

#+RESULTS:

First let's just write colored digits:

#+begin_src jupyter-python
plot_digits(X_reduced, y)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c757927a04ac22d69ec617884136351adbc99499.png]]

#+begin_src jupyter-python
plot_digits(X_reduced, y, images=X, figsize=(20, 16))
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b8dbdb091225881b83e30ad4e91f19acd96f19f8.png]]

#+begin_src jupyter-python
plot_digits(X_subset_reduced, y_subset, images=X_subset, figsize=(22, 22))
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/5b321820063a259613bb48acf6b6eae3f6a4469b.png]]

*** Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations.

#+begin_src jupyter-python
t0 = time.time()
X_pca_reduced = PCA(n_components=2, random_state=42).fit_transform(X)
t1 = time.time()
print("PCA took {:.1f}s.".format(t1 - t0))
plot_digits(X_pca_reduced, y);
#+end_src

#+RESULTS:
:RESULTS:
: PCA took 0.1s.
[[file:./.ob-jupyter/169aaa8210cb1ecf16f03a652ba7bb0e32d745e2.png]]
:END:

#+begin_src jupyter-python
t0 = time.time()
X_lle_reduced = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X)
t1 = time.time()
print("LLE took {:.1f}s.".format(t1 - t0))
plot_digits(X_lle_reduced, y);
#+end_src

#+RESULTS:
:RESULTS:
: LLE took 7.1s.
[[file:./.ob-jupyter/a05d69bc7901bc113345bd72aaf12993051dbe7b.png]]
:END:

PCA + LLE
#+begin_src jupyter-python
pca_lle = Pipeline([
    ("pca", PCA(n_components=0.95, random_state=42)),
    ("lle", LocallyLinearEmbedding(n_components=2, random_state=42)),
])
t0 = time.time()
X_pca_lle_reduced = pca_lle.fit_transform(X)
t1 = time.time()
print("PCA+LLE took {:.1f}s.".format(t1 - t0))
plot_digits(X_pca_lle_reduced, y);
#+end_src

#+RESULTS:
:RESULTS:
: PCA+LLE took 8.0s.
[[file:./.ob-jupyter/c945ffe73f6e4a5bd4041a67278ed6c9ee7aa445.png]]
:END:

MDS

#+begin_src jupyter-python
m = 2000
t0 = time.time()
X_mds_reduced = MDS(n_components=2, random_state=42).fit_transform(X[:m])
t1 = time.time()
print("MDS took {:.1f}s (on just 2,000 MNIST images instead of 10,000).".format(t1 - t0))
plot_digits(X_mds_reduced, y[:m]);
#+end_src

#+RESULTS:
:RESULTS:
: MDS took 49.4s (on just 2,000 MNIST images instead of 10,000).
[[file:./.ob-jupyter/1353a34feb718fac90e27114edbde8b6efd339e5.png]]
:END:

PCA + MDS
#+begin_src jupyter-python
pca_mds = Pipeline([
    ("pca", PCA(n_components=0.95, random_state=42)),
    ("mds", MDS(n_components=2, random_state=42)),
])
t0 = time.time()
X_pca_mds_reduced = pca_mds.fit_transform(X[:m])
t1 = time.time()
print("PCA+MDS took {:.1f}s (on 2,000 MNIST images).".format(t1 - t0))
plot_digits(X_pca_mds_reduced, y[:m]);
#+end_src

#+RESULTS:
:RESULTS:
: PCA+MDS took 49.7s (on 2,000 MNIST images).
[[file:./.ob-jupyter/e731281745556ef930d3e0079d064269ffb63422.png]]
:END:

LDA
#+begin_src jupyter-python
t0 = time.time()
X_lda_reduced = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)
t1 = time.time()
print("LDA took {:.1f}s.".format(t1 - t0))
plot_digits(X_lda_reduced, y, figsize=(12,12));
#+end_src

#+RESULTS:
:RESULTS:
: LDA took 0.7s.
[[file:./.ob-jupyter/86d0e98057582dae45ec20b6ba51bbf0b0e2acef.png]]
:END:

Well, it's pretty clear that t-SNE won this little competition, wouldn't you agree? We did not time it, so let's do that now:

#+begin_src jupyter-python
t0 = time.time()
X_tsne_reduced = TSNE(n_components=2, random_state=42).fit_transform(X)
t1 = time.time()
print("t-SNE took {:.1f}s.".format(t1 - t0))
plot_digits(X_tsne_reduced, y);
#+end_src

#+RESULTS:
:RESULTS:
: t-SNE took 17.5s.
[[file:./.ob-jupyter/c757927a04ac22d69ec617884136351adbc99499.png]]
:END:

It's twice slower than LLE, but still much faster than MDS, and the result looks great. Let's see if a bit of PCA can speed it up:

#+begin_src jupyter-python
pca_tsne = Pipeline([
    ("pca", PCA(n_components=0.95, random_state=42)),
    ("tsne", TSNE(n_components=2, random_state=42)),
])
t0 = time.time()
X_pca_tsne_reduced = pca_tsne.fit_transform(X)
t1 = time.time()
print("PCA+t-SNE took {:.1f}s.".format(t1 - t0))
plot_digits(X_pca_tsne_reduced, y);
#+end_src

#+RESULTS:
:RESULTS:
: PCA+t-SNE took 17.1s.
[[file:./.ob-jupyter/48af989dade9a30b25120255bf20bef7c47e6fe4.png]]
:END:

Yes, PCA roughly gave us a 25% speedup, without damaging the result. We have a winner!
