#+TITLE: Practice Hands-on ML Chap04 - Training Models


#+begin_src jupyter-python
# Py data
import pandas as pd
import numpy as np
from scipy import stats

# Visualization
import matplotlib as mpl
import matplotlib.pyplot as plt

mpl.rc("axes", labelsize=14)
mpl.rc("xtick", labelsize=12)
mpl.rc("ytick", labelsize=12)
mpl.rc("figure", facecolor="white", dpi=120)

# utilities
from copy import deepcopy

# sklearn datasets
from sklearn import datasets

# Data train/test split
from sklearn.model_selection import train_test_split

# Data cleaning and transform
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import OneHotEncoder

# Model selection / Hyper-parameter tuning
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

# Models
from sklearn import set_config
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.linear_model import LogisticRegression
#+end_src

#+RESULTS:


Print all model parameters
#+begin_src jupyter-python
set_config(print_changed_only=False)
#+end_src

#+RESULTS:



* Linear Regression

** Normal Equation

*** Generate samples and estimate with normal equation
Normal Equation is derived by taking derivative on RSS and set it to 0 (ref. Hastie p.12 and p.45):
\( RSS(\beta) = (y-\bm{X}\beta)^T(y-\bm{X}\beta) \)
\( \bm{X}^T(y-\bm{X}\beta) = 0 \)  This is actually the gradient.

Solution to normal equation:
\( \beta = \bm{X}^{\dagger}y = (\bm{X}^T\bm{X})^{-1}\bm{X}^Ty \)

#+Begin_src jupyter-python
m = 100
beta1, beta0 = 3, 4
x1 = 2 * np.random.rand(m, 1)
y = beta0 + beta1 * x1 + np.random.randn(m, 1)

plt.plot(x1, y, "b.")
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)
plt.axis([0, 2, 0, 15])
#+end_src

#+RESULTS:
:RESULTS:
| 0.0 | 2.0 | 0.0 | 15.0 |
[[file:./.ob-jupyter/cec8daaab38c446259530af93e4c326ec573cf85.png]]
:END:

#+begin_src jupyter-python
X = np.c_[x1, np.ones((m, 1))]
beta_hat = (np.linalg.inv(X.T @ X) @ X.T).dot(y)
beta_hat
#+end_src

#+RESULTS:
: array([[2.85492116],
:        [4.08015847]])

*** Predict for X = [0, 2] and draw the regression line

#+begin_src jupyter-python
X_new = np.c_[[0,2], [1,1]]
y_predict = X_new.dot(beta_hat)
print(f'{y_predict = }')
plt.plot(X[:, 0], y, 'b.')
plt.plot(X_new[:, 0], y_predict, 'r-', label='Predictions')
plt.axis([0, 2, 0, 15]);
plt.legend(loc='upper left');
#+end_src

#+RESULTS:
:RESULTS:
: y_predict = array([[4.08015847],
:        [9.7900008 ]])
[[file:./.ob-jupyter/a5474f1453e58115c718e553a704c7a993a83700.png]]
:END:

#+begin_src jupyter-python

#+end_src

#+RESULTS:
: array([[4.08015847],
:        [9.7900008 ]])

*** Use sklearn for linear regression

#+begin_src jupyter-python
lin_reg = LinearRegression()
lin_reg.fit(x1, y)
lin_reg.coef_, lin_reg.intercept_
#+end_src

#+RESULTS:
| array | (((2.85492116))) | array | ((4.08015847)) |

#+begin_src jupyter-python
lin_reg.predict(np.c_[X_new[:, 0]])
#+end_src

#+RESULTS:
: array([[4.08015847],
:        [9.7900008 ]])

*** Use the underhood numpy/scipy linalg.lstsq(), based on SVD.

#+begin_src jupyter-python
beta_svd, residuals, rank, s = np.linalg.lstsq(X, y, rcond=1e-6)
beta_svd, rank, s
#+end_src

#+RESULTS:
| array | (((2.85492116) (4.08015847))) | 2 | array | ((14.304157 4.07782435)) |

*** Use linalg.pinv() directly, which calls linalg.svd().

#+begin_src jupyter-python
np.linalg.pinv(X).dot(y)
#+end_src

#+RESULTS:
: array([[2.85492116],
:        [4.08015847]])


* Gradient Descent

*** Ensure all features have a similar scale when using gradient descent.

** Batch Gradient Descent
MSE(\theta) = RSS(\theta) / m
\( \nabla RSS(\theta) = 2\bm{X}^T (\bm{X}^T\bm{\theta} - \bm{y}) \)
Note the derivative on a quadratic form of matrix can take either a numerator layout or a denominator layout:
https://math.stackexchange.com/a/2655777/801194
https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vecto

#+begin_src jupyter-python
eta = 0.1  # learning rate
n_iterations = 1000
m = 100

theta = np.random.randn(2, 1)  # random initialization

for iteration in range(n_iterations):
    gradients = 2 / m * (X.T @ (X @ theta - y))
    theta -= eta * gradients
theta
#+end_src

#+RESULTS:
: array([[2.85492116],
:        [4.08015847]])

#+begin_src jupyter-python
def plot_gradient_descent(theta, eta, theta_path=None, n_iterations=1000):
    m = len(X)
    plt.plot(X[:, 0], y, "b.")

    theta = theta.copy()
    n_iterations = 1000
    for iteration in range(n_iterations):
        if iteration < 10:
            y_predict = X_new.dot(theta)
            style = "y-" if iteration > 0 else "r--"
            plt.plot(X_new[:, 0], y_predict, style)
        gradients = 2 / m * (X.T @ (X @ theta - y))
        theta -= eta * gradients
        if theta_path is not None:
            theta_path.append(theta.flatten())
    plt.xlabel("$x_1$")
    plt.axis([0, 2, 0, 15])
    plt.title(r"$\eta$ = {}".format(eta))


np.random.seed(42)
theta = np.random.randn(2, 1)[::-1]
theta_path_bgd = []

plt.figure(figsize=(10, 4))
plt.subplot(131)
plot_gradient_descent(theta, eta=0.02)
plt.ylabel("$y$", rotation=0)
plt.subplot(132)
plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)
plt.subplot(133)
plot_gradient_descent(theta, eta=0.5)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/ae31c0187985b8130703134cce8fc9ce754db326.png]]


** Stochastic Gradient Descent

#+begin_src jupyter-python
theta_path_sgd = []
m = len(X)
np.random.seed(42)


def learning_schedule(t, t0, t1):
    return t0 / (t + t1)


n_epochs = 50
t0, t1 = 5, 50  # learning schedule hyperparameters
theta = np.random.randn(2, 1)[::-1]  # random initialization

for epoch in range(n_epochs):
    for i in range(m):
        if epoch == 0 and i < 20:  # plot predictions
            y_predict = X_new @ theta
            style = "b-" if i > 0 else "r--"
            plt.plot(X_new[:, 0], y_predict, style)
        random_index = np.random.randint(m)
        xi = X[random_index : random_index + 1]  # Use slice to keep matrix dimensions
        yi = y[random_index : random_index + 1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(epoch * m + i, t0, t1)
        theta -= eta * gradients
        theta_path_sgd.append(theta.flatten())

plt.plot(X[:, 0], y, "b.")
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)
plt.axis([0, 2, 0, 15]);
print('Fitted theta = \n', theta)
#+end_src

#+RESULTS:
:RESULTS:
: Fitted theta =
:  [[2.83374356]
:  [4.06059447]]
[[file:./.ob-jupyter/f58b156d6cdd615ba07cc170b677cf8a0fa14d12.png]]
:END:

#+begin_src jupyter-python
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)
sgd_reg.fit(np.c_[X[:, 0]], y.ravel())
#+end_src

#+RESULTS:
: SGDRegressor(eta0=0.1, penalty=None, random_state=42)

#+begin_src jupyter-python
sgd_reg.coef_, sgd_reg.intercept_
#+end_src

#+RESULTS:
| array | ((2.88424934)) | array | ((4.10047391)) |


** Mini-batch gradient descent

#+begin_src jupyter-python
def learning_schedule(t, t0, t1):
    return t0 / (t + t1)

theta_path_mgd = []

n_iterations = 50
minibatch_size = 20

np.random.seed(42)
theta = np.random.randn(2, 1)

t0, t1 = 200, 1000
t = 0
for epoch in range(n_iterations):
    shuffled_indices = np.random.permutation(m)
    X_shuffled = X[shuffled_indices]
    y_shuffled = y[shuffled_indices]
    for i in range(0, m, minibatch_size):
        t += 1
        xi = X_shuffled[i:i+minibatch_size]
        yi = y_shuffled[i:i+minibatch_size]
        gradients = (2 / minibatch_size) * xi.T @ (xi @ theta - yi)
        eta = learning_schedule(t, t0, t1)
        theta = theta - eta * gradients
        theta_path_mgd.append(theta.flatten())
theta
#+end_src

#+RESULTS:
: array([[2.89978914],
:        [4.10695506]])

** Compare the 3 GD algorithms
#+begin_src jupyter-python
theta_path_bgd = np.array(theta_path_bgd)
theta_path_sgd = np.array(theta_path_sgd)
theta_path_mgd = np.array(theta_path_mgd)

plt.figure(figsize=(8, 4))
plt.plot(theta_path_bgd[:, 1].T, theta_path_bgd[:, 0].T, 'b-o', linewidth=3, label='Batch')
plt.plot(theta_path_sgd[:, 1].T, theta_path_sgd[:, 0].T, 'r-s', linewidth=1, label='Stochastic')
plt.plot(theta_path_mgd[:, 1], theta_path_mgd[:, 0], 'g-+', linewidth=2, label='Mini-batch')
plt.legend(loc='upper left')
plt.xlabel(r"$\theta_0$")
plt.ylabel(r"$\theta_1$", rotation=0)
plt.axis([2.5, 4.5, 2.3, 3.9]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/8831e6c553ed23a474c248f10ca1a4b201f9f86f.png]]


* Polynomial Regression

#+begin_src jupyter-python
np.random.seed(42)

m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)

plt.plot(X, y, "b.")
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)
plt.axis([-3, 3, 0, 10]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/39bb14e87fba6b403da33eaae414fe2b5b01561f.png]]

#+begin_src jupyter-python
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
print(X[:5])
print(X_poly[:5])
#+end_src

#+RESULTS:
: [[-0.75275929]
:  [ 2.70428584]
:  [ 1.39196365]
:  [ 0.59195091]
:  [-2.06388816]]
: [[-0.75275929  0.56664654]
:  [ 2.70428584  7.3131619 ]
:  [ 1.39196365  1.93756281]
:  [ 0.59195091  0.35040587]
:  [-2.06388816  4.25963433]]

#+begin_src jupyter-python
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
lin_reg.intercept_, lin_reg.coef_
#+end_src

#+RESULTS:
| array | ((1.78134581)) | array | (((0.93366893 0.56456263))) |

#+begin_src jupyter-python
X_new = np.linspace(-3, 3, 100).reshape(100, 1)
X_new_poly = poly_features.transform(X_new)
y_new = lin_reg.predict(X_new_poly)
plt.plot(X, y, 'b.')
plt.plot(X_new, y_new, 'r-', linewidth=2, label='Predictions')
plt.xlabel("$x_1$"); plt.ylabel("$y", rotation=0)
plt.legend(loc='upper left')
plt.axis([-3, 3, 0, 10]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/dc6182a54fddff66f17e5e14c8f8dd53290c74c7.png]]


* Learning Curves - Underfitting vs. Overfitting

#+begin_src jupyter-python
for style, width, degree in (('g-', 1, 300), ('b--', 2, 2), ('r-+', 2, 1)):
    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)
    std_scaler = StandardScaler()
    lin_reg = LinearRegression()
    polynomial_regression = Pipeline([
        ('poly_features', polybig_features),
        ('std_scaler', std_scaler),
        ('lin_reg', lin_reg),
    ])
    polynomial_regression.fit(X, y)
    y_newbig = polynomial_regression.predict(X_new)
    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)

plt.plot(X, y, 'b.', linewidth=3)
plt.legend(loc='upper left')
plt.xlabel("$x_1$"); plt.ylabel("$y$", rotation=0)
plt.axis([-3, 3, 0, 10]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/5eaa632dce108d1afd6227119d266763a1224709.png]]

#+begin_src jupyter-python
def plot_learning_curves(model, X, y):
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=10
    )
    train_errors, val_errors = [], []
    for m in range(1, len(X_train)):  # increase training set size
        model.fit(X_train[:m], y_train[:m])
        y_train_predict = model.predict(X_train[:m])
        y_val_predict = model.predict(X_val)
        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))
        val_errors.append(mean_squared_error(y_val, y_val_predict))

    plt.plot(np.sqrt(train_errors), 'r-+', linewidth=2, label='train')
    plt.plot(np.sqrt(val_errors), 'b-', linewidth=3, label='val')
    plt.legend(loc='upper right')
    plt.xlabel('Training set size')
    plt.ylabel('RMSE')

lin_reg = LinearRegression()
plot_learning_curves(lin_reg, X, y)
plt.axis([0, 80, 0, 3]);
plt.title("Underfitting Learning Curves");
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b9376413ce60e0a4839cc9a7950f248eb2fe66a4.png]]

#+begin_src jupyter-python
polynomial_regression = Pipeline([
    ('poly_features', PolynomialFeatures(degree=10, include_bias=False)),
    ('lin_reg', LinearRegression()),
])
plot_learning_curves(polynomial_regression, X, y)
plt.axis([0, 80, 0, 3])
plt.title("Overfitting Learning Curves");
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/6da71c89a1d45cee4ed9ab245cde92134886c9b3.png]]


* Regularized Linear Models

For linear models, regularization is by constraining the weights of the model.

- Generate data
#+begin_src jupyter-python
np.random.seed(42)
m = 20
X = 3 * np.random.rand(m, 1)
y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5
X_new = np.linspace(0, 3, 100).reshape(100, 1)
#+end_src

#+RESULTS:

** Ridge Regression - Cholesky, SAG, SGDRegressor

#+begin_src jupyter-python
ridge_reg = Ridge(alpha=1, solver="cholesky", random_state=42)
ridge_reg.fit(X, y)
ridge_reg.predict([[1.5]])
#+end_src

#+RESULTS:
: array([[1.55071465]])

#+begin_src jupyter-python
ridge_reg = Ridge(alpha=1, solver="sag", random_state=42)
ridge_reg.fit(X, y)
ridge_reg.predict([[1.5]])
#+end_src


#+begin_src jupyter-python
sgd_reg = SGDRegressor(penalty='l2', max_iter=1000, tol=1e-3, random_state=42)
sgd_reg.fit(X, y.ravel())
sgd_reg.predict([[1.5]])
#+end_src

#+RESULTS:
: array([1.47012588])

#+begin_src jupyter-python
def plot_model(model_class, polynomial, alphas, **model_kargs):
    for alpha, style in zip(alphas, ('b-', 'g--', 'r:')):
        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()
        if polynomial:
            model = Pipeline([
                ('poly_features', PolynomialFeatures(degree=10, include_bias=False)),
                ('std_scaler', StandardScaler()),
                ("regul_reg", model),
            ])
        model.fit(X, y)
        y_new_regul = model.predict(X_new)
        lw = 2 if alpha > 0 else 1
        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=rf"$\alpha = {alpha}$")
    plt.plot(X, y, 'b.', linewidth=3)
    plt.legend(loc='upper left')
    plt.xlabel("$x_1$")
    plt.axis([0, 3, 0, 4])

plt.figure(figsize=(8, 4))
plt.subplot(121); plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)
plt.ylabel("$y$", rotation=0)
plt.subplot(122); plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)
plt.title("Ridge Regression - 1st order and polynomial");
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3db83d8e6829fa578f3514eb98d917d686274a69.png]]


** Lasso Regression

#+begin_src jupyter-python
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)
lasso_reg.predict([[1.5]])
#+end_src

#+RESULTS:
: array([1.53788174])

#+begin_src jupyter-python
plt.figure(figsize=(8, 4))
plt.subplot(121); plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)
plt.ylabel("$y$", rotation=0)
plt.subplot(122); plot_model(Lasso, polynomial=True, alphas=(0, 1e-2, 1), random_state=42)
plt.title("Lasso Regression: 1st order and polynomial");
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/f42cd468f4046ea3d20c067c759564fce05ff2ed.png]]


** Elastic Net

#+begin_src jupyter-python
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
elastic_net.fit(X, y)
elastic_net.predict([[1.5]])
#+end_src

#+RESULTS:
: array([1.54333232])

** Early Stopping

Generate train / test
#+begin_src jupyter-python
np.random.seed(42)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)
X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
poly_scaler = Pipeline(
    [
        ("poly_features", PolynomialFeatures(degree=90, include_bias=False)),
        ("std_scaler", StandardScaler()),
    ]
)
X_train_poly_scaled = poly_scaler.fit_transform(X_train)
X_val_poly_scaled = poly_scaler.transform(X_val)


sgd_reg = SGDRegressor(
    max_iter=1,
    tol=-np.infty,
    warm_start=True,
    penalty=None,
    learning_rate="constant",
    eta0=0.0005,
    random_state=42,
)

n_epochs = 500
train_errors, val_errors = [], []
for epoch in range(n_epochs):
    sgd_reg.fit(X_train_poly_scaled, y_train)
    y_train_predict = sgd_reg.predict(X_train_poly_scaled)
    y_val_predict = sgd_reg.predict(X_val_poly_scaled)
    train_errors.append(mean_squared_error(y_train, y_train_predict))
    val_errors.append(mean_squared_error(y_val, y_val_predict))

best_epoch = np.argmin(val_errors)
best_val_rmse = np.sqrt(val_errors[best_epoch])

plt.annotate(
    "Best model",
    xy=(best_epoch, best_val_rmse),
    xytext=(best_epoch, best_val_rmse + 1),
    ha="center",
    arrowprops=dict(facecolor="black", shrink=0.05),
    fontsize=16,
)

best_val_rmse -= 0.03  # just to make the graph look better
plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], "k:", linewidth=2)
plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="Validation set")
plt.plot(np.sqrt(train_errors), "r--", linewidth=2, label="Training set")
plt.legend(loc="upper right", fontsize=14)
plt.xlabel("Epoch", fontsize=14)
plt.ylabel("RMSE", fontsize=14)
plt.title("Early Stopping")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/e4adbe779e069ed474c81869a01420d266c84072.png]]


* Logistic Regression

Sigmoid function plot.
#+begin_src jupyter-python
t = np.linspace(-10, 10, 100)
sig = 1 / (1 + np.exp(-t))
plt.figure(figsize=(9, 3));
plt.plot([-10, 10], [0, 0], "k-")
plt.plot([-10, 10], [0.5, 0.5], "k:")
plt.plot([-10, 10], [1, 1], "k:")
plt.plot([0, 0], [-1.1, 1.1], "k-")
plt.plot(t, sig, "b-", linewidth=2, label=r"$\sigma(t) = \frac{1}{1 + e^{-t}}$")
plt.xlabel("t");
plt.legend(loc="upper left", fontsize=14)
plt.axis([-10, 10, -0.1, 1.1]);
plt.title("Sigmoid Function");
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b851f7dfa37d990bbbf0ca6311e86edd631fde5b.png]]

#+begin_src jupyter-python
iris = datasets.load_iris()
print(iris.keys())
print(iris.DESCR)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])
.. _iris_dataset:

Iris plants dataset
--------------------

,**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica

    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
from Fisher's paper. Note that it's the same as in R, but not as in the UCI
Machine Learning Repository, which has two wrong data points.

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher's paper is a classic in the field and
is referenced frequently to this day.  (See Duda & Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

.. topic:: References

   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"
     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to
     Mathematical Statistics" (John Wiley, NY, 1950).
   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments".  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...
#+end_example
:END:

*** Single Feature

#+begin_src jupyter-python
X = iris["data"][:, 3:]  # petal width
y = (iris["target"] == 2).astype(np.int)  # binary classes for Iris Virginica
#+end_src

#+RESULTS:

#+begin_src jupyter-python
log_reg = LogisticRegression(solver="lbfgs", random_state=42)
log_reg.fit(X, y)
#+end_src

#+RESULTS:
: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
:                    intercept_scaling=1, l1_ratio=None, max_iter=100,
:                    multi_class='auto', n_jobs=None, penalty='l2',
:                    random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
:                    warm_start=False)

#+begin_src jupyter-python
X_new = np.c_[np.linspace(0, 3, 1000)]
y_proba = log_reg.predict_proba(X_new)
decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]

plt.figure(figsize=(8, 3))
plt.plot(X[y==0], y[y==0], "bs")
plt.plot(X[y==1], y[y==1], "g^")
plt.plot([decision_boundary, decision_boundary], [0, 1], "k:", linewidth=2)
plt.plot(X_new, y_proba[:, 1], "g-", linewidth=2, label="Iris virginica")
plt.plot(X_new, y_proba[:, 0], "b--", linewidth=2, label="Not Iris virginica")
plt.text(decision_boundary+0.02, 0.15, "Decision  boundary", fontsize=14, color="k", ha="center")
plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')
plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')
plt.xlabel("Petal width (cm)", fontsize=14)
plt.ylabel("Probability", fontsize=14)
plt.legend(loc="center left", fontsize=14)
plt.axis([0, 3, -0.02, 1.02])
plt.title("Iris logistic regression.");
#+end_src

#+RESULTS:
:RESULTS:
: /home/ning/apps/conda/envs/ds/lib/python3.9/site-packages/matplotlib/patches.py:1338: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
:   verts = np.dot(coords, M) + (x + dx, y + dy)
[[file:./.ob-jupyter/9ca71feec3ceb9fdc9a525a4b5d2cec1436333bd.png]]
:END:


#+begin_src jupyter-python
decision_boundary
#+end_src

#+RESULTS:
: array([1.66066066])

#+begin_src jupyter-python
log_reg.predict([[1.7], [1.5]])
#+end_src

#+RESULTS:
: array([1, 0])

#+begin_src jupyter-python

#+end_src

#+RESULTS:
: 10.434782608695652

*** Two Features

#+begin_src jupyter-python
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.int)

log_reg = LogisticRegression(
    solver="lbfgs", C=10 ** 10, random_state=42
)  # C = 1 / alpha for regularization
log_reg.fit(X, y)

# Generate new data
x0, x1 = np.meshgrid(
    np.linspace(2.9, 7, 500).reshape(-1, 1), np.linspace(0.8, 2.7, 200).reshape(-1, 1),
)
X_new = np.c_[x0.ravel(), x1.ravel()]

y_proba = log_reg.predict_proba(X_new)

# plots
plt.figure(figsize=(10, 4))

# plot training data
plt.plot(X[y == 0, 0], X[y == 0, 1], "bs")
plt.plot(X[y == 1, 0], X[y == 1, 1], "g^")
plt.text(3.5, 1.5, "Not Iris virginica", fontsize=14, color="b", ha="center")
plt.text(6.5, 2.3, "Iris virginica", fontsize=14, color="g", ha="center")

# plot contour of predicted probabilities
zz = y_proba[:, 1].reshape(x0.shape)
contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)
plt.clabel(contour, inline=1, fontsize=12)

# plot decision boundary
left_right = np.array([2.9, 7])
boundary = (
    -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]
)  # x_2 = -(\theta_1 * x_1 + \theta_0) / \theta_2
plt.plot(left_right, boundary, "k--", linewidth=3)

# figure axis
plt.xlabel("Petal length")
plt.ylabel("Petal width")
plt.axis([2.9, 7, 0.8, 2.7])
plt.title("Iris Logistic Regression with 2 features")
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/d822fffa6c276d80dbb49f4cf8db38321987d9bb.png]]

#+begin_src jupyter-python

#+end_src

#+RESULTS:
: 150

*** Softmax Regression

#+begin_src jupyter-python
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = iris["target"]

softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, random_state=42)
softmax_reg.fit(X, y)
#+end_src

#+RESULTS:
: LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
:                    intercept_scaling=1, l1_ratio=None, max_iter=100,
:                    multi_class='multinomial', n_jobs=None, penalty='l2',
:                    random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
:                    warm_start=False)

#+begin_src jupyter-python
x0, x1 = np.meshgrid(
        np.linspace(0, 8, 500).reshape(-1, 1),
        np.linspace(0, 3.5, 200).reshape(-1, 1),
    )
X_new = np.c_[x0.ravel(), x1.ravel()]


y_proba = softmax_reg.predict_proba(X_new)
y_predict = softmax_reg.predict(X_new)

zz1 = y_proba[:, 1].reshape(x0.shape)
zz = y_predict.reshape(x0.shape)

plt.figure(figsize=(10, 4))
plt.plot(X[y==2, 0], X[y==2, 1], "g^", label="Iris virginica")
plt.plot(X[y==1, 0], X[y==1, 1], "bs", label="Iris versicolor")
plt.plot(X[y==0, 0], X[y==0, 1], "yo", label="Iris setosa")

from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])

plt.contourf(x0, x1, zz, cmap=custom_cmap)
contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)
plt.clabel(contour, inline=1, fontsize=12)
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="center left", fontsize=14)
plt.axis([0, 7, 0, 3.5])
plt.title("Softmax regression for multinomial classification");
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b0ae1614cd04fd8eb30d391707daa5843d0033a1.png]]

#+begin_src jupyter-python
print(softmax_reg.predict([[5,2]]), softmax_reg.predict_proba([[5,2]]))
print(softmax_reg.predict([[5,3]]), softmax_reg.predict_proba([[5,3]]))
#+end_src

#+RESULTS:
: [2] [[6.38014896e-07 5.74929995e-02 9.42506362e-01]]
: [2] [[8.83643458e-10 8.64916081e-05 9.99913508e-01]]



* Exercises

** 12. Implement batch gradient descent with early stopping for softmax regression.
*** Load data.
#+begin_src jupyter-python
X = iris["data"][:, (2, 3)]
y = iris["target"]
X_with_bias = np.c_[np.ones((len(X), 1)), X]
#+end_src

#+RESULTS:

*** train / test split

#+begin_src jupyter-python
X_train, X_test, y_train, y_test = train_test_split(X_with_bias, y, test_size=0.2, random_state=2042, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=2042, stratify=y_train)  # 0.2 = 0.8 * 0.25
#+end_src

#+RESULTS:

*** Convert training class labels to probabilities (0 or 1) with one hot encoding.

#+begin_src jupyter-python
enc = OneHotEncoder(sparse=False)
Y_train_one_hot = enc.fit_transform(y_train.reshape(-1, 1))
Y_valid_one_hot = enc.transform(y_val.reshape(-1, 1))
Y_test_one_hot = enc.transform(y_test.reshape(-1, 1))
#+end_src

#+RESULTS:

#+begin_src jupyter-python
print(Y_train_one_hot[:5], Y_test_one_hot[:5], sep='\n\n')
#+end_src

#+RESULTS:
#+begin_example
[[0. 0. 1.]
 [1. 0. 0.]
 [0. 1. 0.]
 [1. 0. 0.]
 [1. 0. 0.]]

[[1. 0. 0.]
 [1. 0. 0.]
 [0. 0. 1.]
 [0. 1. 0.]
 [0. 0. 1.]]
#+end_example


*** Define the input / output dimensions.

#+begin_src jupyter-python
N = X_train.shape[0]
p = X_train.shape[1]  # p=3 (2 features + 1 bias term)
K = len(np.unique(y_train))  # K=3 iris classes
#+end_src

#+RESULTS:

*** Define softmax function.

#+begin_src jupyter-python
def softmax(logits):
    exps = np.exp(logits)
    exp_sums = np.sum(exps, axis=1, keepdims=True)
    return exps / exp_sums
#+end_src

#+RESULTS:

*** Training with batch gradient descent.

#+begin_src jupyter-python
n_iterations = 5001
epsilon = 1e-7  # disturbance added to log(p) in case p is 0.

# initialize
Theta = np.random.randn(p, K)  # pxK
eta = 0.01  # learning rate

# iterations
for iteration in range(n_iterations):
    logits = X_train @ Theta  # NxK = Nxp @ pxK
    Y_proba = softmax(logits) # NxK
    cross_entropy = -(Y_train_one_hot * np.log(Y_proba + epsilon))  # NxK * NxK
    loss = np.mean(np.sum(cross_entropy, axis=1))
    if iteration % 500 == 0:
        print(iteration, loss)
    error = Y_proba - Y_train_one_hot  # NxK
    gradients = (1/N) * (X_train.T @ error)  # pxN @ NxK = pxK
    Theta -= eta * gradients
#+end_src

#+RESULTS:
#+begin_example
0 2.852404398402622
500 0.7282223929896863
1000 0.6285843253102453
1500 0.5629876588547268
2000 0.5161286137732828
2500 0.48054127933759866
3000 0.4522872201421428
3500 0.4291083175400983
4000 0.40961278862488293
4500 0.39289078737997263
5000 0.37831888753897547
#+end_example

#+begin_src jupyter-python
Theta
#+end_src

#+RESULTS:
: array([[ 4.30717443, -0.74048543, -4.29635837],
:        [-1.77057304,  0.38272864,  0.1704261 ],
:        [-0.32139446, -1.15464498,  1.81464087]])

**** Validation

#+begin_src jupyter-python
logits = X_val @ Theta
Y_proba = softmax(logits)
y_predict = np.argmax(Y_proba, axis=1)
accuracy_score = np.mean(y_predict == y_val)
accuracy_score
#+end_src

#+RESULTS:
: 0.9333333333333333

*** Add l2 ridge regularization
- Do not regularize the first bias term in Theta
- Increase learning rate eta from 0.01 to 0.1

#+begin_src jupyter-python
n_iterations = 5001
epsilon = 1e-7  # disturbance added to log(p) in case p is 0.

# initialize
Theta = np.random.randn(p, K)  # pxK
eta = 0.1  # learning rate
alpha = 0.1  # regularization hyper-parameter

# iterations
for iteration in range(n_iterations):
    logits = X_train @ Theta  # NxK = Nxp @ pxK
    Y_proba = softmax(logits) # NxK
    cross_entropy = -(Y_train_one_hot * np.log(Y_proba + epsilon))  # NxK * NxK
    cross_entropy_loss = np.mean(np.sum(cross_entropy, axis=1))
    l2_loss = 0.5 * np.sum(Theta[1:]**2)  # do not regulrize 1st bias term
    loss = cross_entropy_loss + alpha * l2_loss
    if iteration % 500 == 0:
        print(iteration, loss)
    error = Y_proba - Y_train_one_hot  # NxK
    gradients = (1/N) * (X_train.T @ error) + np.r_[np.zeros((1, K)), alpha * Theta[1:]]  # pxN @ NxK = pxK
    Theta -= eta * gradients
#+end_src

#+RESULTS:
#+begin_example
0 6.395520287140902
500 0.5450585925173804
1000 0.5092193192463617
1500 0.4994363569470744
2000 0.49587412862684954
2500 0.4944503541472405
3000 0.4938545835210668
3500 0.4935987057708081
4000 0.49348705810116855
4500 0.49343785576396215
5000 0.49341603345956686
#+end_example

**** Validation

#+begin_src jupyter-python
logits = X_val @ Theta
Y_proba = softmax(logits)
y_predict = np.argmax(Y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_val)
accuracy_score
#+end_src

#+RESULTS:
: 0.9333333333333333

*** Add early stopping

#+begin_src jupyter-python
n_iterations = 5001
epsilon = 1e-7  # disturbance added to log(p) in case p is 0.

# initialize
Theta = np.random.randn(p, K)  # pxK
eta = 0.1  # learning rate
alpha = 0.1  # regularization hyper-parameter
best_loss = np.inf  # monitor the best loss so far for early stopping

# iterations
for iteration in range(n_iterations):
    logits = X_train @ Theta  # NxK = Nxp @ pxK
    Y_proba = softmax(logits) # NxK
    cross_entropy = -(Y_train_one_hot * np.log(Y_proba + epsilon))  # NxK * NxK
    cross_entropy_loss = np.mean(np.sum(cross_entropy, axis=1))
    l2_loss = 0.5 * np.sum(Theta[1:]**2)  # do not regulrize 1st bias term
    loss = cross_entropy_loss + alpha * l2_loss
    if iteration % 500 == 0:
        print(iteration, loss, end=' ')
    error = Y_proba - Y_train_one_hot  # NxK
    gradients = (1/N) * (X_train.T @ error) + np.r_[np.zeros((1, K)), alpha * Theta[1:]]  # pxN @ NxK = pxK
    Theta -= eta * gradients

    # measure the loss on validation set
    logits = X_val @ Theta
    Y_proba = softmax(logits)
    cross_entropy = -(Y_valid_one_hot * np.log(Y_proba + epsilon))
    cross_entropy_loss = np.mean(np.sum(cross_entropy, axis=1))
    l2_loss = 0.5 * np.sum(np.square(Theta[1:]))
    loss = cross_entropy_loss + alpha * l2_loss
    if iteration % 500 == 0:
        print(loss)
    if loss < best_loss:
        best_loss = loss
    else:
        print(iteration-1, best_loss)
        print(iteration, loss, "early stopping!")
        break
#+end_src

#+RESULTS:
: 0 3.459318044148726 2.4891345071048487
: 500 0.5380576213841575 0.5561851198612765
: 1000 0.5077492900134172 0.5301642783640911
: 1500 0.49893170821813393 0.5238977473256214
: 2000 0.49567734372156735 0.5221358249603424
: 2500 0.4943691033000278 0.5217327179970004
: 2682 0.5217140930193673
: 2683 0.5217140931855128 early stopping!

**** Validation

#+begin_src jupyter-python
Y_proba = softmax(X_val @ Theta)
y_predict = np.argmax(Y_proba, axis=1)
accuracy_score = np.mean(y_predict == y_val)
accuracy_score
#+end_src

#+RESULTS:
: 0.9333333333333333

*** Plot model's predictions on the sample space and whole data set

#+begin_src jupyter-python
x0, x1 = np.meshgrid(
    np.linspace(0, 8, 500).reshape(-1, 1),
    np.linspace(0, 3.5, 200).reshape(-1, 1),
)
X_space = np.c_[x0.ravel(), x1.ravel()]
X_space_with_bias = np.c_[np.ones((len(X_space), 1)), X_space]
Y_space_proba = softmax(X_space_with_bias @ Theta)
y_space_predict = np.argmax(Y_space_proba, axis=1)

zz_k1_proba = Y_space_proba[:, 1].reshape(x0.shape)  # probability of Iris versicolor
zz_predictions = y_space_predict.reshape(x0.shape)

# plot whole data set
plt.figure(figsize=(10,4))
plt.plot(X[y==2, 0], X[y==2, 1], "g^", label="Iris virginica")
plt.plot(X[y==1, 0], X[y==1, 1], "bs", label="Iris versicolor")
plt.plot(X[y==0, 0], X[y==0, 1], "yo", label="Iris setosa")

# plot predictions over the sample space
from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])
plt.contourf(x0, x1, zz_predictions, cmap=custom_cmap)

# plot probability countours for Iris veriscolor
contour = plt.contour(x0, x1, zz_k1_proba, cmap=plt.cm.brg)
plt.clabel(contour, inline=1, fontsize=12)

# axis
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="upper left", fontsize=14)
plt.axis([0, 7, 0, 3.5]);
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2f2d5be8e6fe72c59a1248d4db302bae8382fb9c.png]]

*** Measure generalization error with test data set

#+begin_src jupyter-python
logits = X_test @ Theta
Y_proba = softmax(logits)
y_predict = np.argmax(Y_proba, axis=1)
accuracy_score = np.mean(y_predict == y_test)
accuracy_score
#+end_src

#+RESULTS:
: 0.9666666666666667
